{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Clasificacion de Imagenes del dataset Fashion mnist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import matplotlib as plt\n",
    "from random import randint\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='../data'\n",
    "df_train = pd.read_csv(\"../data/fashion-mnist_train.csv\")\n",
    "df_test = pd.read_csv(\"../data/fashion-mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785)\n",
      "(10000, 785)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_reduced = df_train[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 1)\n",
      "(10000, 784) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Uso values para mandar todo a arrays de numpy\n",
    "X_train = df_train[df_train.columns[1:]].values\n",
    "y_train = df_train[\"label\"].values.reshape(-1, 1)\n",
    "\n",
    "X_test = df_test[df_test.columns[1:]].values\n",
    "y_test = df_test[\"label\"].values.reshape(-1, 1)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorias = ['top', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "\n",
    "#0 T-shirt/top\n",
    "#1 Trouser\n",
    "#2 Pullover\n",
    "#3 Dress\n",
    "#4 Coat\n",
    "#5 Sandal\n",
    "#6 Shirt\n",
    "#7 Sneaker\n",
    "#8 Bag\n",
    "#9 Ankle boot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x640 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "images = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    images.append(X_train[i].reshape(28, 28))\n",
    "\n",
    "\n",
    "figure(figsize=(10, 8), dpi=80)\n",
    "for i in range(25):\n",
    "    j = np.random.randint(0, len(images))\n",
    "    plt.subplot(5,5,i+1) \n",
    "    plt.imshow(images[j], interpolation='none', cmap=\"gray\")\n",
    "    plt.title(\"clase: {}\".format(categorias[y_train[j][0]]), fontsize = 10) \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"graficos/data_examples.svg\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de entrenamiento shape (60000, 784)\n",
      "Matriz de testeo shape (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Cargo los datos\n",
    "\n",
    "from keras.datasets import mnist,fashion_mnist\n",
    "\n",
    "(X_train_raw, y_train), (X_test_raw, y_test) = fashion_mnist.load_data()  # cargo los dataset de entrenamiento y testeo\n",
    "\n",
    "# reshapeamos para obtener un vector de 784 elementos (features) por cada imagen (samples)\n",
    "X_train = X_train_raw.reshape(60000, -1) # no tocamos el numero de samples, 60000, pero reshapeamos las demas dimensiones\n",
    "X_test = X_test_raw.reshape(10000, -1) # no tocamos el numero de samples, 60000, pero reshapeamos las demas dimensiones\n",
    "\n",
    "X_train = X_train.astype('float32') # transformamos el tipo de datos a \"float32\"\n",
    "X_test = X_test.astype('float32') # transformamos el tipo de datos a \"float32\"\n",
    "\n",
    "# normalizamos por el maximo valor que pueden tener los pixels para que los valores queden entre 0 y 1\n",
    "X_train = X_train/255.0\n",
    "X_test = X_test/255.0\n",
    "\n",
    "# print the final input shape ready for training\n",
    "print(\"Matriz de entrenamiento shape\", X_train.shape)\n",
    "print(\"Matriz de testeo shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo: construccion del modelo usando Knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy promedio: 0.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "acc = [] # donde van a ir a parar las accuracies\n",
    "indices = np.arange(len(y_test)) # vector de indices\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=5) # clasificador\n",
    "clf.fit(X_train, y_train) # una sola vez\n",
    "\n",
    "\n",
    "for n in np.arange(0,10): # itero \n",
    "\n",
    "  print(n) # para los ansiosos\n",
    "  shuffle(indices) # permuto el vector de indices\n",
    "  X_KNN = X_test[indices[0:100],:] # los primeros 100 del vector de indices\n",
    "  y_KNN = y_test[indices[0:100]]\n",
    "\n",
    "  y_pred = clf.predict(X_KNN) # predigo etiquetas\n",
    "  accuracy =  accuracy_score(y_pred,y_KNN) # acuraccy\n",
    "  acc.append(accuracy) # guardo\n",
    "\n",
    "print('Accuracy promedio:', np.mean(accuracy)) # el promedio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[855,   1,  17,  16,   3,   1, 100,   1,   6,   0],\n",
       "       [  8, 968,   4,  12,   4,   0,   3,   0,   1,   0],\n",
       "       [ 24,   2, 819,  11,  75,   0,  69,   0,   0,   0],\n",
       "       [ 41,   8,  15, 860,  39,   0,  34,   0,   3,   0],\n",
       "       [  2,   1, 126,  26, 773,   0,  71,   0,   1,   0],\n",
       "       [  1,   0,   0,   0,   0, 822,   5,  96,   1,  75],\n",
       "       [176,   1, 132,  23,  80,   0, 575,   0,  13,   0],\n",
       "       [  0,   0,   0,   0,   0,   3,   0, 961,   0,  36],\n",
       "       [  2,   0,  10,   4,   7,   0,  16,   7, 953,   1],\n",
       "       [  0,   0,   0,   0,   0,   2,   1,  29,   0, 968]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_pred = clf.predict(X_test)\n",
    "\n",
    "#confusion_matrix(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 5, 8, 6, 8, 5, 0, 8, 2, 9, 5, 8, 0, 0, 9, 4, 7, 9, 9, 3, 7, 1,\n",
       "       2, 4, 7, 1, 4, 6, 3, 9, 3, 8, 9, 9, 4, 1, 1, 7, 5, 1, 2, 6, 7, 9,\n",
       "       3, 5, 5, 1, 6, 1, 0, 1, 0, 4, 1, 8, 3, 6, 1, 0, 8, 3, 6, 2, 3, 7,\n",
       "       7, 7, 0, 2, 6, 8, 5, 2, 5, 0, 7, 3, 0, 2, 0, 1, 7, 1, 1, 1, 1, 2,\n",
       "       1, 4, 7, 4, 3, 2, 4, 6, 1, 4, 4, 5], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape\n",
    "y_KNN\n",
    "#indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "exitos = indices[y_pred == y_KNN[0]] # los indices correspondientes a los casos en que le pegamos a la etiqueta\n",
    "fracasos = indices[y_pred != y_KNN[0]]# los indices correspondientes a los casos en que  fallamos la etiqueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1094"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exitos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1765 3957 7261 ... 6103 8923 6702]\n",
      "[8922 3958 7850 ... 4800 6561  508]\n"
     ]
    }
   ],
   "source": [
    "print(fracasos)\n",
    "print(exitos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAAD5CAYAAADGFMJFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAQ6ElEQVR4nO3de4yc5XXH8d/Bt/WNBe/aeA0uNqppjVNVDVikTaGEKFRCbt02EEobASr/cAmkIHpBilJahaYUIUpVCgGJ0gQUp7SxyEWt47pAXXBrKDHYJBCsmJiCvYDxbX1Z2/D0j3m3mS4753j39WbmwPcjWczOmee9zOxv3tk5PO9rpRQB6HzHtXsDABwdwgokQViBJAgrkARhBZIgrEASKcNqZsXM7mj6+SYzu6WNm/QTYWaPm9lZx3iZA8dyeceSmT1oZhdVt4/5vmeTMqySBiX9lpn1tntD8P5gZhPavQ2RrGE9Iuk+STcMLzS/G1c/DzTd/gMze9rMnjezP63u+wszu7bpMbdUR2ozs9vNbJOZbTSzS0ZY13Qz+7aZPVc97pLq/s9X69lkZveZmVX3P25md5rZM2b2fTNbamZfN7OXzewL1WMWmNmLZvZw9Zh/NLNpI6z7AjNbZ2bPmtkjZjbDe8LM7CQzW1lt63Nm9kvD6jPMbE21vI1mtjzYxzPN7Akz+28zW2VmfcH6rzCzR6vn4GUz+5Om/d3U9LjwU5KZXVpt4yYzu6267yozu33Y+v6muv1pM1tvZhvM7EtDwTSzATO7w8yek/SL3jo7Qikl3T9JA5KOl/SKpG5JN0m6pao9KOmi5sdW/71AjYCbGm9S35J0rqRfkPRE0+O/J2m+pE9KWi1pgqSTJG2V1DdsOz4p6f6mn7ur/85quu8rkn6tuv24pNuq25+V9LqkPklTJP2PpB5JCyQVSR+tHveApJuaxp8lqVfSv0uaXt3/R5I+HzxnX5P0+9XtCU3bOvT8TJR0fHW7V9Lm6rl6zz5KmiTpKUmzq/sukfRAdfsqSVeNsP4rJG2r9nGqpE3VviyQtKnpcSO+lk37Pq96LWZX2/xvkn6j+nlz03L+WdIvS1os6ZuSJlX3/62ky6rbRdKn2v37fLT/sh5ZVUrZI+nLkq4/yiEXVP++K+lZST8raVEp5buS5pjZPDP7eUk7SymvqvFCf7WU8k4ppV/SE5KWDlvmRkmfMLPbzOycUsru6v6Pmdl/mdlGSedLWtI05htNY18opWwrpQxK+qEabxKS9Gop5cnq9kPVtjT7iKQzJD1pZhskXS7p1GD/z5d0jyRV+7R7WN0k/bmZPS/pXyWdrMab1Ej7+DOSPiRpdbX+z0k6pVr2vaWUe1tsw+pSyo5SygFJXx9hv47GUkmPl1LeLKUckfSwpHNLKW9K+qGZfcTMetR4fZ+U9HFJZ0p6utrWj0s6rVrWO5L+aQzb0BYT270BNf2VGsH7u6b7jqj6eG9mx0maXN1vkr5YSvnSCMt5RNJFkuaqcQQ6KqWUH5jZhyVdKOkLZrZG0l+q8e59Vinl1eojXVfTsMHqv+823R76eej1GP4/bA//2dT4xb/0aLf1KPyuGkenM0sph83sFUldLfZxpRpvNKP96DjSfv3f61Xp0titkPQpSS9KWllKKdWfIH9fSrl5hMcfLKW8U2N9P1Fpj6ySVEp5W9I/SLqy6e5X1HgnlaRfV+MjmyStkvR7Q3/bmdnJZjanqn1N0m+rEdhHqvvWSrrEzCaY2Ww1PjKvb16/mc2TtL+U8pCk2yV9WD/+ZXurWtdFGr2fMrOhIPyOpP8YVv9PSR81s5+utmO6mZ1e3f6imf3mCMtcI+nq6jETzKx7WL1b0htVUD+m6kjdYh9fkjR7aBvNbJKZLVHsE2Y2y8ymqvHR9UlJ/Wp8sukxsymSlgXLWC/pV8yst/rb81I1PvVIjTeR5dV9K5r2+6Kh17paf/QppCNlP7JK0h2SPtP08/2SHq2+NPgXSfskqZTyHTNbLGld9X3PgKRPq/EL+oKZzZT0WillW7WclWp86fCcGkeAPyylbB+27p+TdLuZvSvpsKSrSym7zOx+Nf4m2y7p6THs00uSrjWzB9T4G/qe5mIp5U0zu0LSV6tfcKnxUfQH1TZ9Q+/1WUn3mdmVanz8u1rSuqb6w5K+WX10f0aNo1OrfTxkjS/x/roK/UQ1PuW8YGZXVds40kfh9Wp87DxF0kOllGckycz+rKq91rTeEZVStpnZH0t6TI1PGN8upTxa1Xaa2fclnVFKWV/d9z0z+5yk71SftA5LulbSj7z1dCKr/tBGhzCzBZK+VUr50BjHryql/Oox3ahjoHpzOauU8pnosRhZ6o/BeK9ODCqODY6sQBIcWYEkCCuQBGEFkhhV68bM+AN3DE4//XS37n1vcOTIEXfs4cOH3fqkSZPc+sSJ/q/A5MmTW9YGBwdb1iRp8+bNbh0jK6XYSPeP6gum92tYJ0zwJ1y88069/8llzZo1bv3dd99tWdu+fXhrd3T1uXPnuvXeXn/i0oIFC1rWtmzZ4o698MIL3fp4qnrpLXXyF6utwsrHYCAJwgokQViBJAgrkARhBZL4wHwb7H07eNxx/ntW9G3weeed59ZXrlzp1vfu3duy1tPT446Nti36pjtqDe3Zs6dlzfsWW5LOPvtstx59k+29Zp38bW5dfBsMJEdYgSQIK5AEYQWSIKxAEoQVSIKwAkl0zNkNo1kS4ynqF0auu+46tz4w4F/7yatHz8uOHTvc+vz58916NIXujTfeGPOyL73UP63xnXfe6da9fa87Uypjn5YjK5AEYQWSIKxAEoQVSIKwAkkQViCJD8wUuTpuuOE9F1j/f2688Ua3Hp2BcOPGjS1r/f397tjLLrvMrUfT/26+eaQrIf7Y8uXLW9b6+tyLnevUU/2LtUXjo7bU+xVT5IDkCCuQBGEFkiCsQBKEFUiCsAJJEFYgiY6ZIjfeLr744pa1K6+80h27aNEitx71qqMrtZ177rkta3fddZc7duvWrW79xRdfdOszZ85064sXL25Z27lzpzs2OtXo66+/7tYfe+yxlrVoet2qVavcesYLV3FkBZIgrEAShBVIgrACSRBWIAnCCiRBWIEk0sxnnTx5sltfsWKFW1+6dGnLWjTfdN++fW49mjManTbT63VG+71//363vnv3brd+wgknuHXvkpDR83bo0CG3Hj1vU6ZMGVNNkm699Va3fu+997r1dvZhmc8KJEdYgSQIK5AEYQWSIKxAEoQVSIKwAkmk6bMuW7bMrd99991u3bt0YdSziy4JGfVRo/HR5Qk9dbc9ev298VEPOFp21Kf1erxRjzbqjZ9zzjluvZ3oswLJEVYgCcIKJEFYgSQIK5AEYQWSSHMq0iVLlrj1wcHBMS87ap1E06Wi8V4LQorbEJ6oRRG1laL2iyd6zqP9ip5Xb9ujsdG6o3rU8moHjqxAEoQVSIKwAkkQViAJwgokQViBJAgrkESaPuvChQvdetR38/qJBw4ccMfWnYYWXfJx2rRpLWvRqUajHu706dNr1b1earTuaApc9Lx5654xY4Y7NnrNZs2a5dbfeustt94OHFmBJAgrkARhBZIgrEAShBVIgrACSRBWIIk0fdaoLxZdXtDrZUbzMg8ePOjWo55eT0+PW/f6vFH/uLu7261H46N983qh0fMWzaWN+rRdXV0ta5MmTaq17NNOO82t02cFMGaEFUiCsAJJEFYgCcIKJEFYgSQIK5BEmj7r3Llz3Xqdnl/Us4vmZUa9zGhOqtdnjXq00XmDo/5z1Av16tG5d6PzKUc93t7e3pa1aL+idZ9xxhluff369W69HTiyAkkQViAJwgokQViBJAgrkARhBZIgrEASafqsJ554oluPzv3ric5BOzAw4Najnt6OHTvcujdvs7+/3x0bzduMRD1mbz5r1F+OzpccnTd45syZLWvRfNNo2/r6+tx6J+LICiRBWIEkCCuQBGEFkiCsQBKEFUgiTesmajHs3bvXrXttguOPP37MYyWplOLWo7aT1+KI2j7RuqP2STTeEz0vdafQeW2paN1R62bevHluvRNxZAWSIKxAEoQVSIKwAkkQViAJwgokQViBJN43fdaI13ebOnWqOzaahhb1MqNTme7atatlLeoB1730YXSa1Khf6anbA65zGtRIdMnHTsSRFUiCsAJJEFYgCcIKJEFYgSQIK5AEYQWSSNNnjXqhUT/Q68tFPbuoXxitO+p1er3SOr3Io1l3tHzvkpJ1T0Ua9YijffNE811nz5495mW3C0dWIAnCCiRBWIEkCCuQBGEFkiCsQBKEFUiiY/qsUU+t7vxFr+9W9/y3keiSkh6vzynVPy9w1Ov0tv3QoUPu2Oi8wFOmTHHrBw8ebFmrc75jSeru7q41vh04sgJJEFYgCcIKJEFYgSQIK5AEYQWSIKxAEh3TZ50+fXqt8VGv1JvXGfXsorm00br37Nnj1gcHB8e87q6uLrce9TqjXqnXx6277GjfvB5z1B+ue87iTsSRFUiCsAJJEFYgCcIKJEFYgSQIK5BEx3x/PWfOHLceTVOL2ifeV/nR1/zRZRe9qVxSfMlH75Se0X7v2LHDrded/udtezTFLRK1frx2W7Tu6DWdPHmyW4+m0O3evdutjweOrEAShBVIgrACSRBWIAnCCiRBWIEkCCuQRMf0WU866aRa4+tctrHuFLm9e/e69Wg6ltdnjXq0dU81GvUbvT5tdMnH6PSy3tRAye+lRsuu219euHChW9+wYYNbHw8cWYEkCCuQBGEFkiCsQBKEFUiCsAJJEFYgiY7ps86fP9+tR3Mfo56fp6enx63v3LnTrUe9zjpzcaP9ivqkkeh59ZYf7XfUC/Xmq0r+nNFojnH0nEf1k08+2a3TZwXQEmEFkiCsQBKEFUiCsAJJEFYgCcIKJNExfdYZM2a49bqX8BsYGGhZW7dunTs26gFH/cQ6fdhobJ3+8tHw9i2aaxvNKY14z0vUZ33ttdfcerRt0fLbgSMrkARhBZIgrEAShBVIgrACSRBWIImOad309fW59ahNELVPvDbAsmXL3LHPP/+8W4+2rc40tmgqV9S6qdv68cZH2xa106LLNq5du7ZlbdGiRbWWHU0NPOWUU9x6O3BkBZIgrEAShBVIgrACSRBWIAnCCiRBWIEkOqbPOmvWLLf+9ttvu/Wop7d///6Wte3bt495rBT3G6OeXp0pcu0U9bYPHDjg1nt7e9366tWrW9bOP/98d2yd3wcp/n1sB46sQBKEFUiCsAJJEFYgCcIKJEFYgSQIK5BEx/RZo1ORRqeOnDZtmlvfvHnzqLdpSHT5v6iPOp690rrLjuazevVo7ODgoFuPTvHa39/fsrZ161Z37IIFC9z6rl273Hp3d7dbbweOrEAShBVIgrACSRBWIAnCCiRBWIEkCCuQRMf0WaO+Vt3LB7700ktjHnvPPfe49Wju5KFDh9x6ncs2jnef1Vt+NHbfvn1uvaury60/9dRTLWvRuZwXLlzo1qO5uFHfvh04sgJJEFYgCcIKJEFYgSQIK5AEYQWSIKxAEh3TZ924caNbX7x4ca3lb9u2bcxjr7/++lrrxrG3ZcsWt16ndy3V7+uPB46sQBKEFUiCsAJJEFYgCcIKJEFYgSQ6pnXjTYeSpMsvv9ytHz582K2vXbt21Ns0JJoCd+TIkTEv+4OszvP64IMPumOvueYat97T0+PWvctNtgtHViAJwgokQViBJAgrkARhBZIgrEAShBVIomP6rNu3b3fr0ZSoPXv2uPUNGzaMepuGjOclGz/IoktleqLTnD777LNuPZpy+fLLL496m8YbR1YgCcIKJEFYgSQIK5AEYQWSIKxAEoQVSMJG00M0szcl/Wj8Ngf4wDu1lDJ7pMKowgqgffgYDCRBWIEkCCuQBGEFkiCsQBKEFUiCsAJJEFYgCcIKJPG/JXVv/njN5UgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n=exitos[10] # elijo uno de los exitos \n",
    "\n",
    "fig = plt.figure(figsize = (4,4))\n",
    "plt.imshow(X_test_raw[n], interpolation='none', cmap=\"gray\") # plotea la imagen correspondiente\n",
    "plt.title(\"Nuevo sample, clase: {}\".format(categorias[y_test[n]]), fontsize = 10) # pongo el titulo a los plots con el nombre de la clase a la que pertenece la imagen y seteo el tamano de letra\n",
    "plt.xticks([]) # le saco los ticks en el eje X\n",
    "plt.yticks([]) # le saco los ticks en el eje Y\n",
    "\n",
    "\n",
    "knn = clf.kneighbors(X_test[n,:].reshape(1, -1), 10 , return_distance=False)[0] # esto me devuelve los indices de los K samples mas cercanos\n",
    "fig = plt.figure(figsize = (20,20), tight_layout = True) # seteo el tamano de la figura\n",
    "\n",
    "for n,i in enumerate(knn):\n",
    "    plt.subplot(1,10,n+1) # Voy a tener una matriz de 1x10 subplots y voy llenando en la iteracion i-esima el subplot i+1\n",
    "    plt.imshow(X_train_raw[i], interpolation='none', cmap=\"gray\") # va ploteando los K samples mas cercanos\n",
    "    plt.title(\"Training set, clase: {}\".format(categorias[y_train[i]]), fontsize = 10) # pongo el titulo a los plots con el nombre de la clase a la que pertenece la imagen y seteo el tamano de letra\n",
    "    plt.xticks([]) # le saco los ticks en el eje X\n",
    "    plt.yticks([]) # le saco los ticks en el eje Y\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"graficos/exitos_knn.svg\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x1008 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n=fracasos[10] # elijo uno de los fracasos \n",
    "\n",
    "fig = plt.figure(figsize = (4,4))\n",
    "plt.imshow(X_test_raw[n], interpolation='none', cmap=\"gray\") # plotea la imagen correspondiente\n",
    "plt.title(\"Nuevo sample, clase: {}\".format(categorias[y_test[n]]), fontsize = 10) # pongo el titulo a los plots con el nombre de la clase a la que pertenece la imagen y seteo el tamano de letra\n",
    "plt.xticks([]) # le saco los ticks en el eje X\n",
    "plt.yticks([]) # le saco los ticks en el eje Y\n",
    "\n",
    "\n",
    "knn = clf.kneighbors(X_test[n,:].reshape(1, -1) , return_distance=False)[0] # esto me devuelve los indices de los K samples mas cercanos\n",
    "fig = plt.figure(figsize = (14,14)) # seteo el tamano de la figura\n",
    "\n",
    "for n,i in enumerate(knn):\n",
    "    plt.subplot(1,5,n+1) # Voy a tener una matriz de 1x5 subplots y voy llenando en la iteracion i-esima el subplot i+1\n",
    "    plt.imshow(X_train_raw[i], interpolation='none', cmap=\"gray\") # va ploteando los K samples mas cercanos\n",
    "    plt.title(\"Training set, clase: {}\".format(categorias[y_train[i]]), fontsize = 10) # pongo el titulo a los plots con el nombre de la clase a la que pertenece la imagen y seteo el tamano de letra\n",
    "    plt.xticks([]) # le saco los ticks en el eje X\n",
    "    plt.yticks([]) # le saco los ticks en el eje Y\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"graficos/fracasos.svg\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEED = 42\n",
    "data = pd.read_csv(\"../data/fashion-mnist_train.csv\")\n",
    "X,y = data.drop(['label'], axis = 1), data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estudio el modelo con 1000 datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_mil = data[:1000]\n",
    "X_mil,y_mil = data_mil.drop(['label'], axis = 1), data_mil['label']\n",
    "X_train_mil, X_test_mil, y_train_mil, y_test_mil = train_test_split(X_mil, y_mil, test_size = 0.2, random_state = SEED)\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "X_train_mil = X_train_mil.reset_index(drop=True)\n",
    "\n",
    "y_train_mil.index = X_train_mil.index\n",
    "performance_accuracy = []\n",
    "\n",
    "fold_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1: \n",
      "-- training --\n",
      "0.22127938270568848\n",
      "-- train ok ! --\n",
      "fold 2: \n",
      "-- training --\n",
      "0.22288036346435547\n",
      "-- train ok ! --\n",
      "fold 3: \n",
      "-- training --\n",
      "0.2235877513885498\n",
      "-- train ok ! --\n",
      "fold 4: \n",
      "-- training --\n",
      "0.22270584106445312\n",
      "-- train ok ! --\n",
      "fold 5: \n",
      "-- training --\n",
      "0.22165489196777344\n",
      "-- train ok ! --\n",
      "0.70625\n",
      "fold 6: \n",
      "-- training --\n",
      "0.22216320037841797\n",
      "-- train ok ! --\n",
      "fold 7: \n",
      "-- training --\n",
      "0.22324705123901367\n",
      "-- train ok ! --\n",
      "fold 8: \n",
      "-- training --\n",
      "0.22394514083862305\n",
      "-- train ok ! --\n",
      "fold 9: \n",
      "-- training --\n",
      "0.22171306610107422\n",
      "-- train ok ! --\n",
      "fold 10: \n",
      "-- training --\n",
      "0.2233867645263672\n",
      "-- train ok ! --\n",
      "0.7449999999999999\n",
      "fold 11: \n",
      "-- training --\n",
      "0.22386455535888672\n",
      "-- train ok ! --\n",
      "fold 12: \n",
      "-- training --\n",
      "0.22336602210998535\n",
      "-- train ok ! --\n",
      "fold 13: \n",
      "-- training --\n",
      "0.23007535934448242\n",
      "-- train ok ! --\n",
      "fold 14: \n",
      "-- training --\n",
      "0.22172236442565918\n",
      "-- train ok ! --\n",
      "fold 15: \n",
      "-- training --\n",
      "0.22353696823120117\n",
      "-- train ok ! --\n",
      "0.7325\n",
      "fold 16: \n",
      "-- training --\n",
      "0.22436165809631348\n",
      "-- train ok ! --\n",
      "fold 17: \n",
      "-- training --\n",
      "0.2252061367034912\n",
      "-- train ok ! --\n",
      "fold 18: \n",
      "-- training --\n",
      "0.22843623161315918\n",
      "-- train ok ! --\n",
      "fold 19: \n",
      "-- training --\n",
      "0.22554826736450195\n",
      "-- train ok ! --\n",
      "fold 20: \n",
      "-- training --\n",
      "0.21776413917541504\n",
      "-- train ok ! --\n",
      "0.73625\n",
      "fold 21: \n",
      "-- training --\n",
      "0.22257399559020996\n",
      "-- train ok ! --\n",
      "fold 22: \n",
      "-- training --\n",
      "0.22322654724121094\n",
      "-- train ok ! --\n",
      "fold 23: \n",
      "-- training --\n",
      "0.2268075942993164\n",
      "-- train ok ! --\n",
      "fold 24: \n",
      "-- training --\n",
      "0.2249002456665039\n",
      "-- train ok ! --\n",
      "fold 25: \n",
      "-- training --\n",
      "0.22630548477172852\n",
      "-- train ok ! --\n",
      "0.7112499999999999\n",
      "fold 26: \n",
      "-- training --\n",
      "0.2227632999420166\n",
      "-- train ok ! --\n",
      "fold 27: \n",
      "-- training --\n",
      "0.2290792465209961\n",
      "-- train ok ! --\n",
      "fold 28: \n",
      "-- training --\n",
      "0.22705674171447754\n",
      "-- train ok ! --\n",
      "fold 29: \n",
      "-- training --\n",
      "0.22388339042663574\n",
      "-- train ok ! --\n",
      "fold 30: \n",
      "-- training --\n",
      "0.2240128517150879\n",
      "-- train ok ! --\n",
      "0.70625\n",
      "fold 31: \n",
      "-- training --\n",
      "0.22580671310424805\n",
      "-- train ok ! --\n",
      "fold 32: \n",
      "-- training --\n",
      "0.2315070629119873\n",
      "-- train ok ! --\n",
      "fold 33: \n",
      "-- training --\n",
      "0.23061919212341309\n",
      "-- train ok ! --\n",
      "fold 34: \n",
      "-- training --\n",
      "0.22999906539916992\n",
      "-- train ok ! --\n",
      "fold 35: \n",
      "-- training --\n",
      "0.22986483573913574\n",
      "-- train ok ! --\n",
      "0.7049999999999998\n",
      "fold 36: \n",
      "-- training --\n",
      "0.2306990623474121\n",
      "-- train ok ! --\n",
      "fold 37: \n",
      "-- training --\n",
      "0.22905445098876953\n",
      "-- train ok ! --\n",
      "fold 38: \n",
      "-- training --\n",
      "0.23130130767822266\n",
      "-- train ok ! --\n",
      "fold 39: \n",
      "-- training --\n",
      "0.2255399227142334\n",
      "-- train ok ! --\n",
      "fold 40: \n",
      "-- training --\n",
      "0.22612714767456055\n",
      "-- train ok ! --\n",
      "0.7062499999999999\n",
      "fold 41: \n",
      "-- training --\n",
      "0.23090600967407227\n",
      "-- train ok ! --\n",
      "fold 42: \n",
      "-- training --\n",
      "0.2309246063232422\n",
      "-- train ok ! --\n",
      "fold 43: \n",
      "-- training --\n",
      "0.23247337341308594\n",
      "-- train ok ! --\n",
      "fold 44: \n",
      "-- training --\n",
      "0.2290942668914795\n",
      "-- train ok ! --\n",
      "fold 45: \n",
      "-- training --\n",
      "0.2287449836730957\n",
      "-- train ok ! --\n",
      "0.6950000000000001\n",
      "fold 46: \n",
      "-- training --\n",
      "0.2214946746826172\n",
      "-- train ok ! --\n",
      "fold 47: \n",
      "-- training --\n",
      "0.22439885139465332\n",
      "-- train ok ! --\n",
      "fold 48: \n",
      "-- training --\n",
      "0.22850966453552246\n",
      "-- train ok ! --\n",
      "fold 49: \n",
      "-- training --\n",
      "0.2257213592529297\n",
      "-- train ok ! --\n",
      "fold 50: \n",
      "-- training --\n",
      "0.22611260414123535\n",
      "-- train ok ! --\n",
      "0.6849999999999999\n",
      "fold 51: \n",
      "-- training --\n",
      "0.22312092781066895\n",
      "-- train ok ! --\n",
      "fold 52: \n",
      "-- training --\n",
      "0.22557401657104492\n",
      "-- train ok ! --\n",
      "fold 53: \n",
      "-- training --\n",
      "0.22719216346740723\n",
      "-- train ok ! --\n",
      "fold 54: \n",
      "-- training --\n",
      "0.22694897651672363\n",
      "-- train ok ! --\n",
      "fold 55: \n",
      "-- training --\n",
      "0.22754979133605957\n",
      "-- train ok ! --\n",
      "0.66875\n",
      "fold 56: \n",
      "-- training --\n",
      "0.22284626960754395\n",
      "-- train ok ! --\n",
      "fold 57: \n",
      "-- training --\n",
      "0.22685742378234863\n",
      "-- train ok ! --\n",
      "fold 58: \n",
      "-- training --\n",
      "0.22751450538635254\n",
      "-- train ok ! --\n",
      "fold 59: \n",
      "-- training --\n",
      "0.22500967979431152\n",
      "-- train ok ! --\n",
      "fold 60: \n",
      "-- training --\n",
      "0.22634482383728027\n",
      "-- train ok ! --\n",
      "0.6775000000000001\n",
      "fold 61: \n",
      "-- training --\n",
      "0.22461771965026855\n",
      "-- train ok ! --\n",
      "fold 62: \n",
      "-- training --\n",
      "0.22573137283325195\n",
      "-- train ok ! --\n",
      "fold 63: \n",
      "-- training --\n",
      "0.22649693489074707\n",
      "-- train ok ! --\n",
      "fold 64: \n",
      "-- training --\n",
      "0.2284085750579834\n",
      "-- train ok ! --\n",
      "fold 65: \n",
      "-- training --\n",
      "0.22706818580627441\n",
      "-- train ok ! --\n",
      "0.66625\n",
      "fold 66: \n",
      "-- training --\n",
      "0.22469711303710938\n",
      "-- train ok ! --\n",
      "fold 67: \n",
      "-- training --\n",
      "0.22661113739013672\n",
      "-- train ok ! --\n",
      "fold 68: \n",
      "-- training --\n",
      "0.22851991653442383\n",
      "-- train ok ! --\n",
      "fold 69: \n",
      "-- training --\n",
      "0.22500348091125488\n",
      "-- train ok ! --\n",
      "fold 70: \n",
      "-- training --\n",
      "0.22678637504577637\n",
      "-- train ok ! --\n",
      "0.65625\n",
      "fold 71: \n",
      "-- training --\n",
      "0.22522473335266113\n",
      "-- train ok ! --\n",
      "fold 72: \n",
      "-- training --\n",
      "0.22709345817565918\n",
      "-- train ok ! --\n",
      "fold 73: \n",
      "-- training --\n",
      "0.22651004791259766\n",
      "-- train ok ! --\n",
      "fold 74: \n",
      "-- training --\n",
      "0.22609877586364746\n",
      "-- train ok ! --\n",
      "fold 75: \n",
      "-- training --\n",
      "0.22424650192260742\n",
      "-- train ok ! --\n",
      "0.6424999999999998\n",
      "fold 76: \n",
      "-- training --\n",
      "0.2227480411529541\n",
      "-- train ok ! --\n",
      "fold 77: \n",
      "-- training --\n",
      "0.2243640422821045\n",
      "-- train ok ! --\n",
      "fold 78: \n",
      "-- training --\n",
      "0.2284095287322998\n",
      "-- train ok ! --\n",
      "fold 79: \n",
      "-- training --\n",
      "0.22536063194274902\n",
      "-- train ok ! --\n",
      "fold 80: \n",
      "-- training --\n",
      "0.22487640380859375\n",
      "-- train ok ! --\n",
      "0.63125\n",
      "fold 81: \n",
      "-- training --\n",
      "0.22483086585998535\n",
      "-- train ok ! --\n",
      "fold 82: \n",
      "-- training --\n",
      "0.2266709804534912\n",
      "-- train ok ! --\n",
      "fold 83: \n",
      "-- training --\n",
      "0.22613930702209473\n",
      "-- train ok ! --\n",
      "fold 84: \n",
      "-- training --\n",
      "0.22458577156066895\n",
      "-- train ok ! --\n",
      "fold 85: \n",
      "-- training --\n",
      "0.22595763206481934\n",
      "-- train ok ! --\n",
      "0.63\n",
      "fold 86: \n",
      "-- training --\n",
      "0.22411394119262695\n",
      "-- train ok ! --\n",
      "fold 87: \n",
      "-- training --\n",
      "0.2252488136291504\n",
      "-- train ok ! --\n",
      "fold 88: \n",
      "-- training --\n",
      "0.22799944877624512\n",
      "-- train ok ! --\n",
      "fold 89: \n",
      "-- training --\n",
      "0.22480344772338867\n",
      "-- train ok ! --\n",
      "fold 90: \n",
      "-- training --\n",
      "0.22532176971435547\n",
      "-- train ok ! --\n",
      "0.6175\n",
      "fold 91: \n",
      "-- training --\n",
      "0.2237546443939209\n",
      "-- train ok ! --\n",
      "fold 92: \n",
      "-- training --\n",
      "0.22536659240722656\n",
      "-- train ok ! --\n",
      "fold 93: \n",
      "-- training --\n",
      "0.2269577980041504\n",
      "-- train ok ! --\n",
      "fold 94: \n",
      "-- training --\n",
      "0.22457671165466309\n",
      "-- train ok ! --\n",
      "fold 95: \n",
      "-- training --\n",
      "0.22640419006347656\n",
      "-- train ok ! --\n",
      "0.6049999999999999\n",
      "fold 96: \n",
      "-- training --\n",
      "0.22406816482543945\n",
      "-- train ok ! --\n",
      "fold 97: \n",
      "-- training --\n",
      "0.22673988342285156\n",
      "-- train ok ! --\n",
      "fold 98: \n",
      "-- training --\n",
      "0.22705769538879395\n",
      "-- train ok ! --\n",
      "fold 99: \n",
      "-- training --\n",
      "0.22432279586791992\n",
      "-- train ok ! --\n",
      "fold 100: \n",
      "-- training --\n",
      "0.22549915313720703\n",
      "-- train ok ! --\n",
      "0.61125\n",
      "fold 101: \n",
      "-- training --\n",
      "0.22336769104003906\n",
      "-- train ok ! --\n",
      "fold 102: \n",
      "-- training --\n",
      "0.22539830207824707\n",
      "-- train ok ! --\n",
      "fold 103: \n",
      "-- training --\n",
      "0.22718167304992676\n",
      "-- train ok ! --\n",
      "fold 104: \n",
      "-- training --\n",
      "0.22465944290161133\n",
      "-- train ok ! --\n",
      "fold 105: \n",
      "-- training --\n",
      "0.22457480430603027\n",
      "-- train ok ! --\n",
      "0.62\n",
      "fold 106: \n",
      "-- training --\n",
      "0.22371435165405273\n",
      "-- train ok ! --\n",
      "fold 107: \n",
      "-- training --\n",
      "0.22481608390808105\n",
      "-- train ok ! --\n",
      "fold 108: \n",
      "-- training --\n",
      "0.2271740436553955\n",
      "-- train ok ! --\n",
      "fold 109: \n",
      "-- training --\n",
      "0.22590923309326172\n",
      "-- train ok ! --\n",
      "fold 110: \n",
      "-- training --\n",
      "0.22486042976379395\n",
      "-- train ok ! --\n",
      "0.61375\n",
      "fold 111: \n",
      "-- training --\n",
      "0.2234513759613037\n",
      "-- train ok ! --\n",
      "fold 112: \n",
      "-- training --\n",
      "0.22524213790893555\n",
      "-- train ok ! --\n",
      "fold 113: \n",
      "-- training --\n",
      "0.22728633880615234\n",
      "-- train ok ! --\n",
      "fold 114: \n",
      "-- training --\n",
      "0.22574973106384277\n",
      "-- train ok ! --\n",
      "fold 115: \n",
      "-- training --\n",
      "0.22549676895141602\n",
      "-- train ok ! --\n",
      "0.6025\n",
      "fold 116: \n",
      "-- training --\n",
      "0.2225954532623291\n",
      "-- train ok ! --\n",
      "fold 117: \n",
      "-- training --\n",
      "0.2257392406463623\n",
      "-- train ok ! --\n",
      "fold 118: \n",
      "-- training --\n",
      "0.22734761238098145\n",
      "-- train ok ! --\n",
      "fold 119: \n",
      "-- training --\n",
      "0.22441983222961426\n",
      "-- train ok ! --\n",
      "fold 120: \n",
      "-- training --\n",
      "0.22683072090148926\n",
      "-- train ok ! --\n",
      "0.58375\n",
      "fold 121: \n",
      "-- training --\n",
      "0.22375035285949707\n",
      "-- train ok ! --\n",
      "fold 122: \n",
      "-- training --\n",
      "0.2253894805908203\n",
      "-- train ok ! --\n",
      "fold 123: \n",
      "-- training --\n",
      "0.22694778442382812\n",
      "-- train ok ! --\n",
      "fold 124: \n",
      "-- training --\n",
      "0.2249457836151123\n",
      "-- train ok ! --\n",
      "fold 125: \n",
      "-- training --\n",
      "0.22539305686950684\n",
      "-- train ok ! --\n",
      "0.6050000000000001\n",
      "fold 126: \n",
      "-- training --\n",
      "0.22380542755126953\n",
      "-- train ok ! --\n",
      "fold 127: \n",
      "-- training --\n",
      "0.22567439079284668\n",
      "-- train ok ! --\n",
      "fold 128: \n",
      "-- training --\n",
      "0.22887206077575684\n",
      "-- train ok ! --\n",
      "fold 129: \n",
      "-- training --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2242908477783203\n",
      "-- train ok ! --\n",
      "fold 130: \n",
      "-- training --\n",
      "0.22474074363708496\n",
      "-- train ok ! --\n",
      "0.5974999999999999\n",
      "fold 131: \n",
      "-- training --\n",
      "0.2239973545074463\n",
      "-- train ok ! --\n",
      "fold 132: \n",
      "-- training --\n",
      "0.2263166904449463\n",
      "-- train ok ! --\n",
      "fold 133: \n",
      "-- training --\n",
      "0.22771883010864258\n",
      "-- train ok ! --\n",
      "fold 134: \n",
      "-- training --\n",
      "0.22431683540344238\n",
      "-- train ok ! --\n",
      "fold 135: \n",
      "-- training --\n",
      "0.22487449645996094\n",
      "-- train ok ! --\n",
      "0.585\n",
      "fold 136: \n",
      "-- training --\n",
      "0.22430992126464844\n",
      "-- train ok ! --\n",
      "fold 137: \n",
      "-- training --\n",
      "0.22492384910583496\n",
      "-- train ok ! --\n",
      "fold 138: \n",
      "-- training --\n",
      "0.2276914119720459\n",
      "-- train ok ! --\n",
      "fold 139: \n",
      "-- training --\n",
      "0.22565221786499023\n",
      "-- train ok ! --\n",
      "fold 140: \n",
      "-- training --\n",
      "0.22580432891845703\n",
      "-- train ok ! --\n",
      "0.56125\n",
      "fold 141: \n",
      "-- training --\n",
      "0.2246863842010498\n",
      "-- train ok ! --\n",
      "fold 142: \n",
      "-- training --\n",
      "0.22583270072937012\n",
      "-- train ok ! --\n",
      "fold 143: \n",
      "-- training --\n",
      "0.22711181640625\n",
      "-- train ok ! --\n",
      "fold 144: \n",
      "-- training --\n",
      "0.22591590881347656\n",
      "-- train ok ! --\n",
      "fold 145: \n",
      "-- training --\n",
      "0.22571420669555664\n",
      "-- train ok ! --\n",
      "0.5824999999999999\n",
      "fold 146: \n",
      "-- training --\n",
      "0.223876953125\n",
      "-- train ok ! --\n",
      "fold 147: \n",
      "-- training --\n",
      "0.22655367851257324\n",
      "-- train ok ! --\n",
      "fold 148: \n",
      "-- training --\n",
      "0.22745060920715332\n",
      "-- train ok ! --\n",
      "fold 149: \n",
      "-- training --\n",
      "0.2260885238647461\n",
      "-- train ok ! --\n",
      "fold 150: \n",
      "-- training --\n",
      "0.22600483894348145\n",
      "-- train ok ! --\n",
      "0.5862499999999999\n",
      "fold 151: \n",
      "-- training --\n",
      "0.22335076332092285\n",
      "-- train ok ! --\n",
      "fold 152: \n",
      "-- training --\n",
      "0.22609162330627441\n",
      "-- train ok ! --\n",
      "fold 153: \n",
      "-- training --\n",
      "0.2283780574798584\n",
      "-- train ok ! --\n",
      "fold 154: \n",
      "-- training --\n",
      "0.2249593734741211\n",
      "-- train ok ! --\n",
      "fold 155: \n",
      "-- training --\n",
      "0.22635293006896973\n",
      "-- train ok ! --\n",
      "0.5687499999999999\n",
      "fold 156: \n",
      "-- training --\n",
      "0.22391343116760254\n",
      "-- train ok ! --\n",
      "fold 157: \n",
      "-- training --\n",
      "0.22677254676818848\n",
      "-- train ok ! --\n",
      "fold 158: \n",
      "-- training --\n",
      "0.2272806167602539\n",
      "-- train ok ! --\n",
      "fold 159: \n",
      "-- training --\n",
      "0.2252519130706787\n",
      "-- train ok ! --\n",
      "fold 160: \n",
      "-- training --\n",
      "0.2266526222229004\n",
      "-- train ok ! --\n",
      "0.56125\n",
      "fold 161: \n",
      "-- training --\n",
      "0.22411417961120605\n",
      "-- train ok ! --\n",
      "fold 162: \n",
      "-- training --\n",
      "0.22599458694458008\n",
      "-- train ok ! --\n",
      "fold 163: \n",
      "-- training --\n",
      "0.22833585739135742\n",
      "-- train ok ! --\n",
      "fold 164: \n",
      "-- training --\n",
      "0.2267453670501709\n",
      "-- train ok ! --\n",
      "fold 165: \n",
      "-- training --\n",
      "0.2263188362121582\n",
      "-- train ok ! --\n",
      "0.5650000000000001\n",
      "fold 166: \n",
      "-- training --\n",
      "0.22515511512756348\n",
      "-- train ok ! --\n",
      "fold 167: \n",
      "-- training --\n",
      "0.2255387306213379\n",
      "-- train ok ! --\n",
      "fold 168: \n",
      "-- training --\n",
      "0.22803425788879395\n",
      "-- train ok ! --\n",
      "fold 169: \n",
      "-- training --\n",
      "0.22528409957885742\n",
      "-- train ok ! --\n",
      "fold 170: \n",
      "-- training --\n",
      "0.22632336616516113\n",
      "-- train ok ! --\n",
      "0.54125\n",
      "fold 171: \n",
      "-- training --\n",
      "0.2240440845489502\n",
      "-- train ok ! --\n",
      "fold 172: \n",
      "-- training --\n",
      "0.22742009162902832\n",
      "-- train ok ! --\n",
      "fold 173: \n",
      "-- training --\n",
      "0.22765731811523438\n",
      "-- train ok ! --\n",
      "fold 174: \n",
      "-- training --\n",
      "0.22614836692810059\n",
      "-- train ok ! --\n",
      "fold 175: \n",
      "-- training --\n",
      "0.2265937328338623\n",
      "-- train ok ! --\n",
      "0.56\n",
      "fold 176: \n",
      "-- training --\n",
      "0.22363901138305664\n",
      "-- train ok ! --\n",
      "fold 177: \n",
      "-- training --\n",
      "0.22619175910949707\n",
      "-- train ok ! --\n",
      "fold 178: \n",
      "-- training --\n",
      "0.23093175888061523\n",
      "-- train ok ! --\n",
      "fold 179: \n",
      "-- training --\n",
      "0.2278914451599121\n",
      "-- train ok ! --\n",
      "fold 180: \n",
      "-- training --\n",
      "0.22914886474609375\n",
      "-- train ok ! --\n",
      "0.55\n",
      "fold 181: \n",
      "-- training --\n",
      "0.22446990013122559\n",
      "-- train ok ! --\n",
      "fold 182: \n",
      "-- training --\n",
      "0.2270514965057373\n",
      "-- train ok ! --\n",
      "fold 183: \n",
      "-- training --\n",
      "0.2290804386138916\n",
      "-- train ok ! --\n",
      "fold 184: \n",
      "-- training --\n",
      "0.22651052474975586\n",
      "-- train ok ! --\n",
      "fold 185: \n",
      "-- training --\n",
      "0.2261807918548584\n",
      "-- train ok ! --\n",
      "0.5362500000000001\n",
      "fold 186: \n",
      "-- training --\n",
      "0.22517180442810059\n",
      "-- train ok ! --\n",
      "fold 187: \n",
      "-- training --\n",
      "0.22662639617919922\n",
      "-- train ok ! --\n",
      "fold 188: \n",
      "-- training --\n",
      "0.2298753261566162\n",
      "-- train ok ! --\n",
      "fold 189: \n",
      "-- training --\n",
      "0.22547245025634766\n",
      "-- train ok ! --\n",
      "fold 190: \n",
      "-- training --\n",
      "0.22748303413391113\n",
      "-- train ok ! --\n",
      "0.5375\n",
      "fold 191: \n",
      "-- training --\n",
      "0.2254314422607422\n",
      "-- train ok ! --\n",
      "fold 192: \n",
      "-- training --\n",
      "0.229766845703125\n",
      "-- train ok ! --\n",
      "fold 193: \n",
      "-- training --\n",
      "0.22815990447998047\n",
      "-- train ok ! --\n",
      "fold 194: \n",
      "-- training --\n",
      "0.230635404586792\n",
      "-- train ok ! --\n",
      "fold 195: \n",
      "-- training --\n",
      "0.23517179489135742\n",
      "-- train ok ! --\n",
      "0.53625\n",
      "fold 196: \n",
      "-- training --\n",
      "0.2298426628112793\n",
      "-- train ok ! --\n",
      "fold 197: \n",
      "-- training --\n",
      "0.2316594123840332\n",
      "-- train ok ! --\n",
      "fold 198: \n",
      "-- training --\n",
      "0.2333383560180664\n",
      "-- train ok ! --\n",
      "fold 199: \n",
      "-- training --\n",
      "0.2264251708984375\n",
      "-- train ok ! --\n",
      "fold 200: \n",
      "-- training --\n",
      "0.23055768013000488\n",
      "-- train ok ! --\n",
      "0.52375\n",
      "fold 201: \n",
      "-- training --\n",
      "0.22664189338684082\n",
      "-- train ok ! --\n",
      "fold 202: \n",
      "-- training --\n",
      "0.22819805145263672\n",
      "-- train ok ! --\n",
      "fold 203: \n",
      "-- training --\n",
      "0.2287919521331787\n",
      "-- train ok ! --\n",
      "fold 204: \n",
      "-- training --\n",
      "0.22736167907714844\n",
      "-- train ok ! --\n",
      "fold 205: \n",
      "-- training --\n",
      "0.22886323928833008\n",
      "-- train ok ! --\n",
      "0.5075000000000001\n",
      "fold 206: \n",
      "-- training --\n",
      "0.22617602348327637\n",
      "-- train ok ! --\n",
      "fold 207: \n",
      "-- training --\n",
      "0.22939085960388184\n",
      "-- train ok ! --\n",
      "fold 208: \n",
      "-- training --\n",
      "0.23097443580627441\n",
      "-- train ok ! --\n",
      "fold 209: \n",
      "-- training --\n",
      "0.22932815551757812\n",
      "-- train ok ! --\n",
      "fold 210: \n",
      "-- training --\n",
      "0.22913026809692383\n",
      "-- train ok ! --\n",
      "0.4925\n",
      "fold 211: \n",
      "-- training --\n",
      "0.22849607467651367\n",
      "-- train ok ! --\n",
      "fold 212: \n",
      "-- training --\n",
      "0.2329709529876709\n",
      "-- train ok ! --\n",
      "fold 213: \n",
      "-- training --\n",
      "0.23564696311950684\n",
      "-- train ok ! --\n",
      "fold 214: \n",
      "-- training --\n",
      "0.2366468906402588\n",
      "-- train ok ! --\n",
      "fold 215: \n",
      "-- training --\n",
      "0.23404145240783691\n",
      "-- train ok ! --\n",
      "0.5050000000000001\n",
      "fold 216: \n",
      "-- training --\n",
      "0.22957134246826172\n",
      "-- train ok ! --\n",
      "fold 217: \n",
      "-- training --\n",
      "0.23215341567993164\n",
      "-- train ok ! --\n",
      "fold 218: \n",
      "-- training --\n",
      "0.23423528671264648\n",
      "-- train ok ! --\n",
      "fold 219: \n",
      "-- training --\n",
      "0.23104476928710938\n",
      "-- train ok ! --\n",
      "fold 220: \n",
      "-- training --\n",
      "0.23196053504943848\n",
      "-- train ok ! --\n",
      "0.48250000000000004\n",
      "fold 221: \n",
      "-- training --\n",
      "0.2368755340576172\n",
      "-- train ok ! --\n",
      "fold 222: \n",
      "-- training --\n",
      "0.2287001609802246\n",
      "-- train ok ! --\n",
      "fold 223: \n",
      "-- training --\n",
      "0.23497557640075684\n",
      "-- train ok ! --\n",
      "fold 224: \n",
      "-- training --\n",
      "0.22834253311157227\n",
      "-- train ok ! --\n",
      "fold 225: \n",
      "-- training --\n",
      "0.22841358184814453\n",
      "-- train ok ! --\n",
      "0.48500000000000004\n",
      "fold 226: \n",
      "-- training --\n",
      "0.2271885871887207\n",
      "-- train ok ! --\n",
      "fold 227: \n",
      "-- training --\n",
      "0.22972702980041504\n",
      "-- train ok ! --\n",
      "fold 228: \n",
      "-- training --\n",
      "0.2316296100616455\n",
      "-- train ok ! --\n",
      "fold 229: \n",
      "-- training --\n",
      "0.22857308387756348\n",
      "-- train ok ! --\n",
      "fold 230: \n",
      "-- training --\n",
      "0.22942161560058594\n",
      "-- train ok ! --\n",
      "0.4875\n",
      "fold 231: \n",
      "-- training --\n",
      "0.22921204566955566\n",
      "-- train ok ! --\n",
      "fold 232: \n",
      "-- training --\n",
      "0.22893548011779785\n",
      "-- train ok ! --\n",
      "fold 233: \n",
      "-- training --\n",
      "0.2310492992401123\n",
      "-- train ok ! --\n",
      "fold 234: \n",
      "-- training --\n",
      "0.22954773902893066\n",
      "-- train ok ! --\n",
      "fold 235: \n",
      "-- training --\n",
      "0.22893142700195312\n",
      "-- train ok ! --\n",
      "0.49000000000000005\n",
      "fold 236: \n",
      "-- training --\n",
      "0.22677302360534668\n",
      "-- train ok ! --\n",
      "fold 237: \n",
      "-- training --\n",
      "0.2286546230316162\n",
      "-- train ok ! --\n",
      "fold 238: \n",
      "-- training --\n",
      "0.23164701461791992\n",
      "-- train ok ! --\n",
      "fold 239: \n",
      "-- training --\n",
      "0.23107004165649414\n",
      "-- train ok ! --\n",
      "fold 240: \n",
      "-- training --\n",
      "0.22801542282104492\n",
      "-- train ok ! --\n",
      "0.46875\n",
      "fold 241: \n",
      "-- training --\n",
      "0.22781848907470703\n",
      "-- train ok ! --\n",
      "fold 242: \n",
      "-- training --\n",
      "0.22904610633850098\n",
      "-- train ok ! --\n",
      "fold 243: \n",
      "-- training --\n",
      "0.23165011405944824\n",
      "-- train ok ! --\n",
      "fold 244: \n",
      "-- training --\n",
      "0.22908806800842285\n",
      "-- train ok ! --\n",
      "fold 245: \n",
      "-- training --\n",
      "0.22880887985229492\n",
      "-- train ok ! --\n",
      "0.47874999999999995\n",
      "fold 246: \n",
      "-- training --\n",
      "0.2282559871673584\n",
      "-- train ok ! --\n",
      "fold 247: \n",
      "-- training --\n",
      "0.22943878173828125\n",
      "-- train ok ! --\n",
      "fold 248: \n",
      "-- training --\n",
      "0.23151111602783203\n",
      "-- train ok ! --\n",
      "fold 249: \n",
      "-- training --\n",
      "0.22912096977233887\n",
      "-- train ok ! --\n",
      "fold 250: \n",
      "-- training --\n",
      "0.23763632774353027\n",
      "-- train ok ! --\n",
      "0.47375\n",
      "fold 251: \n",
      "-- training --\n",
      "0.2332448959350586\n",
      "-- train ok ! --\n",
      "fold 252: \n",
      "-- training --\n",
      "0.22858023643493652\n",
      "-- train ok ! --\n",
      "fold 253: \n",
      "-- training --\n",
      "0.23641228675842285\n",
      "-- train ok ! --\n",
      "fold 254: \n",
      "-- training --\n",
      "0.23413920402526855\n",
      "-- train ok ! --\n",
      "fold 255: \n",
      "-- training --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23055624961853027\n",
      "-- train ok ! --\n",
      "0.47125000000000006\n",
      "fold 256: \n",
      "-- training --\n",
      "0.2297224998474121\n",
      "-- train ok ! --\n",
      "fold 257: \n",
      "-- training --\n",
      "0.23209881782531738\n",
      "-- train ok ! --\n",
      "fold 258: \n",
      "-- training --\n",
      "0.2320263385772705\n",
      "-- train ok ! --\n",
      "fold 259: \n",
      "-- training --\n",
      "0.2288494110107422\n",
      "-- train ok ! --\n",
      "fold 260: \n",
      "-- training --\n",
      "0.23285818099975586\n",
      "-- train ok ! --\n",
      "0.46499999999999997\n",
      "fold 261: \n",
      "-- training --\n",
      "0.23093104362487793\n",
      "-- train ok ! --\n",
      "fold 262: \n",
      "-- training --\n",
      "0.23166465759277344\n",
      "-- train ok ! --\n",
      "fold 263: \n",
      "-- training --\n",
      "0.23258590698242188\n",
      "-- train ok ! --\n",
      "fold 264: \n",
      "-- training --\n",
      "0.2283937931060791\n",
      "-- train ok ! --\n",
      "fold 265: \n",
      "-- training --\n",
      "0.23077988624572754\n",
      "-- train ok ! --\n",
      "0.4625\n",
      "fold 266: \n",
      "-- training --\n",
      "0.2277214527130127\n",
      "-- train ok ! --\n",
      "fold 267: \n",
      "-- training --\n",
      "0.23094582557678223\n",
      "-- train ok ! --\n",
      "fold 268: \n",
      "-- training --\n",
      "0.23206472396850586\n",
      "-- train ok ! --\n",
      "fold 269: \n",
      "-- training --\n",
      "0.22949743270874023\n",
      "-- train ok ! --\n",
      "fold 270: \n",
      "-- training --\n",
      "0.23291301727294922\n",
      "-- train ok ! --\n",
      "0.45999999999999996\n",
      "fold 271: \n",
      "-- training --\n",
      "0.22855281829833984\n",
      "-- train ok ! --\n",
      "fold 272: \n",
      "-- training --\n",
      "0.22924542427062988\n",
      "-- train ok ! --\n",
      "fold 273: \n",
      "-- training --\n",
      "0.23187708854675293\n",
      "-- train ok ! --\n",
      "fold 274: \n",
      "-- training --\n",
      "0.22979187965393066\n",
      "-- train ok ! --\n",
      "fold 275: \n",
      "-- training --\n",
      "0.2295832633972168\n",
      "-- train ok ! --\n",
      "0.45875000000000005\n",
      "fold 276: \n",
      "-- training --\n",
      "0.22659850120544434\n",
      "-- train ok ! --\n",
      "fold 277: \n",
      "-- training --\n",
      "0.22988247871398926\n",
      "-- train ok ! --\n",
      "fold 278: \n",
      "-- training --\n",
      "0.23122668266296387\n",
      "-- train ok ! --\n",
      "fold 279: \n",
      "-- training --\n",
      "0.22883224487304688\n",
      "-- train ok ! --\n",
      "fold 280: \n",
      "-- training --\n",
      "0.22985482215881348\n",
      "-- train ok ! --\n",
      "0.4375\n",
      "fold 281: \n",
      "-- training --\n",
      "0.2292191982269287\n",
      "-- train ok ! --\n",
      "fold 282: \n",
      "-- training --\n",
      "0.22967076301574707\n",
      "-- train ok ! --\n",
      "fold 283: \n",
      "-- training --\n",
      "0.22970175743103027\n",
      "-- train ok ! --\n",
      "fold 284: \n",
      "-- training --\n",
      "0.2267904281616211\n",
      "-- train ok ! --\n",
      "fold 285: \n",
      "-- training --\n",
      "0.2282698154449463\n",
      "-- train ok ! --\n",
      "0.44625000000000004\n",
      "fold 286: \n",
      "-- training --\n",
      "0.22702693939208984\n",
      "-- train ok ! --\n",
      "fold 287: \n",
      "-- training --\n",
      "0.2294778823852539\n",
      "-- train ok ! --\n",
      "fold 288: \n",
      "-- training --\n",
      "0.229905366897583\n",
      "-- train ok ! --\n",
      "fold 289: \n",
      "-- training --\n",
      "0.22745013236999512\n",
      "-- train ok ! --\n",
      "fold 290: \n",
      "-- training --\n",
      "0.2291884422302246\n",
      "-- train ok ! --\n",
      "0.45125000000000004\n",
      "fold 291: \n",
      "-- training --\n",
      "0.2262256145477295\n",
      "-- train ok ! --\n",
      "fold 292: \n",
      "-- training --\n",
      "0.2283632755279541\n",
      "-- train ok ! --\n",
      "fold 293: \n",
      "-- training --\n",
      "0.2301955223083496\n",
      "-- train ok ! --\n",
      "fold 294: \n",
      "-- training --\n",
      "0.22799277305603027\n",
      "-- train ok ! --\n",
      "fold 295: \n",
      "-- training --\n",
      "0.22989940643310547\n",
      "-- train ok ! --\n",
      "0.44750000000000006\n",
      "fold 296: \n",
      "-- training --\n",
      "0.2266230583190918\n",
      "-- train ok ! --\n",
      "fold 297: \n",
      "-- training --\n",
      "0.2288815975189209\n",
      "-- train ok ! --\n",
      "fold 298: \n",
      "-- training --\n",
      "0.23032212257385254\n",
      "-- train ok ! --\n",
      "fold 299: \n",
      "-- training --\n",
      "0.22842812538146973\n",
      "-- train ok ! --\n",
      "fold 300: \n",
      "-- training --\n",
      "0.21901774406433105\n",
      "-- train ok ! --\n",
      "0.4375\n",
      "fold 301: \n",
      "-- training --\n",
      "0.2041325569152832\n",
      "-- train ok ! --\n",
      "fold 302: \n",
      "-- training --\n",
      "0.23004889488220215\n",
      "-- train ok ! --\n",
      "fold 303: \n",
      "-- training --\n",
      "0.2310938835144043\n",
      "-- train ok ! --\n",
      "fold 304: \n",
      "-- training --\n",
      "0.23011374473571777\n",
      "-- train ok ! --\n",
      "fold 305: \n",
      "-- training --\n",
      "0.22928524017333984\n",
      "-- train ok ! --\n",
      "0.45125000000000004\n",
      "fold 306: \n",
      "-- training --\n",
      "0.22656631469726562\n",
      "-- train ok ! --\n",
      "fold 307: \n",
      "-- training --\n",
      "0.23171329498291016\n",
      "-- train ok ! --\n",
      "fold 308: \n",
      "-- training --\n",
      "0.23113274574279785\n",
      "-- train ok ! --\n",
      "fold 309: \n",
      "-- training --\n",
      "0.2295079231262207\n",
      "-- train ok ! --\n",
      "fold 310: \n",
      "-- training --\n",
      "0.22935819625854492\n",
      "-- train ok ! --\n",
      "0.44875\n",
      "fold 311: \n",
      "-- training --\n",
      "0.22763562202453613\n",
      "-- train ok ! --\n",
      "fold 312: \n",
      "-- training --\n",
      "0.2310793399810791\n",
      "-- train ok ! --\n",
      "fold 313: \n",
      "-- training --\n",
      "0.23147797584533691\n",
      "-- train ok ! --\n",
      "fold 314: \n",
      "-- training --\n",
      "0.2298433780670166\n",
      "-- train ok ! --\n",
      "fold 315: \n",
      "-- training --\n",
      "0.23285984992980957\n",
      "-- train ok ! --\n",
      "0.45\n",
      "fold 316: \n",
      "-- training --\n",
      "0.2267143726348877\n",
      "-- train ok ! --\n",
      "fold 317: \n",
      "-- training --\n",
      "0.23183798789978027\n",
      "-- train ok ! --\n",
      "fold 318: \n",
      "-- training --\n",
      "0.2325725555419922\n",
      "-- train ok ! --\n",
      "fold 319: \n",
      "-- training --\n",
      "0.22905182838439941\n",
      "-- train ok ! --\n",
      "fold 320: \n",
      "-- training --\n",
      "0.23079204559326172\n",
      "-- train ok ! --\n",
      "0.4\n",
      "fold 321: \n",
      "-- training --\n",
      "0.22920560836791992\n",
      "-- train ok ! --\n",
      "fold 322: \n",
      "-- training --\n",
      "0.23150181770324707\n",
      "-- train ok ! --\n",
      "fold 323: \n",
      "-- training --\n",
      "0.23266077041625977\n",
      "-- train ok ! --\n",
      "fold 324: \n",
      "-- training --\n",
      "0.22979021072387695\n",
      "-- train ok ! --\n",
      "fold 325: \n",
      "-- training --\n",
      "0.23196649551391602\n",
      "-- train ok ! --\n",
      "0.4425\n",
      "fold 326: \n",
      "-- training --\n",
      "0.22731518745422363\n",
      "-- train ok ! --\n",
      "fold 327: \n",
      "-- training --\n",
      "0.23082756996154785\n",
      "-- train ok ! --\n",
      "fold 328: \n",
      "-- training --\n",
      "0.23324036598205566\n",
      "-- train ok ! --\n",
      "fold 329: \n",
      "-- training --\n",
      "0.2305150032043457\n",
      "-- train ok ! --\n",
      "fold 330: \n",
      "-- training --\n",
      "0.23033809661865234\n",
      "-- train ok ! --\n",
      "0.43\n",
      "fold 331: \n",
      "-- training --\n",
      "0.22913455963134766\n",
      "-- train ok ! --\n",
      "fold 332: \n",
      "-- training --\n",
      "0.2297656536102295\n",
      "-- train ok ! --\n",
      "fold 333: \n",
      "-- training --\n",
      "0.23231196403503418\n",
      "-- train ok ! --\n",
      "fold 334: \n",
      "-- training --\n",
      "0.23054265975952148\n",
      "-- train ok ! --\n",
      "fold 335: \n",
      "-- training --\n",
      "0.22996068000793457\n",
      "-- train ok ! --\n",
      "0.4325\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accs_knn_manhattan_thousand = []\n",
    "validation_times = []\n",
    "for k in range(1, 201, 3):\n",
    "    performance_accuracy = []\n",
    "    pipe = Pipeline([('knn',KNeighborsClassifier(n_neighbors=k, p=1))])\n",
    "    for (train_index, test_index) in kfold.split(X_train_mil):\n",
    "        print('fold '+str(fold_count)+': ')\n",
    "        print('-- training --')\n",
    "        X_kfold_train, y_kfold_train = X_train_mil.iloc[train_index], y_train_mil.loc[train_index]\n",
    "        X_kfold_test, y_kfold_test = X_train_mil.iloc[test_index], y_train_mil.loc[test_index]\n",
    "        start = time.time()\n",
    "        pipe.fit(X_kfold_train, y_kfold_train)\n",
    "        y_kfold_pred = pipe.predict(X_kfold_test)\n",
    "        end = time.time()\n",
    "        train_time = end-start\n",
    "        print(train_time)\n",
    "        print('-- train ok ! --')\n",
    "        perf = accuracy_score(y_kfold_test, y_kfold_pred)\n",
    "        performance_accuracy.append(perf)\n",
    "\n",
    "        fold_count = fold_count + 1\n",
    "    validation_times.append((k,np.mean(train_time)))\n",
    "    accs_knn_manhattan_thousand.append((k,np.mean(performance_accuracy)))\n",
    "    print(np.mean(performance_accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.22165489196777344),\n",
       " (4, 0.2233867645263672),\n",
       " (7, 0.22353696823120117),\n",
       " (10, 0.21776413917541504),\n",
       " (13, 0.22630548477172852),\n",
       " (16, 0.2240128517150879),\n",
       " (19, 0.22986483573913574),\n",
       " (22, 0.22612714767456055),\n",
       " (25, 0.2287449836730957),\n",
       " (28, 0.22611260414123535),\n",
       " (31, 0.22754979133605957),\n",
       " (34, 0.22634482383728027),\n",
       " (37, 0.22706818580627441),\n",
       " (40, 0.22678637504577637),\n",
       " (43, 0.22424650192260742),\n",
       " (46, 0.22487640380859375),\n",
       " (49, 0.22595763206481934),\n",
       " (52, 0.22532176971435547),\n",
       " (55, 0.22640419006347656),\n",
       " (58, 0.22549915313720703),\n",
       " (61, 0.22457480430603027),\n",
       " (64, 0.22486042976379395),\n",
       " (67, 0.22549676895141602),\n",
       " (70, 0.22683072090148926),\n",
       " (73, 0.22539305686950684),\n",
       " (76, 0.22474074363708496),\n",
       " (79, 0.22487449645996094),\n",
       " (82, 0.22580432891845703),\n",
       " (85, 0.22571420669555664),\n",
       " (88, 0.22600483894348145),\n",
       " (91, 0.22635293006896973),\n",
       " (94, 0.2266526222229004),\n",
       " (97, 0.2263188362121582),\n",
       " (100, 0.22632336616516113),\n",
       " (103, 0.2265937328338623),\n",
       " (106, 0.22914886474609375),\n",
       " (109, 0.2261807918548584),\n",
       " (112, 0.22748303413391113),\n",
       " (115, 0.23517179489135742),\n",
       " (118, 0.23055768013000488),\n",
       " (121, 0.22886323928833008),\n",
       " (124, 0.22913026809692383),\n",
       " (127, 0.23404145240783691),\n",
       " (130, 0.23196053504943848),\n",
       " (133, 0.22841358184814453),\n",
       " (136, 0.22942161560058594),\n",
       " (139, 0.22893142700195312),\n",
       " (142, 0.22801542282104492),\n",
       " (145, 0.22880887985229492),\n",
       " (148, 0.23763632774353027),\n",
       " (151, 0.23055624961853027),\n",
       " (154, 0.23285818099975586),\n",
       " (157, 0.23077988624572754),\n",
       " (160, 0.23291301727294922),\n",
       " (163, 0.2295832633972168),\n",
       " (166, 0.22985482215881348),\n",
       " (169, 0.2282698154449463),\n",
       " (172, 0.2291884422302246),\n",
       " (175, 0.22989940643310547),\n",
       " (178, 0.21901774406433105),\n",
       " (181, 0.22928524017333984),\n",
       " (184, 0.22935819625854492),\n",
       " (187, 0.23285984992980957),\n",
       " (190, 0.23079204559326172),\n",
       " (193, 0.23196649551391602),\n",
       " (196, 0.23033809661865234),\n",
       " (199, 0.22996068000793457)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs_knn_manhattan_thousand\n",
    "validation_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 341: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 342: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 343: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 344: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 345: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.6975\n",
      "fold 346: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 347: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 348: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 349: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 350: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7300000000000001\n",
      "fold 351: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 352: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 353: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 354: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 355: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7112499999999999\n",
      "fold 356: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 357: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 358: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 359: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 360: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7075\n",
      "fold 361: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 362: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 363: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 364: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 365: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7150000000000001\n",
      "fold 366: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 367: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 368: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 369: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 370: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7150000000000001\n",
      "fold 371: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 372: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 373: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 374: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 375: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7162499999999999\n",
      "fold 376: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 377: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 378: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 379: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 380: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.6950000000000001\n",
      "fold 381: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 382: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 383: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 384: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 385: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.69625\n",
      "fold 386: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 387: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 388: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 389: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 390: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.68625\n",
      "fold 391: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 392: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 393: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 394: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 395: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7025\n",
      "fold 396: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 397: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 398: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 399: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 400: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.675\n",
      "fold 401: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 402: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 403: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 404: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 405: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.6937500000000001\n",
      "fold 406: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 407: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 408: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 409: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 410: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.66625\n",
      "fold 411: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 412: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 413: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 414: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 415: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.665\n",
      "fold 416: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 417: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 418: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 419: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 420: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.6475\n",
      "fold 421: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 422: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 423: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 424: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 425: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.64625\n",
      "fold 426: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 427: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 428: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 429: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 430: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.65\n",
      "fold 431: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 432: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 433: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 434: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 435: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.65375\n",
      "fold 436: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 437: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 438: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 439: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 440: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.65\n",
      "fold 441: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 442: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 443: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 444: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 445: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.6125\n",
      "fold 446: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 447: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 448: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 449: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 450: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.6250000000000001\n",
      "fold 451: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 452: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 453: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 454: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 455: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.6175\n",
      "fold 456: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 457: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 458: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 459: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 460: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.6225\n",
      "fold 461: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 462: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 463: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 464: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 465: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.625\n",
      "fold 466: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 467: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 468: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 469: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 470: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.6075\n",
      "fold 471: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 472: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 473: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 474: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 475: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.605\n",
      "fold 476: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 477: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 478: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 479: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 480: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.6112500000000001\n",
      "fold 481: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 482: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 483: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 484: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 485: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.6125\n",
      "fold 486: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 487: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 488: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 489: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 490: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.59375\n",
      "fold 491: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 492: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 493: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 494: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 495: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.6025\n",
      "fold 496: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 497: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 498: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 499: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 500: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.58375\n",
      "fold 501: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 502: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 503: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 504: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 505: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.6125\n",
      "fold 506: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 507: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 508: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 509: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 510: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.58125\n",
      "fold 511: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 512: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 513: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 514: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 515: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.58125\n",
      "fold 516: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 517: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 518: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 519: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 520: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.59625\n",
      "fold 521: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 522: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 523: \n",
      "-- training --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- train ok ! --\n",
      "fold 524: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 525: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.58625\n",
      "fold 526: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 527: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 528: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 529: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 530: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.56875\n",
      "fold 531: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 532: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 533: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 534: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 535: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.5774999999999999\n",
      "fold 536: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 537: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 538: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 539: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 540: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.57125\n",
      "fold 541: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 542: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 543: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 544: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 545: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.54625\n",
      "fold 546: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 547: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 548: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 549: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 550: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.55625\n",
      "fold 551: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 552: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 553: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 554: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 555: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.5487500000000001\n",
      "fold 556: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 557: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 558: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 559: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 560: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.5587500000000001\n",
      "fold 561: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 562: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 563: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 564: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 565: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.545\n",
      "fold 566: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 567: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 568: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 569: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 570: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.5549999999999999\n",
      "fold 571: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 572: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 573: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 574: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 575: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.5112500000000001\n",
      "fold 576: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 577: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 578: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 579: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 580: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.49874999999999997\n",
      "fold 581: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 582: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 583: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 584: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 585: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.51875\n",
      "fold 586: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 587: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 588: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 589: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 590: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.5237499999999999\n",
      "fold 591: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 592: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 593: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 594: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 595: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.5137499999999999\n",
      "fold 596: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 597: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 598: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 599: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 600: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.51125\n",
      "fold 601: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 602: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 603: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 604: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 605: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.48999999999999994\n",
      "fold 606: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 607: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 608: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 609: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 610: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.49499999999999994\n",
      "fold 611: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 612: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 613: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 614: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 615: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.48875\n",
      "fold 616: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 617: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 618: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 619: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 620: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.4925\n",
      "fold 621: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 622: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 623: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 624: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 625: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.49624999999999997\n",
      "fold 626: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 627: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 628: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 629: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 630: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.48624999999999996\n",
      "fold 631: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 632: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 633: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 634: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 635: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.4937500000000001\n",
      "fold 636: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 637: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 638: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 639: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 640: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.49874999999999997\n",
      "fold 641: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 642: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 643: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 644: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 645: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.49625\n",
      "fold 646: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 647: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 648: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 649: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 650: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.49000000000000005\n",
      "fold 651: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 652: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 653: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 654: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 655: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.47375\n",
      "fold 656: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 657: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 658: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 659: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 660: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.48375\n",
      "fold 661: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 662: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 663: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 664: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 665: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.48374999999999996\n",
      "fold 666: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 667: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 668: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 669: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 670: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.47124999999999995\n",
      "fold 671: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 672: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 673: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 674: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 675: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.46749999999999997\n"
     ]
    }
   ],
   "source": [
    "accs_knn_euclidean_thousand = []\n",
    "for k in range(1, 200, 3):\n",
    "    performance_accuracy = []\n",
    "    pipe = Pipeline([('knn',KNeighborsClassifier(n_neighbors=k, p=2))])\n",
    "    for (train_index, test_index) in kfold.split(X_train_mil):\n",
    "        print('fold '+str(fold_count)+': ')\n",
    "        print('-- training --')\n",
    "        X_kfold_train, y_kfold_train = X_train_mil.iloc[train_index], y_train_mil.loc[train_index]\n",
    "        X_kfold_test, y_kfold_test = X_train_mil.iloc[test_index], y_train_mil.loc[test_index]\n",
    "        pipe.fit(X_kfold_train, y_kfold_train)\n",
    "        y_kfold_pred = pipe.predict(X_kfold_test)\n",
    "        print('-- train ok ! --')\n",
    "        perf = accuracy_score(y_kfold_test, y_kfold_pred)\n",
    "        performance_accuracy.append(perf)\n",
    "\n",
    "        fold_count = fold_count + 1\n",
    "    accs_knn_euclidean_thousand.append((k,np.mean(performance_accuracy)))\n",
    "    print(np.mean(performance_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.6975),\n",
       " (4, 0.7300000000000001),\n",
       " (7, 0.7112499999999999),\n",
       " (10, 0.7075),\n",
       " (13, 0.7150000000000001),\n",
       " (16, 0.7150000000000001),\n",
       " (19, 0.7162499999999999),\n",
       " (22, 0.6950000000000001),\n",
       " (25, 0.69625),\n",
       " (28, 0.68625),\n",
       " (31, 0.7025),\n",
       " (34, 0.675),\n",
       " (37, 0.6937500000000001),\n",
       " (40, 0.66625),\n",
       " (43, 0.665),\n",
       " (46, 0.6475),\n",
       " (49, 0.64625),\n",
       " (52, 0.65),\n",
       " (55, 0.65375),\n",
       " (58, 0.65),\n",
       " (61, 0.6125),\n",
       " (64, 0.6250000000000001),\n",
       " (67, 0.6175),\n",
       " (70, 0.6225),\n",
       " (73, 0.625),\n",
       " (76, 0.6075),\n",
       " (79, 0.605),\n",
       " (82, 0.6112500000000001),\n",
       " (85, 0.6125),\n",
       " (88, 0.59375),\n",
       " (91, 0.6025),\n",
       " (94, 0.58375),\n",
       " (97, 0.6125),\n",
       " (100, 0.58125),\n",
       " (103, 0.58125),\n",
       " (106, 0.59625),\n",
       " (109, 0.58625),\n",
       " (112, 0.56875),\n",
       " (115, 0.5774999999999999),\n",
       " (118, 0.57125),\n",
       " (121, 0.54625),\n",
       " (124, 0.55625),\n",
       " (127, 0.5487500000000001),\n",
       " (130, 0.5587500000000001),\n",
       " (133, 0.545),\n",
       " (136, 0.5549999999999999),\n",
       " (139, 0.5112500000000001),\n",
       " (142, 0.49874999999999997),\n",
       " (145, 0.51875),\n",
       " (148, 0.5237499999999999),\n",
       " (151, 0.5137499999999999),\n",
       " (154, 0.51125),\n",
       " (157, 0.48999999999999994),\n",
       " (160, 0.49499999999999994),\n",
       " (163, 0.48875),\n",
       " (166, 0.4925),\n",
       " (169, 0.49624999999999997),\n",
       " (172, 0.48624999999999996),\n",
       " (175, 0.4937500000000001),\n",
       " (178, 0.49874999999999997),\n",
       " (181, 0.49625),\n",
       " (184, 0.49000000000000005),\n",
       " (187, 0.47375),\n",
       " (190, 0.48375),\n",
       " (193, 0.48374999999999996),\n",
       " (196, 0.47124999999999995),\n",
       " (199, 0.46749999999999997)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs_knn_euclidean_thousand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 676: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 677: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 678: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 679: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 680: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.48250000000000004\n",
      "fold 681: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 682: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 683: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 684: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 685: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.49000000000000005\n",
      "fold 686: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 687: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 688: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 689: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 690: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.47124999999999995\n",
      "fold 691: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 692: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 693: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 694: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 695: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.47000000000000003\n",
      "fold 696: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 697: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 698: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 699: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 700: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.45125000000000004\n",
      "fold 701: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 702: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 703: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 704: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 705: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.43624999999999997\n",
      "fold 706: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 707: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 708: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 709: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 710: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.43125\n",
      "fold 711: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 712: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 713: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 714: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 715: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.43499999999999994\n",
      "fold 716: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 717: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 718: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 719: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 720: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.42625\n",
      "fold 721: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 722: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 723: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 724: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 725: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.4225\n",
      "fold 726: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 727: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 728: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 729: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 730: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.41500000000000004\n",
      "fold 731: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 732: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 733: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 734: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 735: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.41875\n",
      "fold 736: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 737: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 738: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 739: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 740: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.40750000000000003\n",
      "fold 741: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 742: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 743: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 744: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 745: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.40625\n",
      "fold 746: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 747: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 748: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 749: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 750: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.39875\n",
      "fold 751: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 752: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 753: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 754: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 755: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.4225\n",
      "fold 756: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 757: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 758: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 759: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 760: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.4025\n",
      "fold 761: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 762: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 763: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 764: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 765: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.38875000000000004\n",
      "fold 766: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 767: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 768: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 769: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 770: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.38625000000000004\n",
      "fold 771: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 772: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 773: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 774: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 775: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.38125\n",
      "fold 776: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 777: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 778: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 779: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 780: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.395\n",
      "fold 781: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 782: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 783: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 784: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 785: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.3625\n",
      "fold 786: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 787: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 788: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 789: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 790: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.37625\n",
      "fold 791: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 792: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 793: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 794: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 795: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.36375\n",
      "fold 796: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 797: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 798: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 799: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 800: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.3662500000000001\n",
      "fold 801: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 802: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 803: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 804: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 805: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.35625000000000007\n",
      "fold 806: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 807: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 808: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 809: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 810: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.36375\n",
      "fold 811: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 812: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 813: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 814: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 815: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.36375\n",
      "fold 816: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 817: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 818: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 819: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 820: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.375\n",
      "fold 821: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 822: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 823: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 824: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 825: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.35374999999999995\n",
      "fold 826: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 827: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 828: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 829: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 830: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.35624999999999996\n",
      "fold 831: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 832: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 833: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 834: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 835: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.36499999999999994\n",
      "fold 836: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 837: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 838: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 839: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 840: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.3375\n",
      "fold 841: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 842: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 843: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 844: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 845: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.32875\n",
      "fold 846: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 847: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 848: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 849: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 850: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.37875000000000003\n",
      "fold 851: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 852: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 853: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 854: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 855: \n",
      "-- training --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- train ok ! --\n",
      "0.33125\n",
      "fold 856: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 857: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 858: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 859: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 860: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.335\n",
      "fold 861: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 862: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 863: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 864: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 865: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.3475\n",
      "fold 866: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 867: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 868: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 869: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 870: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.33875\n",
      "fold 871: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 872: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 873: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 874: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 875: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.34875\n",
      "fold 876: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 877: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 878: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 879: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 880: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.35625\n",
      "fold 881: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 882: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 883: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 884: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 885: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.33249999999999996\n",
      "fold 886: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 887: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 888: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 889: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 890: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.3325\n",
      "fold 891: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 892: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 893: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 894: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 895: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.33875\n",
      "fold 896: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 897: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 898: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 899: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 900: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.33625000000000005\n",
      "fold 901: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 902: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 903: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 904: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 905: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.35124999999999995\n",
      "fold 906: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 907: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 908: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 909: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 910: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.33\n",
      "fold 911: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 912: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 913: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 914: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 915: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.33625\n",
      "fold 916: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 917: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 918: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 919: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 920: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.34375000000000006\n",
      "fold 921: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 922: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 923: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 924: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 925: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.3175\n",
      "fold 926: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 927: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 928: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 929: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 930: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.3175\n",
      "fold 931: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 932: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 933: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 934: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 935: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.33999999999999997\n",
      "fold 936: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 937: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 938: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 939: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 940: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.31375000000000003\n",
      "fold 941: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 942: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 943: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 944: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 945: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.3075\n",
      "fold 946: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 947: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 948: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 949: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 950: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.3075\n",
      "fold 951: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 952: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 953: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 954: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 955: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.33875\n",
      "fold 956: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 957: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 958: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 959: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 960: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.3425\n",
      "fold 961: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 962: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 963: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 964: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 965: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.34125\n",
      "fold 966: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 967: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 968: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 969: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 970: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.31500000000000006\n",
      "fold 971: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 972: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 973: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 974: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 975: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.32875\n",
      "fold 976: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 977: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 978: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 979: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 980: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.31625\n",
      "fold 981: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 982: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 983: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 984: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 985: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.33249999999999996\n",
      "fold 986: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 987: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 988: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 989: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 990: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.3275\n",
      "fold 991: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 992: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 993: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 994: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 995: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.34124999999999994\n",
      "fold 996: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 997: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 998: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 999: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 1000: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.30874999999999997\n",
      "fold 1001: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 1002: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 1003: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 1004: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 1005: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.33625\n",
      "fold 1006: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 1007: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 1008: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 1009: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 1010: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.31499999999999995\n"
     ]
    }
   ],
   "source": [
    "accs_knn_chebyshev_thousand = []\n",
    "for k in range(1, 200, 3):\n",
    "    performance_accuracy = []\n",
    "    pipe = Pipeline([('knn',KNeighborsClassifier(n_neighbors=k, metric='chebyshev'))])\n",
    "    for (train_index, test_index) in kfold.split(X_train_mil):\n",
    "        print('fold '+str(fold_count)+': ')\n",
    "        print('-- training --')\n",
    "        X_kfold_train, y_kfold_train = X_train_mil.iloc[train_index], y_train_mil.loc[train_index]\n",
    "        X_kfold_test, y_kfold_test = X_train_mil.iloc[test_index], y_train_mil.loc[test_index]\n",
    "        pipe.fit(X_kfold_train, y_kfold_train)\n",
    "        y_kfold_pred = pipe.predict(X_kfold_test)\n",
    "        print('-- train ok ! --')\n",
    "        perf = accuracy_score(y_kfold_test, y_kfold_pred)\n",
    "        performance_accuracy.append(perf)\n",
    "\n",
    "        fold_count = fold_count + 1\n",
    "    accs_knn_chebyshev_thousand.append((k,np.mean(performance_accuracy)))\n",
    "    print(np.mean(performance_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.48250000000000004),\n",
       " (4, 0.49000000000000005),\n",
       " (7, 0.47124999999999995),\n",
       " (10, 0.47000000000000003),\n",
       " (13, 0.45125000000000004),\n",
       " (16, 0.43624999999999997),\n",
       " (19, 0.43125),\n",
       " (22, 0.43499999999999994),\n",
       " (25, 0.42625),\n",
       " (28, 0.4225),\n",
       " (31, 0.41500000000000004),\n",
       " (34, 0.41875),\n",
       " (37, 0.40750000000000003),\n",
       " (40, 0.40625),\n",
       " (43, 0.39875),\n",
       " (46, 0.4225),\n",
       " (49, 0.4025),\n",
       " (52, 0.38875000000000004),\n",
       " (55, 0.38625000000000004),\n",
       " (58, 0.38125),\n",
       " (61, 0.395),\n",
       " (64, 0.3625),\n",
       " (67, 0.37625),\n",
       " (70, 0.36375),\n",
       " (73, 0.3662500000000001),\n",
       " (76, 0.35625000000000007),\n",
       " (79, 0.36375),\n",
       " (82, 0.36375),\n",
       " (85, 0.375),\n",
       " (88, 0.35374999999999995),\n",
       " (91, 0.35624999999999996),\n",
       " (94, 0.36499999999999994),\n",
       " (97, 0.3375),\n",
       " (100, 0.32875),\n",
       " (103, 0.37875000000000003),\n",
       " (106, 0.33125),\n",
       " (109, 0.335),\n",
       " (112, 0.3475),\n",
       " (115, 0.33875),\n",
       " (118, 0.34875),\n",
       " (121, 0.35625),\n",
       " (124, 0.33249999999999996),\n",
       " (127, 0.3325),\n",
       " (130, 0.33875),\n",
       " (133, 0.33625000000000005),\n",
       " (136, 0.35124999999999995),\n",
       " (139, 0.33),\n",
       " (142, 0.33625),\n",
       " (145, 0.34375000000000006),\n",
       " (148, 0.3175),\n",
       " (151, 0.3175),\n",
       " (154, 0.33999999999999997),\n",
       " (157, 0.31375000000000003),\n",
       " (160, 0.3075),\n",
       " (163, 0.3075),\n",
       " (166, 0.33875),\n",
       " (169, 0.3425),\n",
       " (172, 0.34125),\n",
       " (175, 0.31500000000000006),\n",
       " (178, 0.32875),\n",
       " (181, 0.31625),\n",
       " (184, 0.33249999999999996),\n",
       " (187, 0.3275),\n",
       " (190, 0.34124999999999994),\n",
       " (193, 0.30874999999999997),\n",
       " (196, 0.33625),\n",
       " (199, 0.31499999999999995)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs_knn_chebyshev_thousand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo tomando cinco mil entradas del conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b11d7d8cd0b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_cinco_mil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX_cinco_mil\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_cinco_mil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_cinco_mil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_cinco_mil\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train_cinco_mil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_cinco_mil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_cinco_mil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_cinco_mil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cinco_mil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cinco_mil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "data_cinco_mil = data[:5000]\n",
    "X_cinco_mil,y_cinco_mil = data_cinco_mil.drop(['label'], axis = 1), data_cinco_mil['label']\n",
    "X_train_cinco_mil, X_test_cinco_mil, y_train_cinco_mil, y_test_cinco_mil = train_test_split(X_cinco_mil, y_cinco_mil, test_size = 0.2, random_state = SEED)\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "X_train_cinco_mil = X_train_cinco_mil.reset_index(drop=True)\n",
    "\n",
    "y_train_cinco_mil.index = X_train_cinco_mil.index\n",
    "performance_accuracy = []\n",
    "chebyshev\n",
    "fold_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 2: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 3: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 4: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 5: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7889999999999999\n",
      "fold 6: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 7: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 8: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 9: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 10: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7987500000000001\n",
      "fold 11: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 12: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 13: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 14: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 15: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.8002500000000001\n",
      "fold 16: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 17: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 18: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 19: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 20: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.79125\n",
      "fold 21: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 22: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 23: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 24: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 25: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.79025\n",
      "fold 26: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 27: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 28: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 29: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 30: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.79075\n",
      "fold 31: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 32: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 33: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 34: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 35: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.79275\n",
      "fold 36: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 37: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 38: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 39: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 40: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.78825\n",
      "fold 41: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 42: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 43: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 44: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 45: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7825\n",
      "fold 46: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 47: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 48: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 49: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 50: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.78375\n",
      "fold 51: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 52: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 53: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 54: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 55: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7875\n",
      "fold 56: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 57: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 58: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 59: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 60: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7802499999999999\n",
      "fold 61: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 62: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 63: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 64: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 65: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.77925\n",
      "fold 66: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 67: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 68: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 69: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 70: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.77675\n",
      "fold 71: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 72: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 73: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 74: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 75: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7749999999999999\n",
      "fold 76: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 77: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 78: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 79: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 80: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7765\n",
      "fold 81: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 82: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 83: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 84: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 85: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.772\n",
      "fold 86: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 87: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 88: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 89: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 90: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7705\n",
      "fold 91: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 92: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 93: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 94: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 95: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7665000000000001\n",
      "fold 96: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 97: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 98: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 99: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 100: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.76925\n",
      "fold 101: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 102: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 103: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 104: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 105: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7649999999999999\n",
      "fold 106: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 107: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 108: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 109: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 110: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7609999999999999\n",
      "fold 111: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 112: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 113: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 114: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 115: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7645\n",
      "fold 116: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 117: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 118: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 119: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 120: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7602499999999999\n",
      "fold 121: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 122: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 123: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 124: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 125: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7567499999999999\n",
      "fold 126: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 127: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 128: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 129: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 130: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.75375\n",
      "fold 131: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 132: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 133: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 134: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 135: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.75325\n",
      "fold 136: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 137: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 138: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 139: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 140: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.75025\n",
      "fold 141: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 142: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 143: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 144: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 145: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7535000000000001\n",
      "fold 146: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 147: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 148: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 149: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 150: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.74775\n",
      "fold 151: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 152: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 153: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 154: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 155: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7445\n",
      "fold 156: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 157: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 158: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 159: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 160: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.748\n",
      "fold 161: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 162: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 163: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 164: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 165: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.74675\n",
      "fold 166: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 167: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 168: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 169: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 170: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.74375\n",
      "fold 171: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 172: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 173: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 174: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 175: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7450000000000001\n",
      "fold 176: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 177: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 178: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 179: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 180: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7422500000000001\n",
      "fold 181: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 182: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 183: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 184: \n",
      "-- training --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- train ok ! --\n",
      "fold 185: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7405\n",
      "fold 186: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 187: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 188: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 189: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 190: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.73925\n",
      "fold 191: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 192: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 193: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 194: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 195: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7332500000000002\n",
      "fold 196: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 197: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 198: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 199: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 200: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.735\n",
      "fold 201: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 202: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 203: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 204: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 205: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.735\n",
      "fold 206: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 207: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 208: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 209: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 210: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7384999999999999\n",
      "fold 211: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 212: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 213: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 214: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 215: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.734\n",
      "fold 216: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 217: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 218: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 219: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 220: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7315\n",
      "fold 221: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 222: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 223: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 224: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 225: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7285\n",
      "fold 226: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 227: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 228: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 229: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 230: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.73275\n",
      "fold 231: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 232: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 233: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 234: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 235: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7265\n",
      "fold 236: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 237: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 238: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 239: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 240: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7300000000000001\n",
      "fold 241: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 242: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 243: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 244: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 245: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.72325\n",
      "fold 246: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 247: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 248: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 249: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 250: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.72325\n",
      "fold 251: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 252: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 253: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 254: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 255: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7264999999999999\n",
      "fold 256: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 257: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 258: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 259: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 260: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7224999999999999\n",
      "fold 261: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 262: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 263: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 264: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 265: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7245000000000001\n",
      "fold 266: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 267: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 268: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 269: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 270: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7220000000000001\n",
      "fold 271: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 272: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 273: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 274: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 275: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.718\n",
      "fold 276: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 277: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 278: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 279: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 280: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.71675\n",
      "fold 281: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 282: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 283: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 284: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 285: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.71825\n",
      "fold 286: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 287: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 288: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 289: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 290: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.71675\n",
      "fold 291: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 292: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 293: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 294: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 295: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.717\n",
      "fold 296: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 297: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 298: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 299: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 300: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7112499999999999\n",
      "fold 301: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 302: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 303: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 304: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 305: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.71175\n",
      "fold 306: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 307: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 308: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 309: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 310: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.71675\n",
      "fold 311: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 312: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 313: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 314: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 315: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7085\n",
      "fold 316: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 317: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 318: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 319: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 320: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7100000000000001\n",
      "fold 321: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 322: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 323: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 324: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 325: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7070000000000001\n",
      "fold 326: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 327: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 328: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 329: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 330: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7045000000000001\n",
      "fold 331: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 332: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 333: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 334: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 335: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.70575\n"
     ]
    }
   ],
   "source": [
    "accs_knn_manhattan_five_thousand = []\n",
    "for k in range(1, 201, 3):\n",
    "    performance_accuracy = []\n",
    "    pipe = Pipeline([('knn',KNeighborsClassifier(n_neighbors=k, p=1))])\n",
    "    for (train_index, test_index) in kfold.split(X_train_cinco_mil):\n",
    "        print('fold '+str(fold_count)+': ')\n",
    "        print('-- training --')\n",
    "        X_kfold_train, y_kfold_train = X_train_cinco_mil.iloc[train_index], y_train_cinco_mil.loc[train_index]\n",
    "        X_kfold_test, y_kfold_test = X_train_cinco_mil.iloc[test_index], y_train_cinco_mil.loc[test_index]\n",
    "        pipe.fit(X_kfold_train, y_kfold_train)\n",
    "        y_kfold_pred = pipe.predict(X_kfold_test)\n",
    "        print('-- train ok ! --')\n",
    "        perf = accuracy_score(y_kfold_test, y_kfold_pred)\n",
    "        performance_accuracy.append(perf)\n",
    "\n",
    "        fold_count = fold_count + 1\n",
    "    accs_knn_manhattan_five_thousand.append((k,np.mean(performance_accuracy)))\n",
    "    print(np.mean(performance_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.7889999999999999),\n",
       " (4, 0.7987500000000001),\n",
       " (7, 0.8002500000000001),\n",
       " (10, 0.79125),\n",
       " (13, 0.79025),\n",
       " (16, 0.79075),\n",
       " (19, 0.79275),\n",
       " (22, 0.78825),\n",
       " (25, 0.7825),\n",
       " (28, 0.78375),\n",
       " (31, 0.7875),\n",
       " (34, 0.7802499999999999),\n",
       " (37, 0.77925),\n",
       " (40, 0.77675),\n",
       " (43, 0.7749999999999999),\n",
       " (46, 0.7765),\n",
       " (49, 0.772),\n",
       " (52, 0.7705),\n",
       " (55, 0.7665000000000001),\n",
       " (58, 0.76925),\n",
       " (61, 0.7649999999999999),\n",
       " (64, 0.7609999999999999),\n",
       " (67, 0.7645),\n",
       " (70, 0.7602499999999999),\n",
       " (73, 0.7567499999999999),\n",
       " (76, 0.75375),\n",
       " (79, 0.75325),\n",
       " (82, 0.75025),\n",
       " (85, 0.7535000000000001),\n",
       " (88, 0.74775),\n",
       " (91, 0.7445),\n",
       " (94, 0.748),\n",
       " (97, 0.74675),\n",
       " (100, 0.74375),\n",
       " (103, 0.7450000000000001),\n",
       " (106, 0.7422500000000001),\n",
       " (109, 0.7405),\n",
       " (112, 0.73925),\n",
       " (115, 0.7332500000000002),\n",
       " (118, 0.735),\n",
       " (121, 0.735),\n",
       " (124, 0.7384999999999999),\n",
       " (127, 0.734),\n",
       " (130, 0.7315),\n",
       " (133, 0.7285),\n",
       " (136, 0.73275),\n",
       " (139, 0.7265),\n",
       " (142, 0.7300000000000001),\n",
       " (145, 0.72325),\n",
       " (148, 0.72325),\n",
       " (151, 0.7264999999999999),\n",
       " (154, 0.7224999999999999),\n",
       " (157, 0.7245000000000001),\n",
       " (160, 0.7220000000000001),\n",
       " (163, 0.718),\n",
       " (166, 0.71675),\n",
       " (169, 0.71825),\n",
       " (172, 0.71675),\n",
       " (175, 0.717),\n",
       " (178, 0.7112499999999999),\n",
       " (181, 0.71175),\n",
       " (184, 0.71675),\n",
       " (187, 0.7085),\n",
       " (190, 0.7100000000000001),\n",
       " (193, 0.7070000000000001),\n",
       " (196, 0.7045000000000001),\n",
       " (199, 0.70575)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs_knn_manhattan_five_thousand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 336: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 337: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 338: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 339: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 340: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7875\n",
      "fold 341: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 342: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 343: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 344: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 345: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7929999999999999\n",
      "fold 346: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 347: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 348: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 349: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 350: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7877500000000001\n",
      "fold 351: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 352: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 353: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 354: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 355: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7935000000000001\n",
      "fold 356: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 357: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 358: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 359: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 360: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.788\n",
      "fold 361: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 362: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 363: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 364: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 365: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.782\n",
      "fold 366: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 367: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 368: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 369: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 370: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7835\n",
      "fold 371: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 372: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 373: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 374: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 375: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.782\n",
      "fold 376: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 377: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 378: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 379: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 380: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7772500000000001\n",
      "fold 381: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 382: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 383: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 384: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 385: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7737499999999999\n",
      "fold 386: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 387: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 388: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 389: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 390: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.77175\n",
      "fold 391: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 392: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 393: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 394: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 395: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7729999999999999\n",
      "fold 396: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 397: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 398: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 399: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 400: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7642499999999999\n",
      "fold 401: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 402: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 403: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 404: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 405: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.76625\n",
      "fold 406: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 407: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 408: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 409: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 410: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.76325\n",
      "fold 411: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 412: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 413: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 414: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 415: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7659999999999999\n",
      "fold 416: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 417: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 418: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 419: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 420: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.75825\n",
      "fold 421: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 422: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 423: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 424: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 425: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7627499999999999\n",
      "fold 426: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 427: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 428: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 429: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 430: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7595\n",
      "fold 431: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 432: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 433: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 434: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 435: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.755\n",
      "fold 436: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 437: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 438: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 439: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 440: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7579999999999999\n",
      "fold 441: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 442: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 443: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 444: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 445: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.75575\n",
      "fold 446: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 447: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 448: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 449: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 450: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.752\n",
      "fold 451: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 452: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 453: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 454: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 455: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.75525\n",
      "fold 456: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 457: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 458: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 459: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 460: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7505000000000001\n",
      "fold 461: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 462: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 463: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 464: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 465: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7477499999999999\n",
      "fold 466: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 467: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 468: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 469: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 470: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.75225\n",
      "fold 471: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 472: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 473: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 474: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 475: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.75\n",
      "fold 476: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 477: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 478: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 479: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 480: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.74325\n",
      "fold 481: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 482: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 483: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 484: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 485: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.74475\n",
      "fold 486: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 487: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 488: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 489: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 490: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7394999999999999\n",
      "fold 491: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 492: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 493: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 494: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 495: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7435\n",
      "fold 496: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 497: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 498: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 499: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 500: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7434999999999999\n",
      "fold 501: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 502: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 503: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 504: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 505: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7395\n",
      "fold 506: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 507: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 508: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 509: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 510: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.741\n",
      "fold 511: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 512: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 513: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 514: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 515: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.73525\n",
      "fold 516: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 517: \n",
      "-- training --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- train ok ! --\n",
      "fold 518: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 519: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 520: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.73525\n",
      "fold 521: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 522: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 523: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 524: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 525: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7332500000000001\n",
      "fold 526: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 527: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 528: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 529: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 530: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.734\n",
      "fold 531: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 532: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 533: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 534: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 535: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.733\n",
      "fold 536: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 537: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 538: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 539: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 540: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.729\n",
      "fold 541: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 542: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 543: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 544: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 545: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7302500000000001\n",
      "fold 546: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 547: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 548: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 549: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 550: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.73\n",
      "fold 551: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 552: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 553: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 554: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 555: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.73075\n",
      "fold 556: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 557: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 558: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 559: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 560: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7250000000000001\n",
      "fold 561: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 562: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 563: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 564: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 565: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7304999999999999\n",
      "fold 566: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 567: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 568: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 569: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 570: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7285\n",
      "fold 571: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 572: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 573: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 574: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 575: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7252500000000001\n",
      "fold 576: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 577: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 578: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 579: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 580: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.72675\n",
      "fold 581: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 582: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 583: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 584: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 585: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7244999999999999\n",
      "fold 586: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 587: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 588: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 589: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 590: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7250000000000001\n",
      "fold 591: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 592: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 593: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 594: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 595: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7217499999999999\n",
      "fold 596: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 597: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 598: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 599: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 600: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.72475\n",
      "fold 601: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 602: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 603: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 604: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 605: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.721\n",
      "fold 606: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 607: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 608: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 609: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 610: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.71925\n",
      "fold 611: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 612: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 613: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 614: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 615: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.71425\n",
      "fold 616: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 617: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 618: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 619: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 620: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.719\n",
      "fold 621: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 622: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 623: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 624: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 625: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7154999999999999\n",
      "fold 626: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 627: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 628: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 629: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 630: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7202500000000001\n",
      "fold 631: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 632: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 633: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 634: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 635: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.71325\n",
      "fold 636: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 637: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 638: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 639: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 640: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7150000000000001\n",
      "fold 641: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 642: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 643: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 644: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 645: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.71225\n",
      "fold 646: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 647: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 648: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 649: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 650: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.70975\n",
      "fold 651: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 652: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 653: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 654: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 655: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7085\n",
      "fold 656: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 657: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 658: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 659: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 660: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.7104999999999999\n",
      "fold 661: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 662: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 663: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 664: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 665: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.712\n",
      "fold 666: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 667: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 668: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 669: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "fold 670: \n",
      "-- training --\n",
      "-- train ok ! --\n",
      "0.70925\n"
     ]
    }
   ],
   "source": [
    "accs_knn_euclidean_five_thousand = []\n",
    "for k in range(1, 201, 3):\n",
    "    performance_accuracy = []\n",
    "    pipe = Pipeline([('knn',KNeighborsClassifier(n_neighbors=k, p=2))])\n",
    "    for (train_index, test_index) in kfold.split(X_train_cinco_mil):\n",
    "        print('fold '+str(fold_count)+': ')\n",
    "        print('-- training --')\n",
    "        X_kfold_train, y_kfold_train = X_train_cinco_mil.iloc[train_index], y_train_cinco_mil.loc[train_index]\n",
    "        X_kfold_test, y_kfold_test = X_train_cinco_mil.iloc[test_index], y_train_cinco_mil.loc[test_index]\n",
    "        pipe.fit(X_kfold_train, y_kfold_train)\n",
    "        y_kfold_pred = pipe.predict(X_kfold_test)\n",
    "        print('-- train ok ! --')\n",
    "        perf = accuracy_score(y_kfold_test, y_kfold_pred)\n",
    "        performance_accuracy.append(perf)\n",
    "\n",
    "        fold_count = fold_count + 1\n",
    "    accs_knn_euclidean_five_thousand.append((k,np.mean(performance_accuracy)))\n",
    "    print(np.mean(performance_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.7875),\n",
       " (4, 0.7929999999999999),\n",
       " (7, 0.7877500000000001),\n",
       " (10, 0.7935000000000001),\n",
       " (13, 0.788),\n",
       " (16, 0.782),\n",
       " (19, 0.7835),\n",
       " (22, 0.782),\n",
       " (25, 0.7772500000000001),\n",
       " (28, 0.7737499999999999),\n",
       " (31, 0.77175),\n",
       " (34, 0.7729999999999999),\n",
       " (37, 0.7642499999999999),\n",
       " (40, 0.76625),\n",
       " (43, 0.76325),\n",
       " (46, 0.7659999999999999),\n",
       " (49, 0.75825),\n",
       " (52, 0.7627499999999999),\n",
       " (55, 0.7595),\n",
       " (58, 0.755),\n",
       " (61, 0.7579999999999999),\n",
       " (64, 0.75575),\n",
       " (67, 0.752),\n",
       " (70, 0.75525),\n",
       " (73, 0.7505000000000001),\n",
       " (76, 0.7477499999999999),\n",
       " (79, 0.75225),\n",
       " (82, 0.75),\n",
       " (85, 0.74325),\n",
       " (88, 0.74475),\n",
       " (91, 0.7394999999999999),\n",
       " (94, 0.7435),\n",
       " (97, 0.7434999999999999),\n",
       " (100, 0.7395),\n",
       " (103, 0.741),\n",
       " (106, 0.73525),\n",
       " (109, 0.73525),\n",
       " (112, 0.7332500000000001),\n",
       " (115, 0.734),\n",
       " (118, 0.733),\n",
       " (121, 0.729),\n",
       " (124, 0.7302500000000001),\n",
       " (127, 0.73),\n",
       " (130, 0.73075),\n",
       " (133, 0.7250000000000001),\n",
       " (136, 0.7304999999999999),\n",
       " (139, 0.7285),\n",
       " (142, 0.7252500000000001),\n",
       " (145, 0.72675),\n",
       " (148, 0.7244999999999999),\n",
       " (151, 0.7250000000000001),\n",
       " (154, 0.7217499999999999),\n",
       " (157, 0.72475),\n",
       " (160, 0.721),\n",
       " (163, 0.71925),\n",
       " (166, 0.71425),\n",
       " (169, 0.719),\n",
       " (172, 0.7154999999999999),\n",
       " (175, 0.7202500000000001),\n",
       " (178, 0.71325),\n",
       " (181, 0.7150000000000001),\n",
       " (184, 0.71225),\n",
       " (187, 0.70975),\n",
       " (190, 0.7085),\n",
       " (193, 0.7104999999999999),\n",
       " (196, 0.712),\n",
       " (199, 0.70925)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs_knn_euclidean_five_thousand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entreno el modelo usando todos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "data = pd.read_csv(\"../data/fashion-mnist_train.csv\")\n",
    "X,y = data.drop(['label'], axis = 1), data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = SEED)\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "\n",
    "y_train.index = X_train.index\n",
    "performance_accuracy = []\n",
    "\n",
    "fold_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1: \n",
      "-- training manhattan--\n",
      "-609.3385319709778\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "546.3066351413727\n",
      "-- train ok ! --\n",
      "fold 2: \n",
      "-- training manhattan--\n",
      "-610.418524980545\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "542.4019453525543\n",
      "-- train ok ! --\n",
      "fold 3: \n",
      "-- training manhattan--\n",
      "-607.4063370227814\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "543.966944694519\n",
      "-- train ok ! --\n",
      "fold 4: \n",
      "-- training manhattan--\n",
      "-608.9550671577454\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "553.843407869339\n",
      "-- train ok ! --\n",
      "fold 5: \n",
      "-- training manhattan--\n",
      "-613.0419075489044\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "552.2348008155823\n",
      "-- train ok ! --\n",
      "0.8500208333333333\n",
      "0.8448958333333334\n"
     ]
    }
   ],
   "source": [
    "accs_knn_manhattan = []\n",
    "validation_times_manhattan = []\n",
    "accs_knn_euclidean = []\n",
    "validation_times_euclidean = []\n",
    "for k in range(1, 6, 5):\n",
    "    performance_accuracy_manhattan = []\n",
    "    performance_accuracy_euclidean = []\n",
    "    times_manhattan = []\n",
    "    times_euclidean = []\n",
    "    pipe1 = Pipeline([('knn',KNeighborsClassifier(n_neighbors=k, p=1))])\n",
    "    pipe2 = Pipeline([('knn',KNeighborsClassifier(n_neighbors=k, p=2))])\n",
    "    for (train_index, test_index) in kfold.split(X_train):\n",
    "        \n",
    "        print('fold '+str(fold_count)+': ')\n",
    "        print('-- training manhattan--')\n",
    "        X_kfold_train, y_kfold_train = X_train.iloc[train_index], y_train.loc[train_index]\n",
    "        X_kfold_test, y_kfold_test = X_train.iloc[test_index], y_train.loc[test_index]\n",
    "        \n",
    "        start_manhattan = time.time()\n",
    "        pipe1.fit(X_kfold_train, y_kfold_train)\n",
    "        y_kfold_pred_manhattan = pipe1.predict(X_kfold_test)\n",
    "        end_manhattan = time.time()\n",
    "        train_time_manhattan = start_manhattan - end_manhattan\n",
    "        print(train_time_manhattan)\n",
    "        print('-- train ok ! --')\n",
    "        perf = accuracy_score(y_kfold_test, y_kfold_pred_manhattan)\n",
    "        performance_accuracy_manhattan.append(perf)\n",
    "        times_manhattan.append(train_time_manhattan)\n",
    "        \n",
    "        print('-- training euclidean--')\n",
    "        start_euclidean = time.time()\n",
    "        pipe2.fit(X_kfold_train, y_kfold_train)\n",
    "        y_kfold_pred_euclidean = pipe2.predict(X_kfold_test)\n",
    "        end_euclidean = time.time()\n",
    "        train_time_euclidean = end_euclidean-start_euclidean\n",
    "        print(train_time_euclidean)\n",
    "        print('-- train ok ! --')\n",
    "        perf = accuracy_score(y_kfold_test, y_kfold_pred_euclidean)\n",
    "        performance_accuracy_euclidean.append(perf)\n",
    "        times_euclidean.append(train_time_euclidean)\n",
    "\n",
    "        fold_count = fold_count + 1\n",
    "    validation_times_manhattan.append((k,np.mean(times_manhattan)))\n",
    "    accs_knn_manhattan.append((k,np.mean(performance_accuracy_manhattan)))\n",
    "    print(np.mean(performance_accuracy_manhattan))\n",
    "    validation_times_euclidean.append((k,np.mean(times_euclidean)))\n",
    "    accs_knn_euclidean.append((k,np.mean(performance_accuracy_euclidean)))\n",
    "    print(np.mean(performance_accuracy_euclidean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 6: \n",
      "-- training manhattan--\n",
      "-621.978841304779\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "591.3719611167908\n",
      "-- train ok ! --\n",
      "fold 7: \n",
      "-- training manhattan--\n",
      "-620.2540860176086\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "575.0706098079681\n",
      "-- train ok ! --\n",
      "fold 8: \n",
      "-- training manhattan--\n",
      "-625.2707359790802\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "580.5303499698639\n",
      "-- train ok ! --\n",
      "fold 9: \n",
      "-- training manhattan--\n",
      "-620.3241951465607\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "588.3524477481842\n",
      "-- train ok ! --\n",
      "fold 10: \n",
      "-- training manhattan--\n",
      "-629.2031226158142\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "587.0146899223328\n",
      "-- train ok ! --\n",
      "0.8539791666666666\n",
      "0.8485416666666665\n",
      "fold 11: \n",
      "-- training manhattan--\n",
      "-628.4680244922638\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "599.2653057575226\n",
      "-- train ok ! --\n",
      "fold 12: \n",
      "-- training manhattan--\n",
      "-630.5361597537994\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "591.4146692752838\n",
      "-- train ok ! --\n",
      "fold 13: \n",
      "-- training manhattan--\n",
      "-625.9567558765411\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "592.1645948886871\n",
      "-- train ok ! --\n",
      "fold 14: \n",
      "-- training manhattan--\n",
      "-629.6429531574249\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "596.1904075145721\n",
      "-- train ok ! --\n",
      "fold 15: \n",
      "-- training manhattan--\n",
      "-626.4885153770447\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "581.7245242595673\n",
      "-- train ok ! --\n",
      "0.8513333333333334\n",
      "0.8445625\n",
      "fold 16: \n",
      "-- training manhattan--\n",
      "-624.4169874191284\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "597.6020438671112\n",
      "-- train ok ! --\n",
      "fold 17: \n",
      "-- training manhattan--\n",
      "-616.0749971866608\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "600.158228635788\n",
      "-- train ok ! --\n",
      "fold 18: \n",
      "-- training manhattan--\n",
      "-619.8000075817108\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "590.5741264820099\n",
      "-- train ok ! --\n",
      "fold 19: \n",
      "-- training manhattan--\n",
      "-637.9458501338959\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "594.3853666782379\n",
      "-- train ok ! --\n",
      "fold 20: \n",
      "-- training manhattan--\n",
      "-627.537261724472\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "600.5321736335754\n",
      "-- train ok ! --\n",
      "0.8473124999999999\n",
      "0.8399166666666668\n",
      "fold 21: \n",
      "-- training manhattan--\n",
      "-626.3082666397095\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "606.0710573196411\n",
      "-- train ok ! --\n",
      "fold 22: \n",
      "-- training manhattan--\n",
      "-654.8875510692596\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "604.0938675403595\n",
      "-- train ok ! --\n",
      "fold 23: \n",
      "-- training manhattan--\n",
      "-638.1903185844421\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "623.4734535217285\n",
      "-- train ok ! --\n",
      "fold 24: \n",
      "-- training manhattan--\n",
      "-624.2169415950775\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "601.5022733211517\n",
      "-- train ok ! --\n",
      "fold 25: \n",
      "-- training manhattan--\n",
      "-629.8450677394867\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "604.425493478775\n",
      "-- train ok ! --\n",
      "0.844\n",
      "0.8357916666666666\n",
      "fold 26: \n",
      "-- training manhattan--\n",
      "-631.0111725330353\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "608.0268967151642\n",
      "-- train ok ! --\n",
      "fold 27: \n",
      "-- training manhattan--\n",
      "-636.7989239692688\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "608.6873362064362\n",
      "-- train ok ! --\n",
      "fold 28: \n",
      "-- training manhattan--\n",
      "-636.277862071991\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "606.3380343914032\n",
      "-- train ok ! --\n",
      "fold 29: \n",
      "-- training manhattan--\n",
      "-633.4211270809174\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "606.8982899188995\n",
      "-- train ok ! --\n",
      "fold 30: \n",
      "-- training manhattan--\n",
      "-636.3388612270355\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "601.7740573883057\n",
      "-- train ok ! --\n",
      "0.8422500000000002\n",
      "0.834125\n",
      "fold 31: \n",
      "-- training manhattan--\n",
      "-632.7310962677002\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "603.9948451519012\n",
      "-- train ok ! --\n",
      "fold 32: \n",
      "-- training manhattan--\n",
      "-632.84583568573\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "602.1281189918518\n",
      "-- train ok ! --\n",
      "fold 33: \n",
      "-- training manhattan--\n",
      "-636.7156164646149\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "612.2158613204956\n",
      "-- train ok ! --\n",
      "fold 34: \n",
      "-- training manhattan--\n",
      "-638.8390243053436\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "606.4516582489014\n",
      "-- train ok ! --\n",
      "fold 35: \n",
      "-- training manhattan--\n",
      "-633.49485206604\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "603.1174545288086\n",
      "-- train ok ! --\n",
      "0.8397500000000001\n",
      "0.8314166666666667\n",
      "fold 36: \n",
      "-- training manhattan--\n",
      "-631.4143643379211\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "609.7325122356415\n",
      "-- train ok ! --\n",
      "fold 37: \n",
      "-- training manhattan--\n",
      "-650.8726873397827\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "606.032585144043\n",
      "-- train ok ! --\n",
      "fold 38: \n",
      "-- training manhattan--\n",
      "-638.1226561069489\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "633.2571377754211\n",
      "-- train ok ! --\n",
      "fold 39: \n",
      "-- training manhattan--\n",
      "-635.2183268070221\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "609.7137005329132\n",
      "-- train ok ! --\n",
      "fold 40: \n",
      "-- training manhattan--\n",
      "-636.4812679290771\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "612.4431622028351\n",
      "-- train ok ! --\n",
      "0.8383958333333332\n",
      "0.8300416666666667\n",
      "fold 41: \n",
      "-- training manhattan--\n",
      "-630.9730331897736\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "614.8518822193146\n",
      "-- train ok ! --\n",
      "fold 42: \n",
      "-- training manhattan--\n",
      "-615.5696358680725\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "607.4347121715546\n",
      "-- train ok ! --\n",
      "fold 43: \n",
      "-- training manhattan--\n",
      "-622.2462637424469\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "615.7479681968689\n",
      "-- train ok ! --\n",
      "fold 44: \n",
      "-- training manhattan--\n",
      "-631.4222078323364\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "605.7996537685394\n",
      "-- train ok ! --\n",
      "fold 45: \n",
      "-- training manhattan--\n",
      "-625.5158076286316\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "607.0286581516266\n",
      "-- train ok ! --\n",
      "0.8357083333333334\n",
      "0.827875\n",
      "fold 46: \n",
      "-- training manhattan--\n",
      "-634.1298108100891\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "617.4277079105377\n",
      "-- train ok ! --\n",
      "fold 47: \n",
      "-- training manhattan--\n",
      "-634.358594417572\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "608.9196355342865\n",
      "-- train ok ! --\n",
      "fold 48: \n",
      "-- training manhattan--\n",
      "-639.3387594223022\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "609.1000416278839\n",
      "-- train ok ! --\n",
      "fold 49: \n",
      "-- training manhattan--\n",
      "-636.492448091507\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "609.5885875225067\n",
      "-- train ok ! --\n",
      "fold 50: \n",
      "-- training manhattan--\n",
      "-630.0265874862671\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "611.7436394691467\n",
      "-- train ok ! --\n",
      "0.8342499999999999\n",
      "0.8258333333333333\n",
      "fold 51: \n",
      "-- training manhattan--\n",
      "-634.1205866336823\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "610.6361286640167\n",
      "-- train ok ! --\n",
      "fold 52: \n",
      "-- training manhattan--\n",
      "-637.3507153987885\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "611.4271726608276\n",
      "-- train ok ! --\n",
      "fold 53: \n",
      "-- training manhattan--\n",
      "-633.9867551326752\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "619.2924332618713\n",
      "-- train ok ! --\n",
      "fold 54: \n",
      "-- training manhattan--\n",
      "-633.8560237884521\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "614.4321179389954\n",
      "-- train ok ! --\n",
      "fold 55: \n",
      "-- training manhattan--\n",
      "-640.3342361450195\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "607.6536798477173\n",
      "-- train ok ! --\n",
      "0.8334166666666667\n",
      "0.8245416666666667\n",
      "fold 56: \n",
      "-- training manhattan--\n",
      "-633.5155744552612\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "610.0215439796448\n",
      "-- train ok ! --\n",
      "fold 57: \n",
      "-- training manhattan--\n",
      "-656.5299491882324\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "612.897438287735\n",
      "-- train ok ! --\n",
      "fold 58: \n",
      "-- training manhattan--\n",
      "-634.2617383003235\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "621.7038650512695\n",
      "-- train ok ! --\n",
      "fold 59: \n",
      "-- training manhattan--\n",
      "-633.9616508483887\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "603.6430225372314\n",
      "-- train ok ! --\n",
      "fold 60: \n",
      "-- training manhattan--\n",
      "-634.679018497467\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "612.0727338790894\n",
      "-- train ok ! --\n",
      "0.8313124999999999\n",
      "0.8232083333333333\n",
      "fold 61: \n",
      "-- training manhattan--\n",
      "-633.6719825267792\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "603.3661606311798\n",
      "-- train ok ! --\n",
      "fold 62: \n",
      "-- training manhattan--\n",
      "-633.4040293693542\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "601.7003617286682\n",
      "-- train ok ! --\n",
      "fold 63: \n",
      "-- training manhattan--\n",
      "-634.2254354953766\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "619.9446432590485\n",
      "-- train ok ! --\n",
      "fold 64: \n",
      "-- training manhattan--\n",
      "-624.1091442108154\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "617.4726102352142\n",
      "-- train ok ! --\n",
      "fold 65: \n",
      "-- training manhattan--\n",
      "-638.592232465744\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "610.8017857074738\n",
      "-- train ok ! --\n",
      "0.8306875\n",
      "0.8216041666666666\n",
      "fold 66: \n",
      "-- training manhattan--\n",
      "-633.9992213249207\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614.1539695262909\n",
      "-- train ok ! --\n",
      "fold 67: \n",
      "-- training manhattan--\n",
      "-636.9144403934479\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "612.5433161258698\n",
      "-- train ok ! --\n",
      "fold 68: \n",
      "-- training manhattan--\n",
      "-632.9179444313049\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "606.4772231578827\n",
      "-- train ok ! --\n",
      "fold 69: \n",
      "-- training manhattan--\n",
      "-627.3359279632568\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "607.7244098186493\n",
      "-- train ok ! --\n",
      "fold 70: \n",
      "-- training manhattan--\n",
      "-633.1093571186066\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "608.0204548835754\n",
      "-- train ok ! --\n",
      "0.8287083333333332\n",
      "0.8208333333333334\n",
      "fold 71: \n",
      "-- training manhattan--\n",
      "-635.9064555168152\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "604.9109153747559\n",
      "-- train ok ! --\n",
      "fold 72: \n",
      "-- training manhattan--\n",
      "-626.8272154331207\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "618.1859004497528\n",
      "-- train ok ! --\n",
      "fold 73: \n",
      "-- training manhattan--\n",
      "-635.4604840278625\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "623.6462864875793\n",
      "-- train ok ! --\n",
      "fold 74: \n",
      "-- training manhattan--\n",
      "-636.9774971008301\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "617.8954877853394\n",
      "-- train ok ! --\n",
      "fold 75: \n",
      "-- training manhattan--\n",
      "-633.1315891742706\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "607.0251479148865\n",
      "-- train ok ! --\n",
      "0.827375\n",
      "0.8193125\n",
      "fold 76: \n",
      "-- training manhattan--\n",
      "-626.015777349472\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "616.2430310249329\n",
      "-- train ok ! --\n",
      "fold 77: \n",
      "-- training manhattan--\n",
      "-642.5263528823853\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "618.9210865497589\n",
      "-- train ok ! --\n",
      "fold 78: \n",
      "-- training manhattan--\n",
      "-633.3137588500977\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "602.0314974784851\n",
      "-- train ok ! --\n",
      "fold 79: \n",
      "-- training manhattan--\n",
      "-634.5083131790161\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "614.7932748794556\n",
      "-- train ok ! --\n",
      "fold 80: \n",
      "-- training manhattan--\n",
      "-633.7416985034943\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "620.1551835536957\n",
      "-- train ok ! --\n",
      "0.8256458333333333\n",
      "0.8175416666666667\n",
      "fold 81: \n",
      "-- training manhattan--\n",
      "-638.9827709197998\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "623.2521359920502\n",
      "-- train ok ! --\n",
      "fold 82: \n",
      "-- training manhattan--\n",
      "-633.9988420009613\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "615.1827945709229\n",
      "-- train ok ! --\n",
      "fold 83: \n",
      "-- training manhattan--\n",
      "-630.2350282669067\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "624.5706286430359\n",
      "-- train ok ! --\n",
      "fold 84: \n",
      "-- training manhattan--\n",
      "-640.9979994297028\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "615.9148981571198\n",
      "-- train ok ! --\n",
      "fold 85: \n",
      "-- training manhattan--\n",
      "-640.2135891914368\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "620.3243019580841\n",
      "-- train ok ! --\n",
      "0.8249583333333333\n",
      "0.8172708333333333\n",
      "fold 86: \n",
      "-- training manhattan--\n",
      "-639.1932592391968\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "622.787992477417\n",
      "-- train ok ! --\n",
      "fold 87: \n",
      "-- training manhattan--\n",
      "-639.9013564586639\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "620.8503210544586\n",
      "-- train ok ! --\n",
      "fold 88: \n",
      "-- training manhattan--\n",
      "-641.9540333747864\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "620.2321455478668\n",
      "-- train ok ! --\n",
      "fold 89: \n",
      "-- training manhattan--\n",
      "-632.356439113617\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "620.2933824062347\n",
      "-- train ok ! --\n",
      "fold 90: \n",
      "-- training manhattan--\n",
      "-627.2569725513458\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "610.77863240242\n",
      "-- train ok ! --\n",
      "0.8225416666666667\n",
      "0.8151458333333332\n",
      "fold 91: \n",
      "-- training manhattan--\n",
      "-628.8822395801544\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "616.1717638969421\n",
      "-- train ok ! --\n",
      "fold 92: \n",
      "-- training manhattan--\n",
      "-635.6780304908752\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "620.3960154056549\n",
      "-- train ok ! --\n",
      "fold 93: \n",
      "-- training manhattan--\n",
      "-640.0469567775726\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "612.5455210208893\n",
      "-- train ok ! --\n",
      "fold 94: \n",
      "-- training manhattan--\n",
      "-638.0676083564758\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "640.3188772201538\n",
      "-- train ok ! --\n",
      "fold 95: \n",
      "-- training manhattan--\n",
      "-641.4229309558868\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "622.3271164894104\n",
      "-- train ok ! --\n",
      "0.8231041666666666\n",
      "0.8149999999999998\n",
      "fold 96: \n",
      "-- training manhattan--\n",
      "-631.3887076377869\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "626.7757692337036\n",
      "-- train ok ! --\n",
      "fold 97: \n",
      "-- training manhattan--\n",
      "-640.2120230197906\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "612.1754758358002\n",
      "-- train ok ! --\n",
      "fold 98: \n",
      "-- training manhattan--\n",
      "-643.190586566925\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "624.6008665561676\n",
      "-- train ok ! --\n",
      "fold 99: \n",
      "-- training manhattan--\n",
      "-643.412516117096\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "616.6192941665649\n",
      "-- train ok ! --\n",
      "fold 100: \n",
      "-- training manhattan--\n",
      "-637.381055355072\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "614.896502494812\n",
      "-- train ok ! --\n",
      "0.8216458333333334\n",
      "0.8140416666666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for k in range(10, 101, 5):\n",
    "    performance_accuracy_manhattan = []\n",
    "    performance_accuracy_euclidean = []\n",
    "    times_manhattan = []\n",
    "    times_euclidean = []\n",
    "    pipe1 = Pipeline([('knn',KNeighborsClassifier(n_neighbors=k, p=1))])\n",
    "    pipe2 = Pipeline([('knn',KNeighborsClassifier(n_neighbors=k, p=2))])\n",
    "    for (train_index, test_index) in kfold.split(X_train):\n",
    "        \n",
    "        print('fold '+str(fold_count)+': ')\n",
    "        print('-- training manhattan--')\n",
    "        X_kfold_train, y_kfold_train = X_train.iloc[train_index], y_train.loc[train_index]\n",
    "        X_kfold_test, y_kfold_test = X_train.iloc[test_index], y_train.loc[test_index]\n",
    "        \n",
    "        start_manhattan = time.time()\n",
    "        pipe1.fit(X_kfold_train, y_kfold_train)\n",
    "        y_kfold_pred_manhattan = pipe1.predict(X_kfold_test)\n",
    "        end_manhattan = time.time()\n",
    "        train_time_manhattan = start_manhattan - end_manhattan\n",
    "        print(train_time_manhattan)\n",
    "        print('-- train ok ! --')\n",
    "        perf = accuracy_score(y_kfold_test, y_kfold_pred_manhattan)\n",
    "        performance_accuracy_manhattan.append(perf)\n",
    "        times_manhattan.append(train_time_manhattan)\n",
    "        \n",
    "        print('-- training euclidean--')\n",
    "        start_euclidean = time.time()\n",
    "        pipe2.fit(X_kfold_train, y_kfold_train)\n",
    "        y_kfold_pred_euclidean = pipe2.predict(X_kfold_test)\n",
    "        end_euclidean = time.time()\n",
    "        train_time_euclidean = end_euclidean-start_euclidean\n",
    "        print(train_time_euclidean)\n",
    "        print('-- train ok ! --')\n",
    "        perf = accuracy_score(y_kfold_test, y_kfold_pred_euclidean)\n",
    "        performance_accuracy_euclidean.append(perf)\n",
    "        times_euclidean.append(train_time_euclidean)\n",
    "\n",
    "        fold_count = fold_count + 1\n",
    "    validation_times_manhattan.append((k,np.mean(times_manhattan)))\n",
    "    accs_knn_manhattan.append((k,np.mean(performance_accuracy_manhattan)))\n",
    "    print(np.mean(performance_accuracy_manhattan))\n",
    "    validation_times_euclidean.append((k,np.mean(times_euclidean)))\n",
    "    accs_knn_euclidean.append((k,np.mean(performance_accuracy_euclidean)))\n",
    "    print(np.mean(performance_accuracy_euclidean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 101: \n",
      "-- training manhattan--\n",
      "-609.5566608905792\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "552.5739712715149\n",
      "-- train ok ! --\n",
      "fold 102: \n",
      "-- training manhattan--\n",
      "-588.5166642665863\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "553.286126613617\n",
      "-- train ok ! --\n",
      "fold 103: \n",
      "-- training manhattan--\n",
      "-613.4380753040314\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "549.0837290287018\n",
      "-- train ok ! --\n",
      "fold 104: \n",
      "-- training manhattan--\n",
      "-613.0619382858276\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "552.4760112762451\n",
      "-- train ok ! --\n",
      "fold 105: \n",
      "-- training manhattan--\n",
      "-614.5462417602539\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "558.0396122932434\n",
      "-- train ok ! --\n",
      "0.8485624999999999\n",
      "0.8438958333333332\n",
      "fold 106: \n",
      "-- training manhattan--\n",
      "-603.6853365898132\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "561.3836569786072\n",
      "-- train ok ! --\n",
      "fold 107: \n",
      "-- training manhattan--\n",
      "-602.1410927772522\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "569.5716996192932\n",
      "-- train ok ! --\n",
      "fold 108: \n",
      "-- training manhattan--\n",
      "-615.0076386928558\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "567.6108243465424\n",
      "-- train ok ! --\n",
      "fold 109: \n",
      "-- training manhattan--\n",
      "-618.8497891426086\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "556.6743230819702\n",
      "-- train ok ! --\n",
      "fold 110: \n",
      "-- training manhattan--\n",
      "-617.8625602722168\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "567.4817571640015\n",
      "-- train ok ! --\n",
      "0.84375\n",
      "0.842875\n",
      "fold 111: \n",
      "-- training manhattan--\n",
      "-619.1448087692261\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "567.3206732273102\n",
      "-- train ok ! --\n",
      "fold 112: \n",
      "-- training manhattan--\n",
      "-608.1432061195374\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "578.1647732257843\n",
      "-- train ok ! --\n",
      "fold 113: \n",
      "-- training manhattan--\n",
      "-622.8395798206329\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "575.4962723255157\n",
      "-- train ok ! --\n",
      "fold 114: \n",
      "-- training manhattan--\n",
      "-620.1065418720245\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "560.7561740875244\n",
      "-- train ok ! --\n",
      "fold 115: \n",
      "-- training manhattan--\n",
      "-621.2743294239044\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "577.980840921402\n",
      "-- train ok ! --\n",
      "0.8547916666666666\n",
      "0.8497708333333334\n",
      "fold 116: \n",
      "-- training manhattan--\n",
      "-621.1922540664673\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "567.5288906097412\n",
      "-- train ok ! --\n",
      "fold 117: \n",
      "-- training manhattan--\n",
      "-623.6521129608154\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "589.1298763751984\n",
      "-- train ok ! --\n",
      "fold 118: \n",
      "-- training manhattan--\n",
      "-624.8903965950012\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "577.5097975730896\n",
      "-- train ok ! --\n",
      "fold 119: \n",
      "-- training manhattan--\n",
      "-620.6934840679169\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "577.3974227905273\n",
      "-- train ok ! --\n",
      "fold 120: \n",
      "-- training manhattan--\n",
      "-621.1727802753448\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "573.670670747757\n",
      "-- train ok ! --\n",
      "0.8558333333333333\n",
      "0.8519791666666666\n",
      "fold 121: \n",
      "-- training manhattan--\n",
      "-624.3101263046265\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "578.0585997104645\n",
      "-- train ok ! --\n",
      "fold 122: \n",
      "-- training manhattan--\n",
      "-625.4247803688049\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "582.4069576263428\n",
      "-- train ok ! --\n",
      "fold 123: \n",
      "-- training manhattan--\n",
      "-625.8743734359741\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "579.9851417541504\n",
      "-- train ok ! --\n",
      "fold 124: \n",
      "-- training manhattan--\n",
      "-626.6359822750092\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "577.7256927490234\n",
      "-- train ok ! --\n",
      "fold 125: \n",
      "-- training manhattan--\n",
      "-621.7493622303009\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "571.4494454860687\n",
      "-- train ok ! --\n",
      "0.8560000000000001\n",
      "0.8492291666666667\n",
      "fold 126: \n",
      "-- training manhattan--\n",
      "-618.4232439994812\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "590.2003262042999\n",
      "-- train ok ! --\n",
      "fold 127: \n",
      "-- training manhattan--\n",
      "-628.9343121051788\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "580.3617258071899\n",
      "-- train ok ! --\n",
      "fold 128: \n",
      "-- training manhattan--\n",
      "-627.9495775699615\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "585.5584709644318\n",
      "-- train ok ! --\n",
      "fold 129: \n",
      "-- training manhattan--\n",
      "-622.6131160259247\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "584.5763850212097\n",
      "-- train ok ! --\n",
      "fold 130: \n",
      "-- training manhattan--\n",
      "-626.4276347160339\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "589.0927941799164\n",
      "-- train ok ! --\n",
      "0.856625\n",
      "0.850375\n",
      "fold 131: \n",
      "-- training manhattan--\n",
      "-628.7392976284027\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "585.0375273227692\n",
      "-- train ok ! --\n",
      "fold 132: \n",
      "-- training manhattan--\n",
      "-622.6456151008606\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "580.8590683937073\n",
      "-- train ok ! --\n",
      "fold 133: \n",
      "-- training manhattan--\n",
      "-626.6484959125519\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "583.6750202178955\n",
      "-- train ok ! --\n",
      "fold 134: \n",
      "-- training manhattan--\n",
      "-626.0438375473022\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "584.7890713214874\n",
      "-- train ok ! --\n",
      "fold 135: \n",
      "-- training manhattan--\n",
      "-629.7908446788788\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "582.3888375759125\n",
      "-- train ok ! --\n",
      "0.8561666666666665\n",
      "0.851375\n",
      "fold 136: \n",
      "-- training manhattan--\n",
      "-626.695175409317\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "590.0096487998962\n",
      "-- train ok ! --\n",
      "fold 137: \n",
      "-- training manhattan--\n",
      "-622.3047604560852\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "578.8021025657654\n",
      "-- train ok ! --\n",
      "fold 138: \n",
      "-- training manhattan--\n",
      "-605.975105047226\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "591.2724750041962\n",
      "-- train ok ! --\n",
      "fold 139: \n",
      "-- training manhattan--\n",
      "-627.948691368103\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "586.4797520637512\n",
      "-- train ok ! --\n",
      "fold 140: \n",
      "-- training manhattan--\n",
      "-626.8027160167694\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "588.6200234889984\n",
      "-- train ok ! --\n",
      "0.8561666666666667\n",
      "0.8497499999999999\n",
      "fold 141: \n",
      "-- training manhattan--\n",
      "-625.4936590194702\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "587.5330953598022\n",
      "-- train ok ! --\n",
      "fold 142: \n",
      "-- training manhattan--\n",
      "-625.3334383964539\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "587.8130497932434\n",
      "-- train ok ! --\n",
      "fold 143: \n",
      "-- training manhattan--\n",
      "-630.7421402931213\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "588.9074294567108\n",
      "-- train ok ! --\n",
      "fold 144: \n",
      "-- training manhattan--\n",
      "-631.0836699008942\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "583.9237444400787\n",
      "-- train ok ! --\n",
      "fold 145: \n",
      "-- training manhattan--\n",
      "-619.737542629242\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "591.6719512939453\n",
      "-- train ok ! --\n",
      "0.8541041666666667\n",
      "0.8481041666666667\n",
      "fold 146: \n",
      "-- training manhattan--\n",
      "-630.4603095054626\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "596.8739438056946\n",
      "-- train ok ! --\n",
      "fold 147: \n",
      "-- training manhattan--\n",
      "-618.404824256897\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "592.0950236320496\n",
      "-- train ok ! --\n",
      "fold 148: \n",
      "-- training manhattan--\n",
      "-628.7597846984863\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "597.1708991527557\n",
      "-- train ok ! --\n",
      "fold 149: \n",
      "-- training manhattan--\n",
      "-629.6403732299805\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "585.7811815738678\n",
      "-- train ok ! --\n",
      "fold 150: \n",
      "-- training manhattan--\n",
      "-612.346893787384\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "588.4438500404358\n",
      "-- train ok ! --\n",
      "0.8554375000000001\n",
      "0.8483750000000001\n",
      "fold 151: \n",
      "-- training manhattan--\n",
      "-627.4320871829987\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "578.4712543487549\n",
      "-- train ok ! --\n",
      "fold 152: \n",
      "-- training manhattan--\n",
      "-623.2763888835907\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "589.9087266921997\n",
      "-- train ok ! --\n",
      "fold 153: \n",
      "-- training manhattan--\n",
      "-625.0018870830536\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "597.4497709274292\n",
      "-- train ok ! --\n",
      "fold 154: \n",
      "-- training manhattan--\n",
      "-623.4177963733673\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "587.7957322597504\n",
      "-- train ok ! --\n",
      "fold 155: \n",
      "-- training manhattan--\n",
      "-630.5518629550934\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "590.7698268890381\n",
      "-- train ok ! --\n",
      "0.8534166666666667\n",
      "0.8473958333333333\n",
      "fold 156: \n",
      "-- training manhattan--\n",
      "-630.8362338542938\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "596.8251464366913\n",
      "-- train ok ! --\n",
      "fold 157: \n",
      "-- training manhattan--\n",
      "-627.8561005592346\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "592.1867051124573\n",
      "-- train ok ! --\n",
      "fold 158: \n",
      "-- training manhattan--\n",
      "-623.0848743915558\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "586.6711637973785\n",
      "-- train ok ! --\n",
      "fold 159: \n",
      "-- training manhattan--\n",
      "-640.2420454025269\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "603.0427753925323\n",
      "-- train ok ! --\n",
      "fold 160: \n",
      "-- training manhattan--\n",
      "-623.6721258163452\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "596.0440032482147\n",
      "-- train ok ! --\n",
      "0.8536041666666666\n",
      "0.8470416666666667\n",
      "fold 161: \n",
      "-- training manhattan--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-625.7112503051758\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "599.0907652378082\n",
      "-- train ok ! --\n",
      "fold 162: \n",
      "-- training manhattan--\n",
      "-627.6438784599304\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "592.3292922973633\n",
      "-- train ok ! --\n",
      "fold 163: \n",
      "-- training manhattan--\n",
      "-632.1533749103546\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "591.5854268074036\n",
      "-- train ok ! --\n",
      "fold 164: \n",
      "-- training manhattan--\n",
      "-632.8184990882874\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "597.4998610019684\n",
      "-- train ok ! --\n",
      "fold 165: \n",
      "-- training manhattan--\n",
      "-613.9819095134735\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "593.3849155902863\n",
      "-- train ok ! --\n",
      "0.8513124999999999\n",
      "0.8449375\n",
      "fold 166: \n",
      "-- training manhattan--\n",
      "-634.4882731437683\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "604.7982614040375\n",
      "-- train ok ! --\n",
      "fold 167: \n",
      "-- training manhattan--\n",
      "-631.3054442405701\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "587.031809091568\n",
      "-- train ok ! --\n",
      "fold 168: \n",
      "-- training manhattan--\n",
      "-620.8222072124481\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "587.9437448978424\n",
      "-- train ok ! --\n",
      "fold 169: \n",
      "-- training manhattan--\n",
      "-633.4628789424896\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "594.9016351699829\n",
      "-- train ok ! --\n",
      "fold 170: \n",
      "-- training manhattan--\n",
      "-635.7050712108612\n",
      "-- train ok ! --\n",
      "-- training euclidean--\n",
      "600.0481083393097\n",
      "-- train ok ! --\n",
      "0.8514166666666668\n",
      "0.8448333333333334\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 15):\n",
    "    performance_accuracy_manhattan = []\n",
    "    performance_accuracy_euclidean = []\n",
    "    times_manhattan = []\n",
    "    times_euclidean = []\n",
    "    pipe1 = Pipeline([('knn',KNeighborsClassifier(n_neighbors=k, p=1))])\n",
    "    pipe2 = Pipeline([('knn',KNeighborsClassifier(n_neighbors=k, p=2))])\n",
    "    for (train_index, test_index) in kfold.split(X_train):\n",
    "        \n",
    "        print('fold '+str(fold_count)+': ')\n",
    "        print('-- training manhattan--')\n",
    "        X_kfold_train, y_kfold_train = X_train.iloc[train_index], y_train.loc[train_index]\n",
    "        X_kfold_test, y_kfold_test = X_train.iloc[test_index], y_train.loc[test_index]\n",
    "        \n",
    "        start_manhattan = time.time()\n",
    "        pipe1.fit(X_kfold_train, y_kfold_train)\n",
    "        y_kfold_pred_manhattan = pipe1.predict(X_kfold_test)\n",
    "        end_manhattan = time.time()\n",
    "        train_time_manhattan = start_manhattan - end_manhattan\n",
    "        print(train_time_manhattan)\n",
    "        print('-- train ok ! --')\n",
    "        perf = accuracy_score(y_kfold_test, y_kfold_pred_manhattan)\n",
    "        performance_accuracy_manhattan.append(perf)\n",
    "        times_manhattan.append(train_time_manhattan)\n",
    "        \n",
    "        print('-- training euclidean--')\n",
    "        start_euclidean = time.time()\n",
    "        pipe2.fit(X_kfold_train, y_kfold_train)\n",
    "        y_kfold_pred_euclidean = pipe2.predict(X_kfold_test)\n",
    "        end_euclidean = time.time()\n",
    "        train_time_euclidean = end_euclidean-start_euclidean\n",
    "        print(train_time_euclidean)\n",
    "        print('-- train ok ! --')\n",
    "        perf = accuracy_score(y_kfold_test, y_kfold_pred_euclidean)\n",
    "        performance_accuracy_euclidean.append(perf)\n",
    "        times_euclidean.append(train_time_euclidean)\n",
    "\n",
    "        fold_count = fold_count + 1\n",
    "    validation_times_manhattan.append((k,np.mean(times_manhattan)))\n",
    "    accs_knn_manhattan.append((k,np.mean(performance_accuracy_manhattan)))\n",
    "    print(np.mean(performance_accuracy_manhattan))\n",
    "    validation_times_euclidean.append((k,np.mean(times_euclidean)))\n",
    "    accs_knn_euclidean.append((k,np.mean(performance_accuracy_euclidean)))\n",
    "    print(np.mean(performance_accuracy_euclidean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.844896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.848542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>0.844562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>0.839917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>0.835792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>0.834125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>35</td>\n",
       "      <td>0.831417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40</td>\n",
       "      <td>0.830042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>45</td>\n",
       "      <td>0.827875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50</td>\n",
       "      <td>0.825833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>55</td>\n",
       "      <td>0.824542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>60</td>\n",
       "      <td>0.823208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>65</td>\n",
       "      <td>0.821604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>70</td>\n",
       "      <td>0.820833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>75</td>\n",
       "      <td>0.819312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>80</td>\n",
       "      <td>0.817542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>85</td>\n",
       "      <td>0.817271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>90</td>\n",
       "      <td>0.815146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>95</td>\n",
       "      <td>0.815000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>100</td>\n",
       "      <td>0.814042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0.843896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>0.842875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>0.849771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>0.851979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5</td>\n",
       "      <td>0.849229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6</td>\n",
       "      <td>0.850375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7</td>\n",
       "      <td>0.851375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8</td>\n",
       "      <td>0.849750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9</td>\n",
       "      <td>0.848104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10</td>\n",
       "      <td>0.848375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>11</td>\n",
       "      <td>0.847396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>12</td>\n",
       "      <td>0.847042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>13</td>\n",
       "      <td>0.844938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>14</td>\n",
       "      <td>0.844833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      k  accuracy\n",
       "0     1  0.844896\n",
       "1    10  0.848542\n",
       "2    15  0.844562\n",
       "3    20  0.839917\n",
       "4    25  0.835792\n",
       "5    30  0.834125\n",
       "6    35  0.831417\n",
       "7    40  0.830042\n",
       "8    45  0.827875\n",
       "9    50  0.825833\n",
       "10   55  0.824542\n",
       "11   60  0.823208\n",
       "12   65  0.821604\n",
       "13   70  0.820833\n",
       "14   75  0.819312\n",
       "15   80  0.817542\n",
       "16   85  0.817271\n",
       "17   90  0.815146\n",
       "18   95  0.815000\n",
       "19  100  0.814042\n",
       "20    1  0.843896\n",
       "21    2  0.842875\n",
       "22    3  0.849771\n",
       "23    4  0.851979\n",
       "24    5  0.849229\n",
       "25    6  0.850375\n",
       "26    7  0.851375\n",
       "27    8  0.849750\n",
       "28    9  0.848104\n",
       "29   10  0.848375\n",
       "30   11  0.847396\n",
       "31   12  0.847042\n",
       "32   13  0.844938\n",
       "33   14  0.844833"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_knn_euclidean = pd.DataFrame(accs_knn_euclidean, columns=['k', 'accuracy'])\n",
    "df_knn_euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, -609.8320737361908),\n",
       " (10, -623.4061962127686),\n",
       " (15, -628.2184817314148),\n",
       " (20, -625.1550208091736),\n",
       " (25, -634.689629125595),\n",
       " (30, -634.7695893764496),\n",
       " (35, -634.9252849578858),\n",
       " (40, -638.4218605041503),\n",
       " (45, -625.1453896522522),\n",
       " (50, -634.8692400455475),\n",
       " (55, -635.9296634197235),\n",
       " (60, -638.5895862579346),\n",
       " (65, -632.8005648136138),\n",
       " (70, -632.8553782463074),\n",
       " (75, -633.6606482505798),\n",
       " (80, -634.0211801528931),\n",
       " (85, -636.8856459617615),\n",
       " (90, -636.132412147522),\n",
       " (95, -636.819553232193),\n",
       " (100, -639.1169777393341),\n",
       " (1, -607.8239161014557),\n",
       " (2, -611.5092834949494),\n",
       " (3, -618.3016932010651),\n",
       " (4, -622.3202055931091),\n",
       " (5, -624.7989249229431),\n",
       " (6, -624.8695768833161),\n",
       " (7, -626.7736181735993),\n",
       " (8, -621.9452896595001),\n",
       " (9, -626.4780900478363),\n",
       " (10, -623.9224370956421),\n",
       " (11, -625.9360044956208),\n",
       " (12, -629.1382760047912),\n",
       " (13, -626.4617824554443),\n",
       " (14, -631.1567749500275)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_times_manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_times_manhattan_fixed = validation_times_manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 609.8320737361908),\n",
       " (10, 623.4061962127686),\n",
       " (15, 628.2184817314148),\n",
       " (20, 625.1550208091736),\n",
       " (25, 634.689629125595),\n",
       " (30, 634.7695893764496),\n",
       " (35, 634.9252849578858),\n",
       " (40, 638.4218605041503),\n",
       " (45, 625.1453896522522),\n",
       " (50, 634.8692400455475),\n",
       " (55, 635.9296634197235),\n",
       " (60, 638.5895862579346),\n",
       " (65, 632.8005648136138),\n",
       " (70, 632.8553782463074),\n",
       " (75, 633.6606482505798),\n",
       " (80, 634.0211801528931),\n",
       " (85, 636.8856459617615),\n",
       " (90, 636.132412147522),\n",
       " (95, 636.819553232193),\n",
       " (100, 639.1169777393341),\n",
       " (1, 607.8239161014557),\n",
       " (2, 611.5092834949494),\n",
       " (3, 618.3016932010651),\n",
       " (4, 622.3202055931091),\n",
       " (5, 624.7989249229431),\n",
       " (6, 624.8695768833161),\n",
       " (7, 626.7736181735993),\n",
       " (8, 621.9452896595001),\n",
       " (9, 626.4780900478363),\n",
       " (10, 623.9224370956421),\n",
       " (11, 625.9360044956208),\n",
       " (12, 629.1382760047912),\n",
       " (13, 626.4617824554443),\n",
       " (14, 631.1567749500275)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_times_manhattan_fixed = []\n",
    "for i in range(0, len(validation_times_manhattan)):\n",
    "    data = list(validation_times_manhattan[i])\n",
    "    data[1] = data[1] * (-1)\n",
    "    validation_times_manhattan_fixed.append(data)\n",
    "    validation_times_manhattan_fixed[i] = tuple(validation_times_manhattan_fixed[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knn_manhattan = pd.DataFrame(accs_knn_manhattan, columns=['k', 'accuracy'])\n",
    "df_knn_manhattan.to_csv('results/data_knn_manhattan.csv', index=False)\n",
    "df_validation_times_manhattan = pd.DataFrame(validation_times_manhattan_fixed, columns=['k', 'time'])\n",
    "df_validation_times_manhattan.to_csv('results/times_knn_manhattan.csv', index=False)\n",
    "df_knn_eculidean = pd.DataFrame(accs_knn_euclidean, columns=['k', 'accuracy'])\n",
    "df_knn_euclidean.to_csv('results/data_knn_euclidean.csv', index=False)\n",
    "df_validation_times_euclidean = pd.DataFrame(validation_times_euclidean, columns=['k', 'time'])\n",
    "df_validation_times_euclidean.to_csv('results/times_knn_euclidean.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "fig = sns.lineplot(data=df_knn_manhattan, x='k', y='accuracy', marker='o')\n",
    "plt.gca().locator_params(nbins=20)\n",
    "fig.set(xlabel='k', ylabel='Accuracy',title='Accuracy en funcion de k');\n",
    "fig = sns.lineplot(data=df_knn_euclidean, x='k', y='accuracy', marker='o')\n",
    "plt.gca().locator_params(nbins=20)\n",
    "fig.set(xlabel='k', ylabel='Accuracy',title='Accuracy en funcion de k');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = sns.lineplot(data=df_validation_times_manhattan, x='k', y='time', marker='o')\n",
    "plt.gca().locator_params(nbins=20)\n",
    "fig.set(xlabel='k', ylabel='Accuracy',title='Accuracy en funcion de k');\n",
    "fig = sns.lineplot(data=df_validation_times_euclidean, x='k', y='time', marker='o')\n",
    "plt.gca().locator_params(nbins=20)\n",
    "fig.set(xlabel='k', ylabel='Accuracy',title='Accuracy en funcion de k');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20, 0.8397291666666667),\n",
       " (25, 0.8376250000000001),\n",
       " (30, 0.8329583333333334),\n",
       " (35, 0.8324374999999999),\n",
       " (40, 0.8297916666666666),\n",
       " (45, 0.8266249999999999),\n",
       " (50, 0.8257708333333333),\n",
       " (1, 0.8436041666666666),\n",
       " (6, 0.851125),\n",
       " (11, 0.8469166666666668),\n",
       " (16, 0.8442083333333334),\n",
       " (1, 0.843625),\n",
       " (6, 0.8503125),\n",
       " (11, 0.8463125),\n",
       " (16, 0.8438541666666666)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs_knn_euclidean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento con la mitad de los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "\n",
    "data_30mil = data[:30000]\n",
    "X,y = data_30mil.drop(['label'], axis = 1), data_30mil['label']\n",
    "X_train_30k, X_test_30k, y_train_30k, y_test_30k = train_test_split(X, y, test_size = 0.2, random_state = SEED)\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "X_train_30k = X_train_30k.reset_index(drop=True)\n",
    "\n",
    "y_train_30k.index = X_train_30k.index\n",
    "performance_accuracy = []\n",
    "\n",
    "fold_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1: \n",
      "-- training --\n",
      "154.75428867340088\n",
      "-- train ok ! --\n",
      "fold 2: \n",
      "-- training --\n",
      "155.84276127815247\n",
      "-- train ok ! --\n",
      "fold 3: \n",
      "-- training --\n",
      "154.36296677589417\n",
      "-- train ok ! --\n",
      "fold 4: \n",
      "-- training --\n",
      "155.18692278862\n",
      "-- train ok ! --\n",
      "fold 5: \n",
      "-- training --\n",
      "154.40520930290222\n",
      "-- train ok ! --\n",
      "fold 6: \n",
      "-- training --\n",
      "159.88154196739197\n",
      "-- train ok ! --\n",
      "fold 7: \n",
      "-- training --\n",
      "156.86418795585632\n",
      "-- train ok ! --\n",
      "fold 8: \n",
      "-- training --\n",
      "154.12760496139526\n",
      "-- train ok ! --\n",
      "fold 9: \n",
      "-- training --\n",
      "155.12173199653625\n",
      "-- train ok ! --\n",
      "fold 10: \n",
      "-- training --\n",
      "159.1857030391693\n",
      "-- train ok ! --\n",
      "0.8455416666666666\n",
      "fold 11: \n",
      "-- training --\n",
      "158.12714433670044\n",
      "-- train ok ! --\n",
      "fold 12: \n",
      "-- training --\n",
      "158.27232789993286\n",
      "-- train ok ! --\n",
      "fold 13: \n",
      "-- training --\n",
      "159.45046377182007\n",
      "-- train ok ! --\n",
      "fold 14: \n",
      "-- training --\n",
      "159.24630689620972\n",
      "-- train ok ! --\n",
      "fold 15: \n",
      "-- training --\n",
      "158.37516808509827\n",
      "-- train ok ! --\n",
      "0.8418749999999999\n",
      "fold 16: \n",
      "-- training --\n",
      "160.08305191993713\n",
      "-- train ok ! --\n",
      "fold 17: \n",
      "-- training --\n",
      "159.6512372493744\n",
      "-- train ok ! --\n",
      "fold 18: \n",
      "-- training --\n",
      "159.77135753631592\n",
      "-- train ok ! --\n",
      "fold 19: \n",
      "-- training --\n",
      "159.82612705230713\n",
      "-- train ok ! --\n",
      "fold 20: \n",
      "-- training --\n",
      "156.85642075538635\n",
      "-- train ok ! --\n",
      "0.8376250000000001\n",
      "fold 21: \n",
      "-- training --\n",
      "158.74587059020996\n",
      "-- train ok ! --\n",
      "fold 22: \n",
      "-- training --\n",
      "154.6758668422699\n",
      "-- train ok ! --\n",
      "fold 23: \n",
      "-- training --\n",
      "155.20546984672546\n",
      "-- train ok ! --\n",
      "fold 24: \n",
      "-- training --\n",
      "157.4929904937744\n",
      "-- train ok ! --\n",
      "fold 25: \n",
      "-- training --\n",
      "159.17690324783325\n",
      "-- train ok ! --\n",
      "0.8343333333333334\n",
      "fold 26: \n",
      "-- training --\n",
      "159.3288745880127\n",
      "-- train ok ! --\n",
      "fold 27: \n",
      "-- training --\n",
      "159.49535655975342\n",
      "-- train ok ! --\n",
      "fold 28: \n",
      "-- training --\n",
      "159.71115350723267\n",
      "-- train ok ! --\n",
      "fold 29: \n",
      "-- training --\n",
      "160.75352358818054\n",
      "-- train ok ! --\n",
      "fold 30: \n",
      "-- training --\n",
      "157.39844298362732\n",
      "-- train ok ! --\n",
      "0.83025\n",
      "fold 31: \n",
      "-- training --\n",
      "155.06691312789917\n",
      "-- train ok ! --\n",
      "fold 32: \n",
      "-- training --\n",
      "156.9428834915161\n",
      "-- train ok ! --\n",
      "fold 33: \n",
      "-- training --\n",
      "162.73326063156128\n",
      "-- train ok ! --\n",
      "fold 34: \n",
      "-- training --\n",
      "160.77609729766846\n",
      "-- train ok ! --\n",
      "fold 35: \n",
      "-- training --\n",
      "159.60769844055176\n",
      "-- train ok ! --\n",
      "0.8298333333333334\n",
      "fold 36: \n",
      "-- training --\n",
      "155.41652870178223\n",
      "-- train ok ! --\n",
      "fold 37: \n",
      "-- training --\n",
      "157.02500128746033\n",
      "-- train ok ! --\n",
      "fold 38: \n",
      "-- training --\n",
      "157.82080030441284\n",
      "-- train ok ! --\n",
      "fold 39: \n",
      "-- training --\n",
      "157.53066277503967\n",
      "-- train ok ! --\n",
      "fold 40: \n",
      "-- training --\n",
      "161.69546723365784\n",
      "-- train ok ! --\n",
      "0.8252499999999999\n",
      "fold 41: \n",
      "-- training --\n",
      "157.58890795707703\n",
      "-- train ok ! --\n",
      "fold 42: \n",
      "-- training --\n",
      "157.78523182868958\n",
      "-- train ok ! --\n",
      "fold 43: \n",
      "-- training --\n",
      "160.73634314537048\n",
      "-- train ok ! --\n",
      "fold 44: \n",
      "-- training --\n",
      "162.14729070663452\n",
      "-- train ok ! --\n",
      "fold 45: \n",
      "-- training --\n",
      "160.27878308296204\n",
      "-- train ok ! --\n",
      "0.8246249999999999\n",
      "fold 46: \n",
      "-- training --\n",
      "163.77365112304688\n",
      "-- train ok ! --\n",
      "fold 47: \n",
      "-- training --\n",
      "160.84495496749878\n",
      "-- train ok ! --\n",
      "fold 48: \n",
      "-- training --\n",
      "159.26937747001648\n",
      "-- train ok ! --\n",
      "fold 49: \n",
      "-- training --\n",
      "159.25612998008728\n",
      "-- train ok ! --\n",
      "fold 50: \n",
      "-- training --\n",
      "159.46411037445068\n",
      "-- train ok ! --\n",
      "0.8223333333333335\n",
      "fold 51: \n",
      "-- training --\n",
      "159.65934920310974\n",
      "-- train ok ! --\n",
      "fold 52: \n",
      "-- training --\n",
      "176.56128644943237\n",
      "-- train ok ! --\n",
      "fold 53: \n",
      "-- training --\n",
      "177.55349040031433\n",
      "-- train ok ! --\n",
      "fold 54: \n",
      "-- training --\n",
      "156.16623997688293\n",
      "-- train ok ! --\n",
      "fold 55: \n",
      "-- training --\n",
      "157.5215826034546\n",
      "-- train ok ! --\n",
      "0.8208750000000002\n"
     ]
    }
   ],
   "source": [
    "accs_knn_manhattan = []\n",
    "validation_times = []\n",
    "\n",
    "performance_accuracy = []\n",
    "pipe = Pipeline([('knn',KNeighborsClassifier(n_neighbors=1, p=1))])\n",
    "for (train_index, test_index) in kfold.split(X_train_30k):\n",
    "    print('fold '+str(fold_count)+': ')\n",
    "    print('-- training --')\n",
    "    X_kfold_train_30k, y_kfold_train_30k = X_train_30k.iloc[train_index], y_train_30k.loc[train_index]\n",
    "    X_kfold_test_30k, y_kfold_test_30k = X_train_30k.iloc[test_index], y_train_30k.loc[test_index]\n",
    "    start = time.time()\n",
    "    pipe.fit(X_kfold_train_30k, y_kfold_train_30k)\n",
    "    y_kfold_pred_30k = pipe.predict(X_kfold_test_30k)\n",
    "    end = time.time()\n",
    "    train_time = end-start\n",
    "    print(train_time)\n",
    "    print('-- train ok ! --')\n",
    "    perf = accuracy_score(y_kfold_test_30k, y_kfold_pred_30k)\n",
    "    performance_accuracy.append(perf)\n",
    "\n",
    "    fold_count = fold_count + 1\n",
    "\n",
    "for k in range(5, 51, 5): \n",
    "    performance_accuracy = []\n",
    "    pipe = Pipeline([('knn',KNeighborsClassifier(n_neighbors=k, p=1))])\n",
    "    for (train_index, test_index) in kfold.split(X_train_30k):\n",
    "        print('fold '+str(fold_count)+': ')\n",
    "        print('-- training --')\n",
    "        X_kfold_train_30k, y_kfold_train_30k = X_train_30k.iloc[train_index], y_train_30k.loc[train_index]\n",
    "        X_kfold_test_30k, y_kfold_test_30k = X_train_30k.iloc[test_index], y_train_30k.loc[test_index]\n",
    "        start = time.time()\n",
    "        pipe.fit(X_kfold_train_30k, y_kfold_train_30k)\n",
    "        y_kfold_pred_30k = pipe.predict(X_kfold_test_30k)\n",
    "        end = time.time()\n",
    "        train_time = end-start\n",
    "        print(train_time)\n",
    "        print('-- train ok ! --')\n",
    "        perf = accuracy_score(y_kfold_test_30k, y_kfold_pred_30k)\n",
    "        performance_accuracy.append(perf)\n",
    "\n",
    "        fold_count = fold_count + 1\n",
    "    validation_times.append((k,np.mean(train_time)))\n",
    "    accs_knn_manhattan.append((k,np.mean(performance_accuracy)))\n",
    "    print(np.mean(performance_accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 56: \n",
      "-- training --\n",
      "150.5924470424652\n",
      "-- train ok ! --\n",
      "fold 57: \n",
      "-- training --\n",
      "154.5986852645874\n",
      "-- train ok ! --\n",
      "fold 58: \n",
      "-- training --\n",
      "156.0656294822693\n",
      "-- train ok ! --\n",
      "fold 59: \n",
      "-- training --\n",
      "156.06196546554565\n",
      "-- train ok ! --\n",
      "fold 60: \n",
      "-- training --\n",
      "154.69167518615723\n",
      "-- train ok ! --\n",
      "0.8375833333333335\n",
      "fold 61: \n",
      "-- training --\n",
      "154.95829010009766\n",
      "-- train ok ! --\n",
      "fold 62: \n",
      "-- training --\n",
      "155.89157390594482\n",
      "-- train ok ! --\n",
      "fold 63: \n",
      "-- training --\n",
      "154.92463183403015\n",
      "-- train ok ! --\n",
      "fold 64: \n",
      "-- training --\n",
      "151.51438856124878\n",
      "-- train ok ! --\n",
      "fold 65: \n",
      "-- training --\n",
      "155.1416893005371\n",
      "-- train ok ! --\n",
      "0.833\n",
      "fold 66: \n",
      "-- training --\n",
      "157.20577454566956\n",
      "-- train ok ! --\n",
      "fold 67: \n",
      "-- training --\n",
      "156.75193071365356\n",
      "-- train ok ! --\n",
      "fold 68: \n",
      "-- training --\n",
      "157.49774646759033\n",
      "-- train ok ! --\n",
      "fold 69: \n",
      "-- training --\n",
      "157.52874946594238\n",
      "-- train ok ! --\n",
      "fold 70: \n",
      "-- training --\n",
      "153.39998292922974\n",
      "-- train ok ! --\n",
      "0.8441666666666668\n",
      "fold 71: \n",
      "-- training --\n",
      "154.8115656375885\n",
      "-- train ok ! --\n",
      "fold 72: \n",
      "-- training --\n",
      "157.06518411636353\n",
      "-- train ok ! --\n",
      "fold 73: \n",
      "-- training --\n",
      "157.20786476135254\n",
      "-- train ok ! --\n",
      "fold 74: \n",
      "-- training --\n",
      "157.88284420967102\n",
      "-- train ok ! --\n",
      "fold 75: \n",
      "-- training --\n",
      "156.84349155426025\n",
      "-- train ok ! --\n",
      "0.8456666666666666\n",
      "fold 76: \n",
      "-- training --\n",
      "157.69366478919983\n",
      "-- train ok ! --\n",
      "fold 77: \n",
      "-- training --\n",
      "156.67352294921875\n",
      "-- train ok ! --\n",
      "fold 78: \n",
      "-- training --\n",
      "158.0476644039154\n",
      "-- train ok ! --\n",
      "fold 79: \n",
      "-- training --\n",
      "155.77901935577393\n",
      "-- train ok ! --\n",
      "fold 80: \n",
      "-- training --\n",
      "156.56327867507935\n",
      "-- train ok ! --\n",
      "0.8460416666666667\n",
      "fold 81: \n",
      "-- training --\n",
      "157.5775020122528\n",
      "-- train ok ! --\n",
      "fold 82: \n",
      "-- training --\n",
      "157.87979197502136\n",
      "-- train ok ! --\n",
      "fold 83: \n",
      "-- training --\n",
      "151.2650225162506\n",
      "-- train ok ! --\n",
      "fold 84: \n",
      "-- training --\n",
      "154.37729239463806\n",
      "-- train ok ! --\n",
      "fold 85: \n",
      "-- training --\n",
      "158.07670378684998\n",
      "-- train ok ! --\n",
      "0.8455\n",
      "fold 86: \n",
      "-- training --\n",
      "157.5812473297119\n",
      "-- train ok ! --\n",
      "fold 87: \n",
      "-- training --\n",
      "156.43340587615967\n",
      "-- train ok ! --\n",
      "fold 88: \n",
      "-- training --\n",
      "157.51065182685852\n",
      "-- train ok ! --\n",
      "fold 89: \n",
      "-- training --\n",
      "155.0224072933197\n",
      "-- train ok ! --\n",
      "fold 90: \n",
      "-- training --\n",
      "157.6700325012207\n",
      "-- train ok ! --\n",
      "0.843125\n",
      "fold 91: \n",
      "-- training --\n",
      "158.4430856704712\n",
      "-- train ok ! --\n",
      "fold 92: \n",
      "-- training --\n",
      "155.95503282546997\n",
      "-- train ok ! --\n",
      "fold 93: \n",
      "-- training --\n",
      "154.24973225593567\n",
      "-- train ok ! --\n",
      "fold 94: \n",
      "-- training --\n",
      "154.35915517807007\n",
      "-- train ok ! --\n",
      "fold 95: \n",
      "-- training --\n",
      "157.18680095672607\n",
      "-- train ok ! --\n",
      "0.8432083333333334\n",
      "fold 96: \n",
      "-- training --\n",
      "158.94002890586853\n",
      "-- train ok ! --\n",
      "fold 97: \n",
      "-- training --\n",
      "157.70230627059937\n",
      "-- train ok ! --\n",
      "fold 98: \n",
      "-- training --\n",
      "158.93597769737244\n",
      "-- train ok ! --\n",
      "fold 99: \n",
      "-- training --\n",
      "158.87020349502563\n",
      "-- train ok ! --\n",
      "fold 100: \n",
      "-- training --\n",
      "154.56748223304749\n",
      "-- train ok ! --\n",
      "0.8421249999999999\n",
      "fold 101: \n",
      "-- training --\n",
      "154.8400900363922\n",
      "-- train ok ! --\n",
      "fold 102: \n",
      "-- training --\n",
      "158.35421466827393\n",
      "-- train ok ! --\n",
      "fold 103: \n",
      "-- training --\n",
      "156.92781138420105\n",
      "-- train ok ! --\n",
      "fold 104: \n",
      "-- training --\n",
      "157.69998979568481\n",
      "-- train ok ! --\n",
      "fold 105: \n",
      "-- training --\n",
      "157.94365406036377\n",
      "-- train ok ! --\n",
      "0.8425416666666667\n",
      "fold 106: \n",
      "-- training --\n",
      "159.3161826133728\n",
      "-- train ok ! --\n",
      "fold 107: \n",
      "-- training --\n",
      "158.1678273677826\n",
      "-- train ok ! --\n",
      "fold 108: \n",
      "-- training --\n",
      "158.37124943733215\n",
      "-- train ok ! --\n",
      "fold 109: \n",
      "-- training --\n",
      "157.06521654129028\n",
      "-- train ok ! --\n",
      "fold 110: \n",
      "-- training --\n",
      "155.9303638935089\n",
      "-- train ok ! --\n",
      "0.8409166666666665\n",
      "fold 111: \n",
      "-- training --\n",
      "152.65205812454224\n",
      "-- train ok ! --\n",
      "fold 112: \n",
      "-- training --\n",
      "155.7906391620636\n",
      "-- train ok ! --\n",
      "fold 113: \n",
      "-- training --\n",
      "159.941801071167\n",
      "-- train ok ! --\n",
      "fold 114: \n",
      "-- training --\n",
      "159.56085658073425\n",
      "-- train ok ! --\n",
      "fold 115: \n",
      "-- training --\n",
      "158.42236375808716\n",
      "-- train ok ! --\n",
      "0.8397916666666667\n",
      "fold 116: \n",
      "-- training --\n",
      "158.67751097679138\n",
      "-- train ok ! --\n",
      "fold 117: \n",
      "-- training --\n",
      "157.50489473342896\n",
      "-- train ok ! --\n",
      "fold 118: \n",
      "-- training --\n",
      "158.26924300193787\n",
      "-- train ok ! --\n",
      "fold 119: \n",
      "-- training --\n",
      "160.6804337501526\n",
      "-- train ok ! --\n",
      "fold 120: \n",
      "-- training --\n",
      "158.26082229614258\n",
      "-- train ok ! --\n",
      "0.8382916666666667\n",
      "fold 121: \n",
      "-- training --\n",
      "157.7254045009613\n",
      "-- train ok ! --\n",
      "fold 122: \n",
      "-- training --\n",
      "158.10847401618958\n",
      "-- train ok ! --\n",
      "fold 123: \n",
      "-- training --\n",
      "156.3069829940796\n",
      "-- train ok ! --\n",
      "fold 124: \n",
      "-- training --\n",
      "158.95272731781006\n",
      "-- train ok ! --\n",
      "fold 125: \n",
      "-- training --\n",
      "155.65649247169495\n",
      "-- train ok ! --\n",
      "0.8382083333333334\n",
      "fold 126: \n",
      "-- training --\n",
      "155.89186072349548\n",
      "-- train ok ! --\n",
      "fold 127: \n",
      "-- training --\n",
      "154.88776540756226\n",
      "-- train ok ! --\n",
      "fold 128: \n",
      "-- training --\n",
      "159.90664291381836\n",
      "-- train ok ! --\n",
      "fold 129: \n",
      "-- training --\n",
      "159.57389760017395\n",
      "-- train ok ! --\n",
      "fold 130: \n",
      "-- training --\n",
      "159.1711609363556\n",
      "-- train ok ! --\n",
      "0.8382083333333334\n",
      "fold 131: \n",
      "-- training --\n",
      "159.55642366409302\n",
      "-- train ok ! --\n",
      "fold 132: \n",
      "-- training --\n",
      "158.37149238586426\n",
      "-- train ok ! --\n",
      "fold 133: \n",
      "-- training --\n",
      "158.38133716583252\n",
      "-- train ok ! --\n",
      "fold 134: \n",
      "-- training --\n",
      "158.93272876739502\n",
      "-- train ok ! --\n",
      "fold 135: \n",
      "-- training --\n",
      "158.9434778690338\n",
      "-- train ok ! --\n",
      "0.8377916666666667\n",
      "fold 136: \n",
      "-- training --\n",
      "160.14170122146606\n",
      "-- train ok ! --\n",
      "fold 137: \n",
      "-- training --\n",
      "155.8809609413147\n",
      "-- train ok ! --\n",
      "fold 138: \n",
      "-- training --\n",
      "156.08753848075867\n",
      "-- train ok ! --\n",
      "fold 139: \n",
      "-- training --\n",
      "156.13502550125122\n",
      "-- train ok ! --\n",
      "fold 140: \n",
      "-- training --\n",
      "159.23101234436035\n",
      "-- train ok ! --\n",
      "0.8365833333333335\n",
      "fold 141: \n",
      "-- training --\n",
      "160.66690850257874\n",
      "-- train ok ! --\n",
      "fold 142: \n",
      "-- training --\n",
      "158.86100673675537\n",
      "-- train ok ! --\n",
      "fold 143: \n",
      "-- training --\n",
      "156.70548295974731\n",
      "-- train ok ! --\n",
      "fold 144: \n",
      "-- training --\n",
      "159.92360591888428\n",
      "-- train ok ! --\n",
      "fold 145: \n",
      "-- training --\n",
      "158.53295612335205\n",
      "-- train ok ! --\n",
      "0.8359166666666666\n",
      "fold 146: \n",
      "-- training --\n",
      "153.3652515411377\n",
      "-- train ok ! --\n",
      "fold 147: \n",
      "-- training --\n",
      "158.88531875610352\n",
      "-- train ok ! --\n",
      "fold 148: \n",
      "-- training --\n",
      "159.23822402954102\n",
      "-- train ok ! --\n",
      "fold 149: \n",
      "-- training --\n",
      "157.88478326797485\n",
      "-- train ok ! --\n",
      "fold 150: \n",
      "-- training --\n",
      "158.63978576660156\n",
      "-- train ok ! --\n",
      "0.8344166666666666\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 20):\n",
    "    performance_accuracy = []\n",
    "    pipe = Pipeline([('knn',KNeighborsClassifier(n_neighbors=k, p=1))])\n",
    "    for (train_index, test_index) in kfold.split(X_train_30k):\n",
    "        print('fold '+str(fold_count)+': ')\n",
    "        print('-- training --')\n",
    "        X_kfold_train_30k, y_kfold_train_30k = X_train_30k.iloc[train_index], y_train_30k.loc[train_index]\n",
    "        X_kfold_test_30k, y_kfold_test_30k = X_train_30k.iloc[test_index], y_train_30k.loc[test_index]\n",
    "        start = time.time()\n",
    "        pipe.fit(X_kfold_train_30k, y_kfold_train_30k)\n",
    "        y_kfold_pred_30k = pipe.predict(X_kfold_test_30k)\n",
    "        end = time.time()\n",
    "        train_time = end-start\n",
    "        print(train_time)\n",
    "        print('-- train ok ! --')\n",
    "        perf = accuracy_score(y_kfold_test_30k, y_kfold_pred_30k)\n",
    "        performance_accuracy.append(perf)\n",
    "\n",
    "        fold_count = fold_count + 1\n",
    "    validation_times.append((k,np.mean(train_time)))\n",
    "    accs_knn_manhattan.append((k,np.mean(performance_accuracy)))\n",
    "    print(np.mean(performance_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_manhattan_30k = accs_knn_manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knn_manhattan_30k = pd.DataFrame(accs_manhattan_30k, columns=['k', 'accuracy'])\n",
    "df_knn_manhattan_30k.to_csv('results/data_knn_30k.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig = sns.lineplot(data=df_knn_manhattan_30k, x='k', y='accuracy', marker='o')\n",
    "plt.gca().locator_params(nbins=20)\n",
    "fig.set(xlabel='k', ylabel='Accuracy',title='Accuracy en funcion de k');\n",
    "plt.savefig(\"graficos/knn_manhattan_acc_50_30k.svg\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = data.drop(['label'], axis = 1), data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = SEED)\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "\n",
    "y_train.index = X_train.index\n",
    "performance_accuracy = []\n",
    "\n",
    "fold_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>160</td>\n",
       "      <td>162</td>\n",
       "      <td>163</td>\n",
       "      <td>135</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0          2       0       0       0       0       0       0       0       0   \n",
       "1          9       0       0       0       0       0       0       0       0   \n",
       "2          6       0       0       0       0       0       0       0       5   \n",
       "3          0       0       0       0       1       2       0       0       0   \n",
       "4          3       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "59995      9       0       0       0       0       0       0       0       0   \n",
       "59996      1       0       0       0       0       0       0       0       0   \n",
       "59997      8       0       0       0       0       0       0       0       0   \n",
       "59998      8       0       0       0       0       0       0       0       0   \n",
       "59999      7       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0           0  ...         0         0         0         0         0   \n",
       "1           0  ...         0         0         0         0         0   \n",
       "2           0  ...         0         0         0        30        43   \n",
       "3           0  ...         3         0         0         0         0   \n",
       "4           0  ...         0         0         0         0         0   \n",
       "...       ...  ...       ...       ...       ...       ...       ...   \n",
       "59995       0  ...         0         0         0         0         0   \n",
       "59996       0  ...        73         0         0         0         0   \n",
       "59997       0  ...       160       162       163       135        94   \n",
       "59998       0  ...         0         0         0         0         0   \n",
       "59999       0  ...         0         0         0         0         0   \n",
       "\n",
       "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0             0         0         0         0         0  \n",
       "1             0         0         0         0         0  \n",
       "2             0         0         0         0         0  \n",
       "3             1         0         0         0         0  \n",
       "4             0         0         0         0         0  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "59995         0         0         0         0         0  \n",
       "59996         0         0         0         0         0  \n",
       "59997         0         0         0         0         0  \n",
       "59998         0         0         0         0         0  \n",
       "59999         0         0         0         0         0  \n",
       "\n",
       "[60000 rows x 785 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(12000, 784)\n",
      "(48000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>37</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>174</td>\n",
       "      <td>180</td>\n",
       "      <td>196</td>\n",
       "      <td>235</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>229</td>\n",
       "      <td>166</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>94</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>206</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48000 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
       "0           0       0       0       0       0       0       0       0       0   \n",
       "1           0       0       0       0       0       0       0       0       0   \n",
       "2           0       0       0       0       0       0       0       0       0   \n",
       "3           0       0       0       0       0       0       0       0       0   \n",
       "4           0       0       0       0       0       0       0       0       1   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "47995       0       0       0       0       0       3       7       0       0   \n",
       "47996       0       0       0       0       0       0       0       0       0   \n",
       "47997       0       0       0       0       0       0       0       0       0   \n",
       "47998       0       0       0       0       0       0       0       2       0   \n",
       "47999       0       0       0       0       0       0      92     206     118   \n",
       "\n",
       "       pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0            0  ...         0         0         0         0         0   \n",
       "1            0  ...         0         0         0         0         0   \n",
       "2            0  ...         0         0         0         0         0   \n",
       "3            0  ...         0         0         0         0         0   \n",
       "4            0  ...         0         0         1         0        11   \n",
       "...        ...  ...       ...       ...       ...       ...       ...   \n",
       "47995        0  ...       174       180       196       235         0   \n",
       "47996        0  ...         0         0         0         0         0   \n",
       "47997        0  ...         0         1         0         0         0   \n",
       "47998        0  ...        94        74         0         0         0   \n",
       "47999        0  ...        33        21         3         0        79   \n",
       "\n",
       "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0             0         0         0         0         0  \n",
       "1             0         0         0         0         0  \n",
       "2             0         0         0         0         0  \n",
       "3             0         0         0         0         0  \n",
       "4            37         6         0         0         0  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "47995         0       255       229       166         0  \n",
       "47996         0         0         0         0         0  \n",
       "47997         0         0         0         0         0  \n",
       "47998         0         0         0         0         0  \n",
       "47999         0         0         1         0         0  \n",
       "\n",
       "[48000 rows x 784 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 150)\n",
      "5.71228551864624\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "pca.fit(X_train)\n",
    "X_pca = pca.transform(X_train)\n",
    "pca.transform(X_train)\n",
    "end = time.time()\n",
    "print(X_pca.shape)\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PCA' object has no attribute 'explained_variance_ratio_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-201dd990d6e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# con .explained_variance_ratio_ vemos la fracción de información que aporta cada componente\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mevr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Graficamos la fracción de varianza que aporta cada componente\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# y la información acumulada\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PCA' object has no attribute 'explained_variance_ratio_'"
     ]
    }
   ],
   "source": [
    "# con .explained_variance_ratio_ vemos la fracción de información que aporta cada componente\n",
    "evr = pca.explained_variance_ratio_\n",
    "\n",
    "# Graficamos la fracción de varianza que aporta cada componente\n",
    "# y la información acumulada\n",
    "fig, ax = plt.subplots(1, 2, figsize = (12, 4))\n",
    "\n",
    "ax[0].plot(range(1, len(evr) + 1), evr, '-', linewidth = 2)\n",
    "ax[0].set_ylabel('Fracción de varianza explicada')\n",
    "ax[0].set_xlabel('Número de componentes principal')\n",
    "\n",
    "# Calculamos el acumulado con la función cumsum de numpy \n",
    "varianza_acumulada = np.cumsum(evr)\n",
    "\n",
    "ax[1].plot(range(1, len(evr) + 1), varianza_acumulada, '-', linewidth = 2)\n",
    "ax[1].set_ylabel('Fracción acumulada de varianza explicada')\n",
    "ax[1].set_xlabel('Cantidad de componentes principales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca.shape\n",
    "y = data['label']\n",
    "X_pca = pd.DataFrame(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1: \n",
      "-- training --\n",
      "2.914703130722046\n",
      "-- train ok ! --\n",
      "fold 2: \n",
      "-- training --\n",
      "2.900982141494751\n",
      "-- train ok ! --\n",
      "fold 3: \n",
      "-- training --\n",
      "2.89567232131958\n",
      "-- train ok ! --\n",
      "fold 4: \n",
      "-- training --\n",
      "2.899676561355591\n",
      "-- train ok ! --\n",
      "fold 5: \n",
      "-- training --\n",
      "2.901074171066284\n",
      "-- train ok ! --\n",
      "0.7354999999999999\n",
      "fold 6: \n",
      "-- training --\n",
      "7.300988435745239\n",
      "-- train ok ! --\n",
      "fold 7: \n",
      "-- training --\n",
      "7.409654140472412\n",
      "-- train ok ! --\n",
      "fold 8: \n",
      "-- training --\n",
      "7.322641372680664\n",
      "-- train ok ! --\n",
      "fold 9: \n",
      "-- training --\n",
      "7.421768665313721\n",
      "-- train ok ! --\n",
      "fold 10: \n",
      "-- training --\n",
      "7.350833177566528\n",
      "-- train ok ! --\n",
      "0.8443333333333334\n",
      "fold 11: \n",
      "-- training --\n",
      "13.899157047271729\n",
      "-- train ok ! --\n",
      "fold 12: \n",
      "-- training --\n",
      "14.246610641479492\n",
      "-- train ok ! --\n",
      "fold 13: \n",
      "-- training --\n",
      "14.11925745010376\n",
      "-- train ok ! --\n",
      "fold 14: \n",
      "-- training --\n",
      "14.144119262695312\n",
      "-- train ok ! --\n",
      "fold 15: \n",
      "-- training --\n",
      "13.99708080291748\n",
      "-- train ok ! --\n",
      "0.8516250000000001\n",
      "fold 16: \n",
      "-- training --\n",
      "45.56426286697388\n",
      "-- train ok ! --\n",
      "fold 17: \n",
      "-- training --\n",
      "43.74640965461731\n",
      "-- train ok ! --\n",
      "fold 18: \n",
      "-- training --\n",
      "42.82621765136719\n",
      "-- train ok ! --\n",
      "fold 19: \n",
      "-- training --\n",
      "42.38152527809143\n",
      "-- train ok ! --\n",
      "fold 20: \n",
      "-- training --\n",
      "43.11793231964111\n",
      "-- train ok ! --\n",
      "0.8600208333333335\n",
      "fold 21: \n",
      "-- training --\n",
      "95.04006290435791\n",
      "-- train ok ! --\n",
      "fold 22: \n",
      "-- training --\n",
      "93.04665017127991\n",
      "-- train ok ! --\n",
      "fold 23: \n",
      "-- training --\n",
      "85.0391480922699\n",
      "-- train ok ! --\n",
      "fold 24: \n",
      "-- training --\n",
      "89.22035074234009\n",
      "-- train ok ! --\n",
      "fold 25: \n",
      "-- training --\n",
      "94.80449986457825\n",
      "-- train ok ! --\n",
      "0.8599375\n",
      "fold 26: \n",
      "-- training --\n",
      "144.09603571891785\n",
      "-- train ok ! --\n",
      "fold 27: \n",
      "-- training --\n",
      "147.21662163734436\n",
      "-- train ok ! --\n",
      "fold 28: \n",
      "-- training --\n",
      "143.85483813285828\n",
      "-- train ok ! --\n",
      "fold 29: \n",
      "-- training --\n",
      "152.80062007904053\n",
      "-- train ok ! --\n",
      "fold 30: \n",
      "-- training --\n",
      "143.10782408714294\n",
      "-- train ok ! --\n",
      "0.8630000000000001\n",
      "fold 31: \n",
      "-- training --\n",
      "160.0985291004181\n",
      "-- train ok ! --\n",
      "fold 32: \n",
      "-- training --\n",
      "151.04340863227844\n",
      "-- train ok ! --\n",
      "fold 33: \n",
      "-- training --\n",
      "162.92306876182556\n",
      "-- train ok ! --\n",
      "fold 34: \n",
      "-- training --\n",
      "165.15461540222168\n",
      "-- train ok ! --\n",
      "fold 35: \n",
      "-- training --\n",
      "164.88271522521973\n",
      "-- train ok ! --\n",
      "0.8635000000000002\n",
      "fold 36: \n",
      "-- training --\n",
      "183.76700377464294\n",
      "-- train ok ! --\n",
      "fold 37: \n",
      "-- training --\n",
      "184.14177465438843\n",
      "-- train ok ! --\n",
      "fold 38: \n",
      "-- training --\n",
      "184.06615662574768\n",
      "-- train ok ! --\n",
      "fold 39: \n",
      "-- training --\n",
      "174.68994569778442\n",
      "-- train ok ! --\n",
      "fold 40: \n",
      "-- training --\n",
      "176.55681133270264\n",
      "-- train ok ! --\n",
      "0.8628125000000001\n",
      "fold 41: \n",
      "-- training --\n",
      "2.8793246746063232\n",
      "-- train ok ! --\n",
      "fold 42: \n",
      "-- training --\n",
      "2.9702486991882324\n",
      "-- train ok ! --\n",
      "fold 43: \n",
      "-- training --\n",
      "2.9659838676452637\n",
      "-- train ok ! --\n",
      "fold 44: \n",
      "-- training --\n",
      "2.9603543281555176\n",
      "-- train ok ! --\n",
      "fold 45: \n",
      "-- training --\n",
      "2.964526653289795\n",
      "-- train ok ! --\n",
      "0.7466041666666667\n",
      "fold 46: \n",
      "-- training --\n",
      "7.695366859436035\n",
      "-- train ok ! --\n",
      "fold 47: \n",
      "-- training --\n",
      "7.931829929351807\n",
      "-- train ok ! --\n",
      "fold 48: \n",
      "-- training --\n",
      "7.801464557647705\n",
      "-- train ok ! --\n",
      "fold 49: \n",
      "-- training --\n",
      "7.962771654129028\n",
      "-- train ok ! --\n",
      "fold 50: \n",
      "-- training --\n",
      "7.883835792541504\n",
      "-- train ok ! --\n",
      "0.8472083333333333\n",
      "fold 51: \n",
      "-- training --\n",
      "13.984567165374756\n",
      "-- train ok ! --\n",
      "fold 52: \n",
      "-- training --\n",
      "14.263097524642944\n",
      "-- train ok ! --\n",
      "fold 53: \n",
      "-- training --\n",
      "14.729830980300903\n",
      "-- train ok ! --\n",
      "fold 54: \n",
      "-- training --\n",
      "13.999807357788086\n",
      "-- train ok ! --\n",
      "fold 55: \n",
      "-- training --\n",
      "14.397613286972046\n",
      "-- train ok ! --\n",
      "0.8524166666666668\n",
      "fold 56: \n",
      "-- training --\n",
      "49.679505586624146\n",
      "-- train ok ! --\n",
      "fold 57: \n",
      "-- training --\n",
      "48.60596561431885\n",
      "-- train ok ! --\n",
      "fold 58: \n",
      "-- training --\n",
      "48.378260374069214\n",
      "-- train ok ! --\n",
      "fold 59: \n",
      "-- training --\n",
      "48.69487476348877\n",
      "-- train ok ! --\n",
      "fold 60: \n",
      "-- training --\n",
      "47.837302684783936\n",
      "-- train ok ! --\n",
      "0.85875\n",
      "fold 61: \n",
      "-- training --\n",
      "101.87583827972412\n",
      "-- train ok ! --\n",
      "fold 62: \n",
      "-- training --\n",
      "103.26025915145874\n",
      "-- train ok ! --\n",
      "fold 63: \n",
      "-- training --\n",
      "103.87105011940002\n",
      "-- train ok ! --\n",
      "fold 64: \n",
      "-- training --\n",
      "98.10494613647461\n",
      "-- train ok ! --\n",
      "fold 65: \n",
      "-- training --\n",
      "104.37132740020752\n",
      "-- train ok ! --\n",
      "0.8620208333333335\n",
      "fold 66: \n",
      "-- training --\n",
      "152.43425297737122\n",
      "-- train ok ! --\n",
      "fold 67: \n",
      "-- training --\n",
      "166.62205171585083\n",
      "-- train ok ! --\n",
      "fold 68: \n",
      "-- training --\n",
      "153.79636788368225\n",
      "-- train ok ! --\n",
      "fold 69: \n",
      "-- training --\n",
      "167.65062499046326\n",
      "-- train ok ! --\n",
      "fold 70: \n",
      "-- training --\n",
      "151.47221779823303\n",
      "-- train ok ! --\n",
      "0.8639166666666667\n",
      "fold 71: \n",
      "-- training --\n",
      "171.3366243839264\n",
      "-- train ok ! --\n",
      "fold 72: \n",
      "-- training --\n",
      "170.350182056427\n",
      "-- train ok ! --\n",
      "fold 73: \n",
      "-- training --\n",
      "171.26618766784668\n",
      "-- train ok ! --\n",
      "fold 74: \n",
      "-- training --\n",
      "167.03849411010742\n",
      "-- train ok ! --\n",
      "fold 75: \n",
      "-- training --\n",
      "161.5721230506897\n",
      "-- train ok ! --\n",
      "0.8614583333333334\n",
      "fold 76: \n",
      "-- training --\n",
      "180.9743115901947\n",
      "-- train ok ! --\n",
      "fold 77: \n",
      "-- training --\n",
      "188.21916222572327\n",
      "-- train ok ! --\n",
      "fold 78: \n",
      "-- training --\n",
      "184.94246888160706\n",
      "-- train ok ! --\n",
      "fold 79: \n",
      "-- training --\n",
      "186.3775758743286\n",
      "-- train ok ! --\n",
      "fold 80: \n",
      "-- training --\n",
      "187.36325550079346\n",
      "-- train ok ! --\n",
      "0.8615416666666667\n",
      "fold 81: \n",
      "-- training --\n",
      "2.9282689094543457\n",
      "-- train ok ! --\n",
      "fold 82: \n",
      "-- training --\n",
      "2.9017622470855713\n",
      "-- train ok ! --\n",
      "fold 83: \n",
      "-- training --\n",
      "2.90254545211792\n",
      "-- train ok ! --\n",
      "fold 84: \n",
      "-- training --\n",
      "2.900299072265625\n",
      "-- train ok ! --\n",
      "fold 85: \n",
      "-- training --\n",
      "2.8965671062469482\n",
      "-- train ok ! --\n",
      "0.7485416666666667\n",
      "fold 86: \n",
      "-- training --\n",
      "8.141732454299927\n",
      "-- train ok ! --\n",
      "fold 87: \n",
      "-- training --\n",
      "8.197570562362671\n",
      "-- train ok ! --\n",
      "fold 88: \n",
      "-- training --\n",
      "8.176753759384155\n",
      "-- train ok ! --\n",
      "fold 89: \n",
      "-- training --\n",
      "8.207878589630127\n",
      "-- train ok ! --\n",
      "fold 90: \n",
      "-- training --\n",
      "8.163418054580688\n",
      "-- train ok ! --\n",
      "0.844125\n",
      "fold 91: \n",
      "-- training --\n",
      "15.043604135513306\n",
      "-- train ok ! --\n",
      "fold 92: \n",
      "-- training --\n",
      "15.086381435394287\n",
      "-- train ok ! --\n",
      "fold 93: \n",
      "-- training --\n",
      "14.779241800308228\n",
      "-- train ok ! --\n",
      "fold 94: \n",
      "-- training --\n",
      "14.819440364837646\n",
      "-- train ok ! --\n",
      "fold 95: \n",
      "-- training --\n",
      "14.768277645111084\n",
      "-- train ok ! --\n",
      "0.8515\n",
      "fold 96: \n",
      "-- training --\n",
      "51.25551986694336\n",
      "-- train ok ! --\n",
      "fold 97: \n",
      "-- training --\n",
      "49.72482180595398\n",
      "-- train ok ! --\n",
      "fold 98: \n",
      "-- training --\n",
      "47.125770568847656\n",
      "-- train ok ! --\n",
      "fold 99: \n",
      "-- training --\n",
      "50.33027243614197\n",
      "-- train ok ! --\n",
      "fold 100: \n",
      "-- training --\n",
      "50.284064531326294\n",
      "-- train ok ! --\n",
      "0.8556874999999999\n",
      "fold 101: \n",
      "-- training --\n",
      "107.09443545341492\n",
      "-- train ok ! --\n",
      "fold 102: \n",
      "-- training --\n",
      "108.39777874946594\n",
      "-- train ok ! --\n",
      "fold 103: \n",
      "-- training --\n",
      "108.56705498695374\n",
      "-- train ok ! --\n",
      "fold 104: \n",
      "-- training --\n",
      "108.41550731658936\n",
      "-- train ok ! --\n",
      "fold 105: \n",
      "-- training --\n",
      "104.50058436393738\n",
      "-- train ok ! --\n",
      "0.8564583333333333\n",
      "fold 106: \n",
      "-- training --\n",
      "155.47865772247314\n",
      "-- train ok ! --\n",
      "fold 107: \n",
      "-- training --\n",
      "168.12036752700806\n",
      "-- train ok ! --\n",
      "fold 108: \n",
      "-- training --\n",
      "155.14455842971802\n",
      "-- train ok ! --\n",
      "fold 109: \n",
      "-- training --\n",
      "164.02532124519348\n",
      "-- train ok ! --\n",
      "fold 110: \n",
      "-- training --\n",
      "155.3297245502472\n",
      "-- train ok ! --\n",
      "0.8578541666666666\n",
      "fold 111: \n",
      "-- training --\n",
      "165.16434693336487\n",
      "-- train ok ! --\n",
      "fold 112: \n",
      "-- training --\n",
      "166.8408191204071\n",
      "-- train ok ! --\n",
      "fold 113: \n",
      "-- training --\n",
      "167.31711339950562\n",
      "-- train ok ! --\n",
      "fold 114: \n",
      "-- training --\n",
      "171.88001585006714\n",
      "-- train ok ! --\n",
      "fold 115: \n",
      "-- training --\n",
      "175.18935656547546\n",
      "-- train ok ! --\n",
      "0.8576041666666667\n",
      "fold 116: \n",
      "-- training --\n",
      "191.19501614570618\n",
      "-- train ok ! --\n",
      "fold 117: \n",
      "-- training --\n",
      "188.54054999351501\n",
      "-- train ok ! --\n",
      "fold 118: \n",
      "-- training --\n",
      "188.74647760391235\n",
      "-- train ok ! --\n",
      "fold 119: \n",
      "-- training --\n",
      "189.17202138900757\n",
      "-- train ok ! --\n",
      "fold 120: \n",
      "-- training --\n",
      "185.44593453407288\n",
      "-- train ok ! --\n",
      "0.8561875000000001\n",
      "fold 121: \n",
      "-- training --\n",
      "3.0330097675323486\n",
      "-- train ok ! --\n",
      "fold 122: \n",
      "-- training --\n",
      "3.127838611602783\n",
      "-- train ok ! --\n",
      "fold 123: \n",
      "-- training --\n",
      "3.1313843727111816\n",
      "-- train ok ! --\n",
      "fold 124: \n",
      "-- training --\n",
      "3.1277382373809814\n",
      "-- train ok ! --\n",
      "fold 125: \n",
      "-- training --\n",
      "3.1252033710479736\n",
      "-- train ok ! --\n",
      "0.7474791666666667\n",
      "fold 126: \n",
      "-- training --\n",
      "8.50352954864502\n",
      "-- train ok ! --\n",
      "fold 127: \n",
      "-- training --\n",
      "8.641996383666992\n",
      "-- train ok ! --\n",
      "fold 128: \n",
      "-- training --\n",
      "8.70399022102356\n",
      "-- train ok ! --\n",
      "fold 129: \n",
      "-- training --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.758979082107544\n",
      "-- train ok ! --\n",
      "fold 130: \n",
      "-- training --\n",
      "8.876681327819824\n",
      "-- train ok ! --\n",
      "0.8426041666666666\n",
      "fold 131: \n",
      "-- training --\n",
      "15.376794815063477\n",
      "-- train ok ! --\n",
      "fold 132: \n",
      "-- training --\n",
      "15.529240846633911\n",
      "-- train ok ! --\n",
      "fold 133: \n",
      "-- training --\n",
      "15.23164987564087\n",
      "-- train ok ! --\n",
      "fold 134: \n",
      "-- training --\n",
      "15.771849870681763\n",
      "-- train ok ! --\n",
      "fold 135: \n",
      "-- training --\n",
      "15.727764129638672\n",
      "-- train ok ! --\n",
      "0.8496875000000002\n",
      "fold 136: \n",
      "-- training --\n",
      "53.322253704071045\n",
      "-- train ok ! --\n",
      "fold 137: \n",
      "-- training --\n",
      "52.43113112449646\n",
      "-- train ok ! --\n",
      "fold 138: \n",
      "-- training --\n",
      "52.78441762924194\n",
      "-- train ok ! --\n",
      "fold 139: \n",
      "-- training --\n",
      "51.86120939254761\n",
      "-- train ok ! --\n",
      "fold 140: \n",
      "-- training --\n",
      "52.97868537902832\n",
      "-- train ok ! --\n",
      "0.8536041666666666\n",
      "fold 141: \n",
      "-- training --\n",
      "103.24718308448792\n",
      "-- train ok ! --\n",
      "fold 142: \n",
      "-- training --\n",
      "101.39643263816833\n",
      "-- train ok ! --\n",
      "fold 143: \n",
      "-- training --\n",
      "100.07583546638489\n",
      "-- train ok ! --\n",
      "fold 144: \n",
      "-- training --\n",
      "106.28418707847595\n",
      "-- train ok ! --\n",
      "fold 145: \n",
      "-- training --\n",
      "111.89839172363281\n",
      "-- train ok ! --\n",
      "0.8546875\n",
      "fold 146: \n",
      "-- training --\n",
      "158.2211630344391\n",
      "-- train ok ! --\n",
      "fold 147: \n",
      "-- training --\n",
      "162.8102571964264\n",
      "-- train ok ! --\n",
      "fold 148: \n",
      "-- training --\n",
      "158.11549186706543\n",
      "-- train ok ! --\n",
      "fold 149: \n",
      "-- training --\n",
      "172.91541504859924\n",
      "-- train ok ! --\n",
      "fold 150: \n",
      "-- training --\n",
      "157.9379711151123\n",
      "-- train ok ! --\n",
      "0.8560833333333333\n",
      "fold 151: \n",
      "-- training --\n",
      "180.255197763443\n",
      "-- train ok ! --\n",
      "fold 152: \n",
      "-- training --\n",
      "171.5011339187622\n",
      "-- train ok ! --\n",
      "fold 153: \n",
      "-- training --\n",
      "160.86254024505615\n",
      "-- train ok ! --\n",
      "fold 154: \n",
      "-- training --\n",
      "161.4968400001526\n",
      "-- train ok ! --\n",
      "fold 155: \n",
      "-- training --\n",
      "164.46412992477417\n",
      "-- train ok ! --\n",
      "0.8551875000000001\n",
      "fold 156: \n",
      "-- training --\n",
      "179.08646273612976\n",
      "-- train ok ! --\n",
      "fold 157: \n",
      "-- training --\n",
      "174.91246962547302\n",
      "-- train ok ! --\n",
      "fold 158: \n",
      "-- training --\n",
      "174.45092582702637\n",
      "-- train ok ! --\n",
      "fold 159: \n",
      "-- training --\n",
      "175.6763505935669\n",
      "-- train ok ! --\n",
      "fold 160: \n",
      "-- training --\n",
      "179.97011423110962\n",
      "-- train ok ! --\n",
      "0.8543749999999999\n",
      "fold 161: \n",
      "-- training --\n",
      "3.000312566757202\n",
      "-- train ok ! --\n",
      "fold 162: \n",
      "-- training --\n",
      "2.9932103157043457\n",
      "-- train ok ! --\n",
      "fold 163: \n",
      "-- training --\n",
      "2.991462469100952\n",
      "-- train ok ! --\n",
      "fold 164: \n",
      "-- training --\n",
      "3.017362594604492\n",
      "-- train ok ! --\n",
      "fold 165: \n",
      "-- training --\n",
      "3.0992705821990967\n",
      "-- train ok ! --\n",
      "0.74875\n",
      "fold 166: \n",
      "-- training --\n",
      "8.515192031860352\n",
      "-- train ok ! --\n",
      "fold 167: \n",
      "-- training --\n",
      "8.530951261520386\n",
      "-- train ok ! --\n",
      "fold 168: \n",
      "-- training --\n",
      "8.515778064727783\n",
      "-- train ok ! --\n",
      "fold 169: \n",
      "-- training --\n",
      "8.586174011230469\n",
      "-- train ok ! --\n",
      "fold 170: \n",
      "-- training --\n",
      "8.503003120422363\n",
      "-- train ok ! --\n",
      "0.8401458333333334\n",
      "fold 171: \n",
      "-- training --\n",
      "15.58514952659607\n",
      "-- train ok ! --\n",
      "fold 172: \n",
      "-- training --\n",
      "15.41060495376587\n",
      "-- train ok ! --\n",
      "fold 173: \n",
      "-- training --\n",
      "15.641285419464111\n",
      "-- train ok ! --\n",
      "fold 174: \n",
      "-- training --\n",
      "15.461413383483887\n",
      "-- train ok ! --\n",
      "fold 175: \n",
      "-- training --\n",
      "15.619016885757446\n",
      "-- train ok ! --\n",
      "0.8461458333333333\n",
      "fold 176: \n",
      "-- training --\n",
      "50.52794885635376\n",
      "-- train ok ! --\n",
      "fold 177: \n",
      "-- training --\n",
      "49.689149379730225\n",
      "-- train ok ! --\n",
      "fold 178: \n",
      "-- training --\n",
      "51.076892137527466\n",
      "-- train ok ! --\n",
      "fold 179: \n",
      "-- training --\n",
      "50.500561237335205\n",
      "-- train ok ! --\n",
      "fold 180: \n",
      "-- training --\n",
      "52.19193077087402\n",
      "-- train ok ! --\n",
      "0.8517291666666666\n",
      "fold 181: \n",
      "-- training --\n",
      "105.0278708934784\n",
      "-- train ok ! --\n",
      "fold 182: \n",
      "-- training --\n",
      "103.42248439788818\n",
      "-- train ok ! --\n",
      "fold 183: \n",
      "-- training --\n",
      "102.57291316986084\n",
      "-- train ok ! --\n",
      "fold 184: \n",
      "-- training --\n",
      "103.47236013412476\n",
      "-- train ok ! --\n",
      "fold 185: \n",
      "-- training --\n",
      "102.8790557384491\n",
      "-- train ok ! --\n",
      "0.8521041666666667\n",
      "fold 186: \n",
      "-- training --\n",
      "160.2197287082672\n",
      "-- train ok ! --\n",
      "fold 187: \n",
      "-- training --\n",
      "176.02145504951477\n",
      "-- train ok ! --\n",
      "fold 188: \n",
      "-- training --\n",
      "160.3251495361328\n",
      "-- train ok ! --\n",
      "fold 189: \n",
      "-- training --\n",
      "165.9919147491455\n",
      "-- train ok ! --\n",
      "fold 190: \n",
      "-- training --\n",
      "160.81953191757202\n",
      "-- train ok ! --\n",
      "0.8533125\n",
      "fold 191: \n",
      "-- training --\n",
      "163.46427249908447\n",
      "-- train ok ! --\n",
      "fold 192: \n",
      "-- training --\n",
      "174.56185173988342\n",
      "-- train ok ! --\n",
      "fold 193: \n",
      "-- training --\n",
      "171.58843731880188\n",
      "-- train ok ! --\n",
      "fold 194: \n",
      "-- training --\n",
      "168.40096402168274\n",
      "-- train ok ! --\n",
      "fold 195: \n",
      "-- training --\n",
      "178.69402766227722\n",
      "-- train ok ! --\n",
      "0.8521041666666667\n",
      "fold 196: \n",
      "-- training --\n",
      "175.12701201438904\n",
      "-- train ok ! --\n",
      "fold 197: \n",
      "-- training --\n",
      "176.31503748893738\n",
      "-- train ok ! --\n",
      "fold 198: \n",
      "-- training --\n",
      "175.18064761161804\n",
      "-- train ok ! --\n",
      "fold 199: \n",
      "-- training --\n",
      "177.9705832004547\n",
      "-- train ok ! --\n",
      "fold 200: \n",
      "-- training --\n",
      "181.04015970230103\n",
      "-- train ok ! --\n",
      "0.8502916666666668\n",
      "fold 201: \n",
      "-- training --\n",
      "3.968832015991211\n",
      "-- train ok ! --\n",
      "fold 202: \n",
      "-- training --\n",
      "4.054740905761719\n",
      "-- train ok ! --\n",
      "fold 203: \n",
      "-- training --\n",
      "4.059897184371948\n",
      "-- train ok ! --\n",
      "fold 204: \n",
      "-- training --\n",
      "4.066030740737915\n",
      "-- train ok ! --\n",
      "fold 205: \n",
      "-- training --\n",
      "4.057758331298828\n",
      "-- train ok ! --\n",
      "0.7491249999999999\n",
      "fold 206: \n",
      "-- training --\n",
      "8.979063272476196\n",
      "-- train ok ! --\n",
      "fold 207: \n",
      "-- training --\n",
      "9.212216138839722\n",
      "-- train ok ! --\n",
      "fold 208: \n",
      "-- training --\n",
      "9.553003787994385\n",
      "-- train ok ! --\n",
      "fold 209: \n",
      "-- training --\n",
      "9.307921409606934\n",
      "-- train ok ! --\n",
      "fold 210: \n",
      "-- training --\n",
      "9.250049352645874\n",
      "-- train ok ! --\n",
      "0.8389791666666667\n",
      "fold 211: \n",
      "-- training --\n",
      "16.497501134872437\n",
      "-- train ok ! --\n",
      "fold 212: \n",
      "-- training --\n",
      "16.51713228225708\n",
      "-- train ok ! --\n",
      "fold 213: \n",
      "-- training --\n",
      "16.729204416275024\n",
      "-- train ok ! --\n",
      "fold 214: \n",
      "-- training --\n",
      "16.5447838306427\n",
      "-- train ok ! --\n",
      "fold 215: \n",
      "-- training --\n",
      "16.678187131881714\n",
      "-- train ok ! --\n",
      "0.8451458333333333\n",
      "fold 216: \n",
      "-- training --\n",
      "56.00969219207764\n",
      "-- train ok ! --\n",
      "fold 217: \n",
      "-- training --\n",
      "56.20882844924927\n",
      "-- train ok ! --\n",
      "fold 218: \n",
      "-- training --\n",
      "52.307236433029175\n",
      "-- train ok ! --\n",
      "fold 219: \n",
      "-- training --\n",
      "52.127875328063965\n",
      "-- train ok ! --\n",
      "fold 220: \n",
      "-- training --\n",
      "52.018948793411255\n",
      "-- train ok ! --\n",
      "0.8503333333333334\n",
      "fold 221: \n",
      "-- training --\n",
      "111.40393471717834\n",
      "-- train ok ! --\n",
      "fold 222: \n",
      "-- training --\n",
      "116.72600293159485\n",
      "-- train ok ! --\n",
      "fold 223: \n",
      "-- training --\n",
      "115.55959463119507\n",
      "-- train ok ! --\n",
      "fold 224: \n",
      "-- training --\n",
      "105.34933257102966\n",
      "-- train ok ! --\n",
      "fold 225: \n",
      "-- training --\n",
      "104.84991335868835\n",
      "-- train ok ! --\n",
      "0.8495833333333334\n",
      "fold 226: \n",
      "-- training --\n",
      "161.76169776916504\n",
      "-- train ok ! --\n",
      "fold 227: \n",
      "-- training --\n",
      "160.54917740821838\n",
      "-- train ok ! --\n",
      "fold 228: \n",
      "-- training --\n",
      "163.46231627464294\n",
      "-- train ok ! --\n",
      "fold 229: \n",
      "-- training --\n",
      "159.80388879776\n",
      "-- train ok ! --\n",
      "fold 230: \n",
      "-- training --\n",
      "162.11636590957642\n",
      "-- train ok ! --\n",
      "0.851125\n",
      "fold 231: \n",
      "-- training --\n",
      "162.67868542671204\n",
      "-- train ok ! --\n",
      "fold 232: \n",
      "-- training --\n",
      "164.12872099876404\n",
      "-- train ok ! --\n",
      "fold 233: \n",
      "-- training --\n",
      "164.75047707557678\n",
      "-- train ok ! --\n",
      "fold 234: \n",
      "-- training --\n",
      "169.46036505699158\n",
      "-- train ok ! --\n",
      "fold 235: \n",
      "-- training --\n",
      "176.60836911201477\n",
      "-- train ok ! --\n",
      "0.8495416666666668\n",
      "fold 236: \n",
      "-- training --\n",
      "175.60770177841187\n",
      "-- train ok ! --\n",
      "fold 237: \n",
      "-- training --\n",
      "176.77108669281006\n",
      "-- train ok ! --\n",
      "fold 238: \n",
      "-- training --\n",
      "176.12833714485168\n",
      "-- train ok ! --\n",
      "fold 239: \n",
      "-- training --\n",
      "183.53217148780823\n",
      "-- train ok ! --\n",
      "fold 240: \n",
      "-- training --\n",
      "182.05969786643982\n",
      "-- train ok ! --\n",
      "0.8478541666666667\n"
     ]
    }
   ],
   "source": [
    "accs_knn_manhattan_pca = []\n",
    "validation_times_manhattan_pca = []\n",
    "for k in range(5, 31, 5):\n",
    "    for n in [5, 20, 30, 50, 70, 100, 130, 150]:\n",
    "        start_transform = time.time()\n",
    "        pca = PCA(n_components=n)\n",
    "        pca.fit(X_train)\n",
    "        X_train_pca = pca.transform(X_train)\n",
    "        pca.transform(X_train)\n",
    "        end_transform = time.time()\n",
    "        X_train_pca = pd.DataFrame(X_train_pca)\n",
    "        performance_accuracy = []\n",
    "        pipe = Pipeline([('knn',KNeighborsClassifier(n_neighbors=k, p=1))])\n",
    "        for (train_index, test_index) in kfold.split(X_train_pca):\n",
    "            print('fold '+str(fold_count)+': ')\n",
    "            print('-- training --')\n",
    "            X_kfold_train_pca, y_kfold_train = X_train_pca.iloc[train_index], y_train.loc[train_index]\n",
    "            X_kfold_test_pca, y_kfold_test = X_train_pca.iloc[test_index], y_train.loc[test_index]\n",
    "            start = time.time()\n",
    "            pipe.fit(X_kfold_train_pca, y_kfold_train)\n",
    "            y_kfold_pred = pipe.predict(X_kfold_test_pca)\n",
    "            end = time.time()\n",
    "            train_time = end-start + end_transform - start_transform\n",
    "            print(train_time)\n",
    "            print('-- train ok ! --')\n",
    "            perf = accuracy_score(y_kfold_test, y_kfold_pred)\n",
    "            performance_accuracy.append(perf)\n",
    "\n",
    "            fold_count = fold_count + 1\n",
    "        validation_times_manhattan_pca.append((k,n,np.mean(train_time)))\n",
    "        accs_knn_manhattan_pca.append((k,n,np.mean(performance_accuracy)))\n",
    "        print(np.mean(performance_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 5, 2.901074171066284),\n",
       " (5, 20, 7.350833177566528),\n",
       " (5, 30, 13.99708080291748),\n",
       " (5, 50, 43.11793231964111),\n",
       " (5, 70, 94.80449986457825),\n",
       " (5, 100, 143.10782408714294),\n",
       " (5, 130, 164.88271522521973),\n",
       " (5, 150, 176.55681133270264),\n",
       " (10, 5, 2.964526653289795),\n",
       " (10, 20, 7.883835792541504),\n",
       " (10, 30, 14.397613286972046),\n",
       " (10, 50, 47.837302684783936),\n",
       " (10, 70, 104.37132740020752),\n",
       " (10, 100, 151.47221779823303),\n",
       " (10, 130, 161.5721230506897),\n",
       " (10, 150, 187.36325550079346),\n",
       " (15, 5, 2.8965671062469482),\n",
       " (15, 20, 8.163418054580688),\n",
       " (15, 30, 14.768277645111084),\n",
       " (15, 50, 50.284064531326294),\n",
       " (15, 70, 104.50058436393738),\n",
       " (15, 100, 155.3297245502472),\n",
       " (15, 130, 175.18935656547546),\n",
       " (15, 150, 185.44593453407288),\n",
       " (20, 5, 3.1252033710479736),\n",
       " (20, 20, 8.876681327819824),\n",
       " (20, 30, 15.727764129638672),\n",
       " (20, 50, 52.97868537902832),\n",
       " (20, 70, 111.89839172363281),\n",
       " (20, 100, 157.9379711151123),\n",
       " (20, 130, 164.46412992477417),\n",
       " (20, 150, 179.97011423110962),\n",
       " (25, 5, 3.0992705821990967),\n",
       " (25, 20, 8.503003120422363),\n",
       " (25, 30, 15.619016885757446),\n",
       " (25, 50, 52.19193077087402),\n",
       " (25, 70, 102.8790557384491),\n",
       " (25, 100, 160.81953191757202),\n",
       " (25, 130, 178.69402766227722),\n",
       " (25, 150, 181.04015970230103),\n",
       " (30, 5, 4.057758331298828),\n",
       " (30, 20, 9.250049352645874),\n",
       " (30, 30, 16.678187131881714),\n",
       " (30, 50, 52.018948793411255),\n",
       " (30, 70, 104.84991335868835),\n",
       " (30, 100, 162.11636590957642),\n",
       " (30, 130, 176.60836911201477),\n",
       " (30, 150, 182.05969786643982)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs_knn_manhattan_pca\n",
    "validation_times_manhattan_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knn_manhattan_pca = pd.DataFrame(accs_knn_manhattan_pca, columns=['k','componentes principales','accuracy'])\n",
    "df_knn_manhattan_pca.to_csv('results/data_knn_manhattan_pca.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 241: \n",
      "-- training --\n",
      "2.8472893238067627\n",
      "-- train ok ! --\n",
      "fold 242: \n",
      "-- training --\n",
      "2.8702874183654785\n",
      "-- train ok ! --\n",
      "fold 243: \n",
      "-- training --\n",
      "2.8694770336151123\n",
      "-- train ok ! --\n",
      "fold 244: \n",
      "-- training --\n",
      "2.843780755996704\n",
      "-- train ok ! --\n",
      "fold 245: \n",
      "-- training --\n",
      "2.803779363632202\n",
      "-- train ok ! --\n",
      "0.736625\n",
      "fold 246: \n",
      "-- training --\n",
      "5.17755651473999\n",
      "-- train ok ! --\n",
      "fold 247: \n",
      "-- training --\n",
      "5.2147064208984375\n",
      "-- train ok ! --\n",
      "fold 248: \n",
      "-- training --\n",
      "5.022188425064087\n",
      "-- train ok ! --\n",
      "fold 249: \n",
      "-- training --\n",
      "4.953359603881836\n",
      "-- train ok ! --\n",
      "fold 250: \n",
      "-- training --\n",
      "4.942146062850952\n",
      "-- train ok ! --\n",
      "0.843\n",
      "fold 251: \n",
      "-- training --\n",
      "6.412498950958252\n",
      "-- train ok ! --\n",
      "fold 252: \n",
      "-- training --\n",
      "6.45081639289856\n",
      "-- train ok ! --\n",
      "fold 253: \n",
      "-- training --\n",
      "6.296723127365112\n",
      "-- train ok ! --\n",
      "fold 254: \n",
      "-- training --\n",
      "6.233080148696899\n",
      "-- train ok ! --\n",
      "fold 255: \n",
      "-- training --\n",
      "6.249108552932739\n",
      "-- train ok ! --\n",
      "0.8509583333333334\n",
      "fold 256: \n",
      "-- training --\n",
      "11.10752534866333\n",
      "-- train ok ! --\n",
      "fold 257: \n",
      "-- training --\n",
      "11.049863338470459\n",
      "-- train ok ! --\n",
      "fold 258: \n",
      "-- training --\n",
      "11.167001724243164\n",
      "-- train ok ! --\n",
      "fold 259: \n",
      "-- training --\n",
      "11.449420690536499\n",
      "-- train ok ! --\n",
      "fold 260: \n",
      "-- training --\n",
      "11.78178596496582\n",
      "-- train ok ! --\n",
      "0.8560208333333333\n",
      "fold 261: \n",
      "-- training --\n",
      "19.090667724609375\n",
      "-- train ok ! --\n",
      "fold 262: \n",
      "-- training --\n",
      "18.48813796043396\n",
      "-- train ok ! --\n",
      "fold 263: \n",
      "-- training --\n",
      "18.374555826187134\n",
      "-- train ok ! --\n",
      "fold 264: \n",
      "-- training --\n",
      "18.626111030578613\n",
      "-- train ok ! --\n",
      "fold 265: \n",
      "-- training --\n",
      "18.35278630256653\n",
      "-- train ok ! --\n",
      "0.8584166666666666\n",
      "fold 266: \n",
      "-- training --\n",
      "26.350308418273926\n",
      "-- train ok ! --\n",
      "fold 267: \n",
      "-- training --\n",
      "26.21703553199768\n",
      "-- train ok ! --\n",
      "fold 268: \n",
      "-- training --\n",
      "26.11929988861084\n",
      "-- train ok ! --\n",
      "fold 269: \n",
      "-- training --\n",
      "26.136635541915894\n",
      "-- train ok ! --\n",
      "fold 270: \n",
      "-- training --\n",
      "26.445220470428467\n",
      "-- train ok ! --\n",
      "0.8571458333333333\n",
      "fold 271: \n",
      "-- training --\n",
      "32.65401315689087\n",
      "-- train ok ! --\n",
      "fold 272: \n",
      "-- training --\n",
      "33.35900044441223\n",
      "-- train ok ! --\n",
      "fold 273: \n",
      "-- training --\n",
      "33.488123655319214\n",
      "-- train ok ! --\n",
      "fold 274: \n",
      "-- training --\n",
      "33.30632448196411\n",
      "-- train ok ! --\n",
      "fold 275: \n",
      "-- training --\n",
      "33.59924936294556\n",
      "-- train ok ! --\n",
      "0.8597916666666666\n",
      "fold 276: \n",
      "-- training --\n",
      "37.3650336265564\n",
      "-- train ok ! --\n",
      "fold 277: \n",
      "-- training --\n",
      "36.388522148132324\n",
      "-- train ok ! --\n",
      "fold 278: \n",
      "-- training --\n",
      "35.582865953445435\n",
      "-- train ok ! --\n",
      "fold 279: \n",
      "-- training --\n",
      "36.21484327316284\n",
      "-- train ok ! --\n",
      "fold 280: \n",
      "-- training --\n",
      "37.3067889213562\n",
      "-- train ok ! --\n",
      "0.8586874999999999\n",
      "fold 281: \n",
      "-- training --\n",
      "2.967818021774292\n",
      "-- train ok ! --\n",
      "fold 282: \n",
      "-- training --\n",
      "3.0447146892547607\n",
      "-- train ok ! --\n",
      "fold 283: \n",
      "-- training --\n",
      "3.0351240634918213\n",
      "-- train ok ! --\n",
      "fold 284: \n",
      "-- training --\n",
      "3.039600133895874\n",
      "-- train ok ! --\n",
      "fold 285: \n",
      "-- training --\n",
      "3.0380568504333496\n",
      "-- train ok ! --\n",
      "0.7458750000000001\n",
      "fold 286: \n",
      "-- training --\n",
      "5.15501594543457\n",
      "-- train ok ! --\n",
      "fold 287: \n",
      "-- training --\n",
      "5.378007650375366\n",
      "-- train ok ! --\n",
      "fold 288: \n",
      "-- training --\n",
      "5.132428884506226\n",
      "-- train ok ! --\n",
      "fold 289: \n",
      "-- training --\n",
      "5.216372966766357\n",
      "-- train ok ! --\n",
      "fold 290: \n",
      "-- training --\n",
      "5.403245687484741\n",
      "-- train ok ! --\n",
      "0.8448333333333334\n",
      "fold 291: \n",
      "-- training --\n",
      "7.279397010803223\n",
      "-- train ok ! --\n",
      "fold 292: \n",
      "-- training --\n",
      "7.524123191833496\n",
      "-- train ok ! --\n",
      "fold 293: \n",
      "-- training --\n",
      "7.470427513122559\n",
      "-- train ok ! --\n",
      "fold 294: \n",
      "-- training --\n",
      "7.463112831115723\n",
      "-- train ok ! --\n",
      "fold 295: \n",
      "-- training --\n",
      "7.532310485839844\n",
      "-- train ok ! --\n",
      "0.8509791666666668\n",
      "fold 296: \n",
      "-- training --\n",
      "12.759688377380371\n",
      "-- train ok ! --\n",
      "fold 297: \n",
      "-- training --\n",
      "12.36475944519043\n",
      "-- train ok ! --\n",
      "fold 298: \n",
      "-- training --\n",
      "12.350700378417969\n",
      "-- train ok ! --\n",
      "fold 299: \n",
      "-- training --\n",
      "12.271862506866455\n",
      "-- train ok ! --\n",
      "fold 300: \n",
      "-- training --\n",
      "12.225313425064087\n",
      "-- train ok ! --\n",
      "0.8554583333333333\n",
      "fold 301: \n",
      "-- training --\n",
      "19.814998388290405\n",
      "-- train ok ! --\n",
      "fold 302: \n",
      "-- training --\n",
      "19.65703058242798\n",
      "-- train ok ! --\n",
      "fold 303: \n",
      "-- training --\n",
      "19.471965074539185\n",
      "-- train ok ! --\n",
      "fold 304: \n",
      "-- training --\n",
      "19.709816455841064\n",
      "-- train ok ! --\n",
      "fold 305: \n",
      "-- training --\n",
      "19.923282623291016\n",
      "-- train ok ! --\n",
      "0.8558749999999999\n",
      "fold 306: \n",
      "-- training --\n",
      "29.675182104110718\n",
      "-- train ok ! --\n",
      "fold 307: \n",
      "-- training --\n",
      "29.502767086029053\n",
      "-- train ok ! --\n",
      "fold 308: \n",
      "-- training --\n",
      "29.39933705329895\n",
      "-- train ok ! --\n",
      "fold 309: \n",
      "-- training --\n",
      "30.7922625541687\n",
      "-- train ok ! --\n",
      "fold 310: \n",
      "-- training --\n",
      "30.166527271270752\n",
      "-- train ok ! --\n",
      "0.8589791666666666\n",
      "fold 311: \n",
      "-- training --\n",
      "35.78077960014343\n",
      "-- train ok ! --\n",
      "fold 312: \n",
      "-- training --\n",
      "36.32186579704285\n",
      "-- train ok ! --\n",
      "fold 313: \n",
      "-- training --\n",
      "36.15617871284485\n",
      "-- train ok ! --\n",
      "fold 314: \n",
      "-- training --\n",
      "35.79672360420227\n",
      "-- train ok ! --\n",
      "fold 315: \n",
      "-- training --\n",
      "36.879148960113525\n",
      "-- train ok ! --\n",
      "0.858375\n",
      "fold 316: \n",
      "-- training --\n",
      "43.706937074661255\n",
      "-- train ok ! --\n",
      "fold 317: \n",
      "-- training --\n",
      "43.60062074661255\n",
      "-- train ok ! --\n",
      "fold 318: \n",
      "-- training --\n",
      "43.72156071662903\n",
      "-- train ok ! --\n",
      "fold 319: \n",
      "-- training --\n",
      "43.56240963935852\n",
      "-- train ok ! --\n",
      "fold 320: \n",
      "-- training --\n",
      "43.21544528007507\n",
      "-- train ok ! --\n",
      "0.8594583333333332\n",
      "fold 321: \n",
      "-- training --\n",
      "3.445634126663208\n",
      "-- train ok ! --\n",
      "fold 322: \n",
      "-- training --\n",
      "3.446603298187256\n",
      "-- train ok ! --\n",
      "fold 323: \n",
      "-- training --\n",
      "3.443432569503784\n",
      "-- train ok ! --\n",
      "fold 324: \n",
      "-- training --\n",
      "3.4429516792297363\n",
      "-- train ok ! --\n",
      "fold 325: \n",
      "-- training --\n",
      "3.4506168365478516\n",
      "-- train ok ! --\n",
      "0.7492916666666666\n",
      "fold 326: \n",
      "-- training --\n",
      "5.602769374847412\n",
      "-- train ok ! --\n",
      "fold 327: \n",
      "-- training --\n",
      "5.634534597396851\n",
      "-- train ok ! --\n",
      "fold 328: \n",
      "-- training --\n",
      "5.503480672836304\n",
      "-- train ok ! --\n",
      "fold 329: \n",
      "-- training --\n",
      "5.769989967346191\n",
      "-- train ok ! --\n",
      "fold 330: \n",
      "-- training --\n",
      "5.694737911224365\n",
      "-- train ok ! --\n",
      "0.8412916666666665\n",
      "fold 331: \n",
      "-- training --\n",
      "7.508225917816162\n",
      "-- train ok ! --\n",
      "fold 332: \n",
      "-- training --\n",
      "7.290662050247192\n",
      "-- train ok ! --\n",
      "fold 333: \n",
      "-- training --\n",
      "7.600423097610474\n",
      "-- train ok ! --\n",
      "fold 334: \n",
      "-- training --\n",
      "7.2081005573272705\n",
      "-- train ok ! --\n",
      "fold 335: \n",
      "-- training --\n",
      "7.340939283370972\n",
      "-- train ok ! --\n",
      "0.8488541666666667\n",
      "fold 336: \n",
      "-- training --\n",
      "14.045744895935059\n",
      "-- train ok ! --\n",
      "fold 337: \n",
      "-- training --\n",
      "14.031995296478271\n",
      "-- train ok ! --\n",
      "fold 338: \n",
      "-- training --\n",
      "13.785485982894897\n",
      "-- train ok ! --\n",
      "fold 339: \n",
      "-- training --\n",
      "13.794387578964233\n",
      "-- train ok ! --\n",
      "fold 340: \n",
      "-- training --\n",
      "13.626383066177368\n",
      "-- train ok ! --\n",
      "0.8528958333333334\n",
      "fold 341: \n",
      "-- training --\n",
      "21.280648708343506\n",
      "-- train ok ! --\n",
      "fold 342: \n",
      "-- training --\n",
      "22.10212254524231\n",
      "-- train ok ! --\n",
      "fold 343: \n",
      "-- training --\n",
      "21.40242314338684\n",
      "-- train ok ! --\n",
      "fold 344: \n",
      "-- training --\n",
      "21.181748390197754\n",
      "-- train ok ! --\n",
      "fold 345: \n",
      "-- training --\n",
      "21.43671178817749\n",
      "-- train ok ! --\n",
      "0.8543541666666666\n",
      "fold 346: \n",
      "-- training --\n",
      "31.35823392868042\n",
      "-- train ok ! --\n",
      "fold 347: \n",
      "-- training --\n",
      "30.82541275024414\n",
      "-- train ok ! --\n",
      "fold 348: \n",
      "-- training --\n",
      "31.522294521331787\n",
      "-- train ok ! --\n",
      "fold 349: \n",
      "-- training --\n",
      "33.26503372192383\n",
      "-- train ok ! --\n",
      "fold 350: \n",
      "-- training --\n",
      "31.613593101501465\n",
      "-- train ok ! --\n",
      "0.8554791666666667\n",
      "fold 351: \n",
      "-- training --\n",
      "38.6260302066803\n",
      "-- train ok ! --\n",
      "fold 352: \n",
      "-- training --\n",
      "37.666993141174316\n",
      "-- train ok ! --\n",
      "fold 353: \n",
      "-- training --\n",
      "38.346529483795166\n",
      "-- train ok ! --\n",
      "fold 354: \n",
      "-- training --\n",
      "37.585198640823364\n",
      "-- train ok ! --\n",
      "fold 355: \n",
      "-- training --\n",
      "38.95792007446289\n",
      "-- train ok ! --\n",
      "0.8543333333333333\n",
      "fold 356: \n",
      "-- training --\n",
      "40.430532932281494\n",
      "-- train ok ! --\n",
      "fold 357: \n",
      "-- training --\n",
      "41.467320919036865\n",
      "-- train ok ! --\n",
      "fold 358: \n",
      "-- training --\n",
      "43.591527700424194\n",
      "-- train ok ! --\n",
      "fold 359: \n",
      "-- training --\n",
      "43.35753917694092\n",
      "-- train ok ! --\n",
      "fold 360: \n",
      "-- training --\n",
      "43.05773305892944\n",
      "-- train ok ! --\n",
      "0.8567916666666667\n",
      "fold 361: \n",
      "-- training --\n",
      "3.073518753051758\n",
      "-- train ok ! --\n",
      "fold 362: \n",
      "-- training --\n",
      "3.1562118530273438\n",
      "-- train ok ! --\n",
      "fold 363: \n",
      "-- training --\n",
      "3.1556456089019775\n",
      "-- train ok ! --\n",
      "fold 364: \n",
      "-- training --\n",
      "3.1594433784484863\n",
      "-- train ok ! --\n",
      "fold 365: \n",
      "-- training --\n",
      "3.162985324859619\n",
      "-- train ok ! --\n",
      "0.7504375000000001\n",
      "fold 366: \n",
      "-- training --\n",
      "5.2900755405426025\n",
      "-- train ok ! --\n",
      "fold 367: \n",
      "-- training --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.260538816452026\n",
      "-- train ok ! --\n",
      "fold 368: \n",
      "-- training --\n",
      "5.224232912063599\n",
      "-- train ok ! --\n",
      "fold 369: \n",
      "-- training --\n",
      "5.182076692581177\n",
      "-- train ok ! --\n",
      "fold 370: \n",
      "-- training --\n",
      "5.1810243129730225\n",
      "-- train ok ! --\n",
      "0.8414791666666668\n",
      "fold 371: \n",
      "-- training --\n",
      "6.853325843811035\n",
      "-- train ok ! --\n",
      "fold 372: \n",
      "-- training --\n",
      "7.2072179317474365\n",
      "-- train ok ! --\n",
      "fold 373: \n",
      "-- training --\n",
      "6.954585790634155\n",
      "-- train ok ! --\n",
      "fold 374: \n",
      "-- training --\n",
      "6.948232412338257\n",
      "-- train ok ! --\n",
      "fold 375: \n",
      "-- training --\n",
      "7.01555061340332\n",
      "-- train ok ! --\n",
      "0.8476250000000001\n",
      "fold 376: \n",
      "-- training --\n",
      "13.870150089263916\n",
      "-- train ok ! --\n",
      "fold 377: \n",
      "-- training --\n",
      "13.419175148010254\n",
      "-- train ok ! --\n",
      "fold 378: \n",
      "-- training --\n",
      "13.170130252838135\n",
      "-- train ok ! --\n",
      "fold 379: \n",
      "-- training --\n",
      "13.111849546432495\n",
      "-- train ok ! --\n",
      "fold 380: \n",
      "-- training --\n",
      "13.038488864898682\n",
      "-- train ok ! --\n",
      "0.8515416666666666\n",
      "fold 381: \n",
      "-- training --\n",
      "21.5809326171875\n",
      "-- train ok ! --\n",
      "fold 382: \n",
      "-- training --\n",
      "21.36501693725586\n",
      "-- train ok ! --\n",
      "fold 383: \n",
      "-- training --\n",
      "21.511438131332397\n",
      "-- train ok ! --\n",
      "fold 384: \n",
      "-- training --\n",
      "22.334810733795166\n",
      "-- train ok ! --\n",
      "fold 385: \n",
      "-- training --\n",
      "21.201390266418457\n",
      "-- train ok ! --\n",
      "0.8525\n",
      "fold 386: \n",
      "-- training --\n",
      "32.049580335617065\n",
      "-- train ok ! --\n",
      "fold 387: \n",
      "-- training --\n",
      "33.402966022491455\n",
      "-- train ok ! --\n",
      "fold 388: \n",
      "-- training --\n",
      "31.99691128730774\n",
      "-- train ok ! --\n",
      "fold 389: \n",
      "-- training --\n",
      "34.37865948677063\n",
      "-- train ok ! --\n",
      "fold 390: \n",
      "-- training --\n",
      "32.30135703086853\n",
      "-- train ok ! --\n",
      "0.8517708333333334\n",
      "fold 391: \n",
      "-- training --\n",
      "37.006680488586426\n",
      "-- train ok ! --\n",
      "fold 392: \n",
      "-- training --\n",
      "36.85999870300293\n",
      "-- train ok ! --\n",
      "fold 393: \n",
      "-- training --\n",
      "39.32718825340271\n",
      "-- train ok ! --\n",
      "fold 394: \n",
      "-- training --\n",
      "39.400038719177246\n",
      "-- train ok ! --\n",
      "fold 395: \n",
      "-- training --\n",
      "39.66453790664673\n",
      "-- train ok ! --\n",
      "0.8531458333333333\n",
      "fold 396: \n",
      "-- training --\n",
      "41.002278089523315\n",
      "-- train ok ! --\n",
      "fold 397: \n",
      "-- training --\n",
      "43.34069609642029\n",
      "-- train ok ! --\n",
      "fold 398: \n",
      "-- training --\n",
      "41.78474187850952\n",
      "-- train ok ! --\n",
      "fold 399: \n",
      "-- training --\n",
      "41.72536873817444\n",
      "-- train ok ! --\n",
      "fold 400: \n",
      "-- training --\n",
      "41.6560423374176\n",
      "-- train ok ! --\n",
      "0.8518333333333334\n",
      "fold 401: \n",
      "-- training --\n",
      "2.8309457302093506\n",
      "-- train ok ! --\n",
      "fold 402: \n",
      "-- training --\n",
      "2.915703296661377\n",
      "-- train ok ! --\n",
      "fold 403: \n",
      "-- training --\n",
      "2.9150400161743164\n",
      "-- train ok ! --\n",
      "fold 404: \n",
      "-- training --\n",
      "2.9197213649749756\n",
      "-- train ok ! --\n",
      "fold 405: \n",
      "-- training --\n",
      "2.845349073410034\n",
      "-- train ok ! --\n",
      "0.7509791666666665\n",
      "fold 406: \n",
      "-- training --\n",
      "5.269315719604492\n",
      "-- train ok ! --\n",
      "fold 407: \n",
      "-- training --\n",
      "5.394859552383423\n",
      "-- train ok ! --\n",
      "fold 408: \n",
      "-- training --\n",
      "5.423910617828369\n",
      "-- train ok ! --\n",
      "fold 409: \n",
      "-- training --\n",
      "5.447903394699097\n",
      "-- train ok ! --\n",
      "fold 410: \n",
      "-- training --\n",
      "5.345784902572632\n",
      "-- train ok ! --\n",
      "0.8384375000000001\n",
      "fold 411: \n",
      "-- training --\n",
      "7.695829391479492\n",
      "-- train ok ! --\n",
      "fold 412: \n",
      "-- training --\n",
      "7.757014274597168\n",
      "-- train ok ! --\n",
      "fold 413: \n",
      "-- training --\n",
      "7.78005051612854\n",
      "-- train ok ! --\n",
      "fold 414: \n",
      "-- training --\n",
      "7.658889055252075\n",
      "-- train ok ! --\n",
      "fold 415: \n",
      "-- training --\n",
      "7.553767919540405\n",
      "-- train ok ! --\n",
      "0.8447083333333334\n",
      "fold 416: \n",
      "-- training --\n",
      "14.379526138305664\n",
      "-- train ok ! --\n",
      "fold 417: \n",
      "-- training --\n",
      "14.864812135696411\n",
      "-- train ok ! --\n",
      "fold 418: \n",
      "-- training --\n",
      "14.548636674880981\n",
      "-- train ok ! --\n",
      "fold 419: \n",
      "-- training --\n",
      "14.924524545669556\n",
      "-- train ok ! --\n",
      "fold 420: \n",
      "-- training --\n",
      "14.608902215957642\n",
      "-- train ok ! --\n",
      "0.8486458333333333\n",
      "fold 421: \n",
      "-- training --\n",
      "24.072990655899048\n",
      "-- train ok ! --\n",
      "fold 422: \n",
      "-- training --\n",
      "24.36797571182251\n",
      "-- train ok ! --\n",
      "fold 423: \n",
      "-- training --\n",
      "24.179136753082275\n",
      "-- train ok ! --\n",
      "fold 424: \n",
      "-- training --\n",
      "24.594857931137085\n",
      "-- train ok ! --\n",
      "fold 425: \n",
      "-- training --\n",
      "23.936311721801758\n",
      "-- train ok ! --\n",
      "0.8491250000000001\n",
      "fold 426: \n",
      "-- training --\n",
      "33.38710308074951\n",
      "-- train ok ! --\n",
      "fold 427: \n",
      "-- training --\n",
      "35.50111389160156\n",
      "-- train ok ! --\n",
      "fold 428: \n",
      "-- training --\n",
      "33.224005937576294\n",
      "-- train ok ! --\n",
      "fold 429: \n",
      "-- training --\n",
      "35.243874073028564\n",
      "-- train ok ! --\n",
      "fold 430: \n",
      "-- training --\n",
      "32.989418029785156\n",
      "-- train ok ! --\n",
      "0.8502291666666666\n",
      "fold 431: \n",
      "-- training --\n",
      "41.15315318107605\n",
      "-- train ok ! --\n",
      "fold 432: \n",
      "-- training --\n",
      "40.69821786880493\n",
      "-- train ok ! --\n",
      "fold 433: \n",
      "-- training --\n",
      "40.89438605308533\n",
      "-- train ok ! --\n",
      "fold 434: \n",
      "-- training --\n",
      "40.40451526641846\n",
      "-- train ok ! --\n",
      "fold 435: \n",
      "-- training --\n",
      "38.212302446365356\n",
      "-- train ok ! --\n",
      "0.8486041666666667\n",
      "fold 436: \n",
      "-- training --\n",
      "45.41697692871094\n",
      "-- train ok ! --\n",
      "fold 437: \n",
      "-- training --\n",
      "46.33884239196777\n",
      "-- train ok ! --\n",
      "fold 438: \n",
      "-- training --\n",
      "45.385722637176514\n",
      "-- train ok ! --\n",
      "fold 439: \n",
      "-- training --\n",
      "45.640493392944336\n",
      "-- train ok ! --\n",
      "fold 440: \n",
      "-- training --\n",
      "42.90109872817993\n",
      "-- train ok ! --\n",
      "0.8484166666666667\n",
      "fold 441: \n",
      "-- training --\n",
      "3.1740715503692627\n",
      "-- train ok ! --\n",
      "fold 442: \n",
      "-- training --\n",
      "3.238060235977173\n",
      "-- train ok ! --\n",
      "fold 443: \n",
      "-- training --\n",
      "3.2355260848999023\n",
      "-- train ok ! --\n",
      "fold 444: \n",
      "-- training --\n",
      "3.158228874206543\n",
      "-- train ok ! --\n",
      "fold 445: \n",
      "-- training --\n",
      "3.2354276180267334\n",
      "-- train ok ! --\n",
      "0.7496875000000001\n",
      "fold 446: \n",
      "-- training --\n",
      "5.483358383178711\n",
      "-- train ok ! --\n",
      "fold 447: \n",
      "-- training --\n",
      "5.689182996749878\n",
      "-- train ok ! --\n",
      "fold 448: \n",
      "-- training --\n",
      "5.482764005661011\n",
      "-- train ok ! --\n",
      "fold 449: \n",
      "-- training --\n",
      "5.45288348197937\n",
      "-- train ok ! --\n",
      "fold 450: \n",
      "-- training --\n",
      "5.4660234451293945\n",
      "-- train ok ! --\n",
      "0.8365833333333332\n",
      "fold 451: \n",
      "-- training --\n",
      "7.755756616592407\n",
      "-- train ok ! --\n",
      "fold 452: \n",
      "-- training --\n",
      "7.856012344360352\n",
      "-- train ok ! --\n",
      "fold 453: \n",
      "-- training --\n",
      "7.718936920166016\n",
      "-- train ok ! --\n",
      "fold 454: \n",
      "-- training --\n",
      "8.006288051605225\n",
      "-- train ok ! --\n",
      "fold 455: \n",
      "-- training --\n",
      "7.773843288421631\n",
      "-- train ok ! --\n",
      "0.8435416666666666\n",
      "fold 456: \n",
      "-- training --\n",
      "15.08386754989624\n",
      "-- train ok ! --\n",
      "fold 457: \n",
      "-- training --\n",
      "15.022394895553589\n",
      "-- train ok ! --\n",
      "fold 458: \n",
      "-- training --\n",
      "14.869517803192139\n",
      "-- train ok ! --\n",
      "fold 459: \n",
      "-- training --\n",
      "14.942163705825806\n",
      "-- train ok ! --\n",
      "fold 460: \n",
      "-- training --\n",
      "14.648578405380249\n",
      "-- train ok ! --\n",
      "0.8462916666666669\n",
      "fold 461: \n",
      "-- training --\n",
      "23.891627073287964\n",
      "-- train ok ! --\n",
      "fold 462: \n",
      "-- training --\n",
      "23.07041096687317\n",
      "-- train ok ! --\n",
      "fold 463: \n",
      "-- training --\n",
      "23.50881290435791\n",
      "-- train ok ! --\n",
      "fold 464: \n",
      "-- training --\n",
      "23.990924835205078\n",
      "-- train ok ! --\n",
      "fold 465: \n",
      "-- training --\n",
      "23.71826457977295\n",
      "-- train ok ! --\n",
      "0.8467500000000001\n",
      "fold 466: \n",
      "-- training --\n",
      "34.558857440948486\n",
      "-- train ok ! --\n",
      "fold 467: \n",
      "-- training --\n",
      "36.74731755256653\n",
      "-- train ok ! --\n",
      "fold 468: \n",
      "-- training --\n",
      "35.095329999923706\n",
      "-- train ok ! --\n",
      "fold 469: \n",
      "-- training --\n",
      "36.8754346370697\n",
      "-- train ok ! --\n",
      "fold 470: \n",
      "-- training --\n",
      "34.47063398361206\n",
      "-- train ok ! --\n",
      "0.8466875\n",
      "fold 471: \n",
      "-- training --\n",
      "38.17557692527771\n",
      "-- train ok ! --\n",
      "fold 472: \n",
      "-- training --\n",
      "38.408385276794434\n",
      "-- train ok ! --\n",
      "fold 473: \n",
      "-- training --\n",
      "38.44570541381836\n",
      "-- train ok ! --\n",
      "fold 474: \n",
      "-- training --\n",
      "41.32941460609436\n",
      "-- train ok ! --\n",
      "fold 475: \n",
      "-- training --\n",
      "41.32424879074097\n",
      "-- train ok ! --\n",
      "0.8469583333333333\n",
      "fold 476: \n",
      "-- training --\n",
      "45.54215598106384\n",
      "-- train ok ! --\n",
      "fold 477: \n",
      "-- training --\n",
      "43.700597286224365\n",
      "-- train ok ! --\n",
      "fold 478: \n",
      "-- training --\n",
      "43.591227293014526\n",
      "-- train ok ! --\n",
      "fold 479: \n",
      "-- training --\n",
      "43.56443905830383\n",
      "-- train ok ! --\n",
      "fold 480: \n",
      "-- training --\n",
      "43.96083116531372\n",
      "-- train ok ! --\n",
      "0.8466875\n"
     ]
    }
   ],
   "source": [
    "accs_knn_euclidean_pca = []\n",
    "validation_times_euclidean_pca = []\n",
    "for k in range(5, 31, 5):\n",
    "    for n in [5, 20, 30, 50, 70, 100, 130, 150]:\n",
    "        start_transform = time.time()\n",
    "        pca = PCA(n_components=n)\n",
    "        pca.fit(X_train)\n",
    "        X_train_pca = pca.transform(X_train)\n",
    "        pca.transform(X_train)\n",
    "        end_transform = time.time()\n",
    "        X_train_pca = pd.DataFrame(X_train_pca)\n",
    "        performance_accuracy = []\n",
    "        pipe = Pipeline([('knn',KNeighborsClassifier(n_neighbors=k, p=2))])\n",
    "        for (train_index, test_index) in kfold.split(X_train_pca):\n",
    "            print('fold '+str(fold_count)+': ')\n",
    "            print('-- training --')\n",
    "            X_kfold_train_pca, y_kfold_train = X_train_pca.iloc[train_index], y_train.loc[train_index]\n",
    "            X_kfold_test_pca, y_kfold_test = X_train_pca.iloc[test_index], y_train.loc[test_index]\n",
    "            start = time.time()\n",
    "            pipe.fit(X_kfold_train_pca, y_kfold_train)\n",
    "            y_kfold_pred = pipe.predict(X_kfold_test_pca)\n",
    "            end = time.time()\n",
    "            train_time = end-start + end_transform - start_transform\n",
    "            print(train_time)\n",
    "            print('-- train ok ! --')\n",
    "            perf = accuracy_score(y_kfold_test, y_kfold_pred)\n",
    "            performance_accuracy.append(perf)\n",
    "\n",
    "            fold_count = fold_count + 1\n",
    "        validation_times_euclidean_pca.append((k,n,np.mean(train_time)))\n",
    "        accs_knn_euclidean_pca.append((k,n,np.mean(performance_accuracy)))\n",
    "        print(np.mean(performance_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>componentes principales</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.736625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0.843000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.850958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>0.856021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>70</td>\n",
       "      <td>0.858417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>0.857146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>130</td>\n",
       "      <td>0.859792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>0.858687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.745875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.844833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>0.850979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>0.855458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>70</td>\n",
       "      <td>0.855875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.858979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>130</td>\n",
       "      <td>0.858375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>150</td>\n",
       "      <td>0.859458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>0.749292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>0.841292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "      <td>0.848854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15</td>\n",
       "      <td>50</td>\n",
       "      <td>0.852896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>15</td>\n",
       "      <td>70</td>\n",
       "      <td>0.854354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>0.855479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15</td>\n",
       "      <td>130</td>\n",
       "      <td>0.854333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15</td>\n",
       "      <td>150</td>\n",
       "      <td>0.856792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.750438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.841479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>0.847625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.851542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20</td>\n",
       "      <td>70</td>\n",
       "      <td>0.852500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>0.851771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>20</td>\n",
       "      <td>130</td>\n",
       "      <td>0.853146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>20</td>\n",
       "      <td>150</td>\n",
       "      <td>0.851833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>0.750979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>0.838438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "      <td>0.844708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>0.848646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>25</td>\n",
       "      <td>70</td>\n",
       "      <td>0.849125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>0.850229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>25</td>\n",
       "      <td>130</td>\n",
       "      <td>0.848604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>25</td>\n",
       "      <td>150</td>\n",
       "      <td>0.848417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>0.749688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>0.836583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0.843542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>0.846292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>30</td>\n",
       "      <td>70</td>\n",
       "      <td>0.846750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>30</td>\n",
       "      <td>100</td>\n",
       "      <td>0.846688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>30</td>\n",
       "      <td>130</td>\n",
       "      <td>0.846958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>30</td>\n",
       "      <td>150</td>\n",
       "      <td>0.846688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k  componentes principales  accuracy\n",
       "0    5                        5  0.736625\n",
       "1    5                       20  0.843000\n",
       "2    5                       30  0.850958\n",
       "3    5                       50  0.856021\n",
       "4    5                       70  0.858417\n",
       "5    5                      100  0.857146\n",
       "6    5                      130  0.859792\n",
       "7    5                      150  0.858687\n",
       "8   10                        5  0.745875\n",
       "9   10                       20  0.844833\n",
       "10  10                       30  0.850979\n",
       "11  10                       50  0.855458\n",
       "12  10                       70  0.855875\n",
       "13  10                      100  0.858979\n",
       "14  10                      130  0.858375\n",
       "15  10                      150  0.859458\n",
       "16  15                        5  0.749292\n",
       "17  15                       20  0.841292\n",
       "18  15                       30  0.848854\n",
       "19  15                       50  0.852896\n",
       "20  15                       70  0.854354\n",
       "21  15                      100  0.855479\n",
       "22  15                      130  0.854333\n",
       "23  15                      150  0.856792\n",
       "24  20                        5  0.750438\n",
       "25  20                       20  0.841479\n",
       "26  20                       30  0.847625\n",
       "27  20                       50  0.851542\n",
       "28  20                       70  0.852500\n",
       "29  20                      100  0.851771\n",
       "30  20                      130  0.853146\n",
       "31  20                      150  0.851833\n",
       "32  25                        5  0.750979\n",
       "33  25                       20  0.838438\n",
       "34  25                       30  0.844708\n",
       "35  25                       50  0.848646\n",
       "36  25                       70  0.849125\n",
       "37  25                      100  0.850229\n",
       "38  25                      130  0.848604\n",
       "39  25                      150  0.848417\n",
       "40  30                        5  0.749688\n",
       "41  30                       20  0.836583\n",
       "42  30                       30  0.843542\n",
       "43  30                       50  0.846292\n",
       "44  30                       70  0.846750\n",
       "45  30                      100  0.846688\n",
       "46  30                      130  0.846958\n",
       "47  30                      150  0.846688"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_knn_euclidean_pca = pd.DataFrame(accs_knn_euclidean_pca, columns=['k','componentes principales','accuracy'])\n",
    "df_knn_euclidean_pca.to_csv('results/data_knn_euclidean_pca.csv', index=False)\n",
    "df_knn_euclidean_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 481: \n",
      "-- training --\n",
      "3.070502758026123\n",
      "-- train ok ! --\n",
      "fold 482: \n",
      "-- training --\n",
      "3.1025028228759766\n",
      "-- train ok ! --\n",
      "fold 483: \n",
      "-- training --\n",
      "3.092284917831421\n",
      "-- train ok ! --\n",
      "fold 484: \n",
      "-- training --\n",
      "3.091824531555176\n",
      "-- train ok ! --\n",
      "fold 485: \n",
      "-- training --\n",
      "3.0905966758728027\n",
      "-- train ok ! --\n",
      "0.6946666666666668\n",
      "fold 486: \n",
      "-- training --\n",
      "6.15043568611145\n",
      "-- train ok ! --\n",
      "fold 487: \n",
      "-- training --\n",
      "6.437746047973633\n",
      "-- train ok ! --\n",
      "fold 488: \n",
      "-- training --\n",
      "6.175350189208984\n",
      "-- train ok ! --\n",
      "fold 489: \n",
      "-- training --\n",
      "6.2648093700408936\n",
      "-- train ok ! --\n",
      "fold 490: \n",
      "-- training --\n",
      "6.138769149780273\n",
      "-- train ok ! --\n",
      "0.8245000000000001\n",
      "fold 491: \n",
      "-- training --\n",
      "10.426049709320068\n",
      "-- train ok ! --\n",
      "fold 492: \n",
      "-- training --\n",
      "10.510246753692627\n",
      "-- train ok ! --\n",
      "fold 493: \n",
      "-- training --\n",
      "10.434494256973267\n",
      "-- train ok ! --\n",
      "fold 494: \n",
      "-- training --\n",
      "10.818878889083862\n",
      "-- train ok ! --\n",
      "fold 495: \n",
      "-- training --\n",
      "10.602410554885864\n",
      "-- train ok ! --\n",
      "0.8342916666666665\n",
      "fold 496: \n",
      "-- training --\n",
      "32.28813433647156\n",
      "-- train ok ! --\n",
      "fold 497: \n",
      "-- training --\n",
      "32.11109447479248\n",
      "-- train ok ! --\n",
      "fold 498: \n",
      "-- training --\n",
      "32.184096336364746\n",
      "-- train ok ! --\n",
      "fold 499: \n",
      "-- training --\n",
      "32.06669306755066\n",
      "-- train ok ! --\n",
      "fold 500: \n",
      "-- training --\n",
      "32.82722234725952\n",
      "-- train ok ! --\n",
      "0.8420624999999999\n",
      "fold 501: \n",
      "-- training --\n",
      "75.30235576629639\n",
      "-- train ok ! --\n",
      "fold 502: \n",
      "-- training --\n",
      "76.1406614780426\n",
      "-- train ok ! --\n",
      "fold 503: \n",
      "-- training --\n",
      "76.35691905021667\n",
      "-- train ok ! --\n",
      "fold 504: \n",
      "-- training --\n",
      "75.4712598323822\n",
      "-- train ok ! --\n",
      "fold 505: \n",
      "-- training --\n",
      "76.49991941452026\n",
      "-- train ok ! --\n",
      "0.846125\n",
      "fold 506: \n",
      "-- training --\n",
      "120.85928058624268\n",
      "-- train ok ! --\n",
      "fold 507: \n",
      "-- training --\n",
      "133.54992127418518\n",
      "-- train ok ! --\n",
      "fold 508: \n",
      "-- training --\n",
      "121.06160092353821\n",
      "-- train ok ! --\n",
      "fold 509: \n",
      "-- training --\n",
      "126.27195334434509\n",
      "-- train ok ! --\n",
      "fold 510: \n",
      "-- training --\n",
      "122.64803433418274\n",
      "-- train ok ! --\n",
      "0.8508125\n",
      "fold 511: \n",
      "-- training --\n",
      "153.18960547447205\n",
      "-- train ok ! --\n",
      "fold 512: \n",
      "-- training --\n",
      "149.08202290534973\n",
      "-- train ok ! --\n",
      "fold 513: \n",
      "-- training --\n",
      "151.55365562438965\n",
      "-- train ok ! --\n",
      "fold 514: \n",
      "-- training --\n",
      "148.35026597976685\n",
      "-- train ok ! --\n",
      "fold 515: \n",
      "-- training --\n",
      "152.639333486557\n",
      "-- train ok ! --\n",
      "0.8522083333333335\n",
      "fold 516: \n",
      "-- training --\n",
      "166.81338834762573\n",
      "-- train ok ! --\n",
      "fold 517: \n",
      "-- training --\n",
      "171.5614836215973\n",
      "-- train ok ! --\n",
      "fold 518: \n",
      "-- training --\n",
      "166.28463339805603\n",
      "-- train ok ! --\n",
      "fold 519: \n",
      "-- training --\n",
      "156.91943359375\n",
      "-- train ok ! --\n",
      "fold 520: \n",
      "-- training --\n",
      "162.9583284854889\n",
      "-- train ok ! --\n",
      "0.852\n",
      "fold 521: \n",
      "-- training --\n",
      "201.90913224220276\n",
      "-- train ok ! --\n",
      "fold 522: \n",
      "-- training --\n",
      "210.4296236038208\n",
      "-- train ok ! --\n",
      "fold 523: \n",
      "-- training --\n",
      "216.1013457775116\n",
      "-- train ok ! --\n",
      "fold 524: \n",
      "-- training --\n",
      "216.65260553359985\n",
      "-- train ok ! --\n",
      "fold 525: \n",
      "-- training --\n",
      "215.607488155365\n",
      "-- train ok ! --\n",
      "0.8521875000000001\n",
      "fold 526: \n",
      "-- training --\n",
      "2.7022576332092285\n",
      "-- train ok ! --\n",
      "fold 527: \n",
      "-- training --\n",
      "2.7037510871887207\n",
      "-- train ok ! --\n",
      "fold 528: \n",
      "-- training --\n",
      "2.7044341564178467\n",
      "-- train ok ! --\n",
      "fold 529: \n",
      "-- training --\n",
      "2.701059103012085\n",
      "-- train ok ! --\n",
      "fold 530: \n",
      "-- training --\n",
      "2.6956722736358643\n",
      "-- train ok ! --\n",
      "0.6913958333333332\n",
      "fold 531: \n",
      "-- training --\n",
      "6.428119421005249\n",
      "-- train ok ! --\n",
      "fold 532: \n",
      "-- training --\n",
      "6.372520685195923\n",
      "-- train ok ! --\n",
      "fold 533: \n",
      "-- training --\n",
      "6.364168405532837\n",
      "-- train ok ! --\n",
      "fold 534: \n",
      "-- training --\n",
      "6.44272518157959\n",
      "-- train ok ! --\n",
      "fold 535: \n",
      "-- training --\n",
      "6.38249135017395\n",
      "-- train ok ! --\n",
      "0.8204374999999999\n",
      "fold 536: \n",
      "-- training --\n",
      "11.27524447441101\n",
      "-- train ok ! --\n",
      "fold 537: \n",
      "-- training --\n",
      "11.26697587966919\n",
      "-- train ok ! --\n",
      "fold 538: \n",
      "-- training --\n",
      "11.294355630874634\n",
      "-- train ok ! --\n",
      "fold 539: \n",
      "-- training --\n",
      "11.25732421875\n",
      "-- train ok ! --\n",
      "fold 540: \n",
      "-- training --\n",
      "11.488087177276611\n",
      "-- train ok ! --\n",
      "0.8323958333333333\n",
      "fold 541: \n",
      "-- training --\n",
      "37.07589793205261\n",
      "-- train ok ! --\n",
      "fold 542: \n",
      "-- training --\n",
      "36.48147010803223\n",
      "-- train ok ! --\n",
      "fold 543: \n",
      "-- training --\n",
      "36.78286123275757\n",
      "-- train ok ! --\n",
      "fold 544: \n",
      "-- training --\n",
      "35.79353737831116\n",
      "-- train ok ! --\n",
      "fold 545: \n",
      "-- training --\n",
      "36.08779454231262\n",
      "-- train ok ! --\n",
      "0.8392916666666667\n",
      "fold 546: \n",
      "-- training --\n",
      "80.4975197315216\n",
      "-- train ok ! --\n",
      "fold 547: \n",
      "-- training --\n",
      "75.90331053733826\n",
      "-- train ok ! --\n",
      "fold 548: \n",
      "-- training --\n",
      "82.62567019462585\n",
      "-- train ok ! --\n",
      "fold 549: \n",
      "-- training --\n",
      "84.09214973449707\n",
      "-- train ok ! --\n",
      "fold 550: \n",
      "-- training --\n",
      "83.85391068458557\n",
      "-- train ok ! --\n",
      "0.8440000000000001\n",
      "fold 551: \n",
      "-- training --\n",
      "132.798077583313\n",
      "-- train ok ! --\n",
      "fold 552: \n",
      "-- training --\n",
      "144.99805450439453\n",
      "-- train ok ! --\n",
      "fold 553: \n",
      "-- training --\n",
      "131.97047448158264\n",
      "-- train ok ! --\n",
      "fold 554: \n",
      "-- training --\n",
      "138.5944802761078\n",
      "-- train ok ! --\n",
      "fold 555: \n",
      "-- training --\n",
      "132.31660199165344\n",
      "-- train ok ! --\n",
      "0.8495833333333334\n",
      "fold 556: \n",
      "-- training --\n",
      "145.94696879386902\n",
      "-- train ok ! --\n",
      "fold 557: \n",
      "-- training --\n",
      "145.08074498176575\n",
      "-- train ok ! --\n",
      "fold 558: \n",
      "-- training --\n",
      "149.28896713256836\n",
      "-- train ok ! --\n",
      "fold 559: \n",
      "-- training --\n",
      "158.31124782562256\n",
      "-- train ok ! --\n",
      "fold 560: \n",
      "-- training --\n",
      "149.39887022972107\n",
      "-- train ok ! --\n",
      "0.8502916666666668\n",
      "fold 561: \n",
      "-- training --\n",
      "162.10354781150818\n",
      "-- train ok ! --\n",
      "fold 562: \n",
      "-- training --\n",
      "163.1001877784729\n",
      "-- train ok ! --\n",
      "fold 563: \n",
      "-- training --\n",
      "161.95980381965637\n",
      "-- train ok ! --\n",
      "fold 564: \n",
      "-- training --\n",
      "163.0793697834015\n",
      "-- train ok ! --\n",
      "fold 565: \n",
      "-- training --\n",
      "161.95078301429749\n",
      "-- train ok ! --\n",
      "0.8496666666666666\n",
      "fold 566: \n",
      "-- training --\n",
      "213.89123630523682\n",
      "-- train ok ! --\n",
      "fold 567: \n",
      "-- training --\n",
      "222.2872211933136\n",
      "-- train ok ! --\n",
      "fold 568: \n",
      "-- training --\n",
      "221.219336271286\n",
      "-- train ok ! --\n",
      "fold 569: \n",
      "-- training --\n",
      "212.45389938354492\n",
      "-- train ok ! --\n",
      "fold 570: \n",
      "-- training --\n",
      "215.4507701396942\n",
      "-- train ok ! --\n",
      "0.8499166666666668\n",
      "fold 571: \n",
      "-- training --\n",
      "2.8091366291046143\n",
      "-- train ok ! --\n",
      "fold 572: \n",
      "-- training --\n",
      "2.8874711990356445\n",
      "-- train ok ! --\n",
      "fold 573: \n",
      "-- training --\n",
      "2.8805246353149414\n",
      "-- train ok ! --\n",
      "fold 574: \n",
      "-- training --\n",
      "2.881474256515503\n",
      "-- train ok ! --\n",
      "fold 575: \n",
      "-- training --\n",
      "2.8816404342651367\n",
      "-- train ok ! --\n",
      "0.7208958333333333\n",
      "fold 576: \n",
      "-- training --\n",
      "6.632701635360718\n",
      "-- train ok ! --\n",
      "fold 577: \n",
      "-- training --\n",
      "6.85538387298584\n",
      "-- train ok ! --\n",
      "fold 578: \n",
      "-- training --\n",
      "6.887585639953613\n",
      "-- train ok ! --\n",
      "fold 579: \n",
      "-- training --\n",
      "6.903929233551025\n",
      "-- train ok ! --\n",
      "fold 580: \n",
      "-- training --\n",
      "6.884250164031982\n",
      "-- train ok ! --\n",
      "0.8383749999999999\n",
      "fold 581: \n",
      "-- training --\n",
      "11.854361534118652\n",
      "-- train ok ! --\n",
      "fold 582: \n",
      "-- training --\n",
      "12.015635013580322\n",
      "-- train ok ! --\n",
      "fold 583: \n",
      "-- training --\n",
      "12.129704236984253\n",
      "-- train ok ! --\n",
      "fold 584: \n",
      "-- training --\n",
      "12.154233455657959\n",
      "-- train ok ! --\n",
      "fold 585: \n",
      "-- training --\n",
      "12.072359085083008\n",
      "-- train ok ! --\n",
      "0.8473749999999999\n",
      "fold 586: \n",
      "-- training --\n",
      "38.220736026763916\n",
      "-- train ok ! --\n",
      "fold 587: \n",
      "-- training --\n",
      "38.236403703689575\n",
      "-- train ok ! --\n",
      "fold 588: \n",
      "-- training --\n",
      "38.63207125663757\n",
      "-- train ok ! --\n",
      "fold 589: \n",
      "-- training --\n",
      "38.65328669548035\n",
      "-- train ok ! --\n",
      "fold 590: \n",
      "-- training --\n",
      "39.20639514923096\n",
      "-- train ok ! --\n",
      "0.8528333333333334\n",
      "fold 591: \n",
      "-- training --\n",
      "86.91668558120728\n",
      "-- train ok ! --\n",
      "fold 592: \n",
      "-- training --\n",
      "84.69672060012817\n",
      "-- train ok ! --\n",
      "fold 593: \n",
      "-- training --\n",
      "80.39794254302979\n",
      "-- train ok ! --\n",
      "fold 594: \n",
      "-- training --\n",
      "88.30744218826294\n",
      "-- train ok ! --\n",
      "fold 595: \n",
      "-- training --\n",
      "90.44503045082092\n",
      "-- train ok ! --\n",
      "0.8575208333333333\n",
      "fold 596: \n",
      "-- training --\n",
      "138.14447736740112\n",
      "-- train ok ! --\n",
      "fold 597: \n",
      "-- training --\n",
      "147.75864243507385\n",
      "-- train ok ! --\n",
      "fold 598: \n",
      "-- training --\n",
      "138.8785092830658\n",
      "-- train ok ! --\n",
      "fold 599: \n",
      "-- training --\n",
      "144.5416431427002\n",
      "-- train ok ! --\n",
      "fold 600: \n",
      "-- training --\n",
      "138.11175084114075\n",
      "-- train ok ! --\n",
      "0.8604791666666667\n",
      "fold 601: \n",
      "-- training --\n",
      "158.8264148235321\n",
      "-- train ok ! --\n",
      "fold 602: \n",
      "-- training --\n",
      "160.37931275367737\n",
      "-- train ok ! --\n",
      "fold 603: \n",
      "-- training --\n",
      "168.83456110954285\n",
      "-- train ok ! --\n",
      "fold 604: \n",
      "-- training --\n",
      "155.38946533203125\n",
      "-- train ok ! --\n",
      "fold 605: \n",
      "-- training --\n",
      "164.83404278755188\n",
      "-- train ok ! --\n",
      "0.8607708333333333\n",
      "fold 606: \n",
      "-- training --\n",
      "177.23867058753967\n",
      "-- train ok ! --\n",
      "fold 607: \n",
      "-- training --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178.65300703048706\n",
      "-- train ok ! --\n",
      "fold 608: \n",
      "-- training --\n",
      "177.891597032547\n",
      "-- train ok ! --\n",
      "fold 609: \n",
      "-- training --\n",
      "176.41160106658936\n",
      "-- train ok ! --\n",
      "fold 610: \n",
      "-- training --\n",
      "174.05706644058228\n",
      "-- train ok ! --\n",
      "0.861375\n",
      "fold 611: \n",
      "-- training --\n",
      "208.055114030838\n",
      "-- train ok ! --\n",
      "fold 612: \n",
      "-- training --\n",
      "208.90649151802063\n",
      "-- train ok ! --\n",
      "fold 613: \n",
      "-- training --\n",
      "208.6435067653656\n",
      "-- train ok ! --\n",
      "fold 614: \n",
      "-- training --\n",
      "219.80918049812317\n",
      "-- train ok ! --\n",
      "fold 615: \n",
      "-- training --\n",
      "219.07820105552673\n",
      "-- train ok ! --\n",
      "0.8587083333333334\n",
      "fold 616: \n",
      "-- training --\n",
      "2.843568801879883\n",
      "-- train ok ! --\n",
      "fold 617: \n",
      "-- training --\n",
      "2.923926830291748\n",
      "-- train ok ! --\n",
      "fold 618: \n",
      "-- training --\n",
      "2.923936605453491\n",
      "-- train ok ! --\n",
      "fold 619: \n",
      "-- training --\n",
      "2.930628776550293\n",
      "-- train ok ! --\n",
      "fold 620: \n",
      "-- training --\n",
      "2.9264423847198486\n",
      "-- train ok ! --\n",
      "0.7275208333333334\n",
      "fold 621: \n",
      "-- training --\n",
      "7.194141387939453\n",
      "-- train ok ! --\n",
      "fold 622: \n",
      "-- training --\n",
      "7.263725280761719\n",
      "-- train ok ! --\n",
      "fold 623: \n",
      "-- training --\n",
      "7.014299154281616\n",
      "-- train ok ! --\n",
      "fold 624: \n",
      "-- training --\n",
      "7.156823396682739\n",
      "-- train ok ! --\n",
      "fold 625: \n",
      "-- training --\n",
      "7.261556386947632\n",
      "-- train ok ! --\n",
      "0.8438125\n",
      "fold 626: \n",
      "-- training --\n",
      "12.338279008865356\n",
      "-- train ok ! --\n",
      "fold 627: \n",
      "-- training --\n",
      "12.478382349014282\n",
      "-- train ok ! --\n",
      "fold 628: \n",
      "-- training --\n",
      "12.252140998840332\n",
      "-- train ok ! --\n",
      "fold 629: \n",
      "-- training --\n",
      "12.157580137252808\n",
      "-- train ok ! --\n",
      "fold 630: \n",
      "-- training --\n",
      "12.1501305103302\n",
      "-- train ok ! --\n",
      "0.8509791666666666\n",
      "fold 631: \n",
      "-- training --\n",
      "38.394296646118164\n",
      "-- train ok ! --\n",
      "fold 632: \n",
      "-- training --\n",
      "37.78042817115784\n",
      "-- train ok ! --\n",
      "fold 633: \n",
      "-- training --\n",
      "39.39935922622681\n",
      "-- train ok ! --\n",
      "fold 634: \n",
      "-- training --\n",
      "37.75987792015076\n",
      "-- train ok ! --\n",
      "fold 635: \n",
      "-- training --\n",
      "38.02481698989868\n",
      "-- train ok ! --\n",
      "0.8574791666666666\n",
      "fold 636: \n",
      "-- training --\n",
      "90.11744451522827\n",
      "-- train ok ! --\n",
      "fold 637: \n",
      "-- training --\n",
      "86.18704771995544\n",
      "-- train ok ! --\n",
      "fold 638: \n",
      "-- training --\n",
      "82.46504092216492\n",
      "-- train ok ! --\n",
      "fold 639: \n",
      "-- training --\n",
      "85.51650285720825\n",
      "-- train ok ! --\n",
      "fold 640: \n",
      "-- training --\n",
      "92.78107070922852\n",
      "-- train ok ! --\n",
      "0.8607708333333333\n",
      "fold 641: \n",
      "-- training --\n",
      "141.87189173698425\n",
      "-- train ok ! --\n",
      "fold 642: \n",
      "-- training --\n",
      "154.45692944526672\n",
      "-- train ok ! --\n",
      "fold 643: \n",
      "-- training --\n",
      "140.01337599754333\n",
      "-- train ok ! --\n",
      "fold 644: \n",
      "-- training --\n",
      "155.21708250045776\n",
      "-- train ok ! --\n",
      "fold 645: \n",
      "-- training --\n",
      "140.74426412582397\n",
      "-- train ok ! --\n",
      "0.8624791666666667\n",
      "fold 646: \n",
      "-- training --\n",
      "154.6686613559723\n",
      "-- train ok ! --\n",
      "fold 647: \n",
      "-- training --\n",
      "149.20182991027832\n",
      "-- train ok ! --\n",
      "fold 648: \n",
      "-- training --\n",
      "159.01536226272583\n",
      "-- train ok ! --\n",
      "fold 649: \n",
      "-- training --\n",
      "163.23872876167297\n",
      "-- train ok ! --\n",
      "fold 650: \n",
      "-- training --\n",
      "169.02773642539978\n",
      "-- train ok ! --\n",
      "0.8641041666666667\n",
      "fold 651: \n",
      "-- training --\n",
      "182.22041177749634\n",
      "-- train ok ! --\n",
      "fold 652: \n",
      "-- training --\n",
      "184.64880847930908\n",
      "-- train ok ! --\n",
      "fold 653: \n",
      "-- training --\n",
      "180.8766748905182\n",
      "-- train ok ! --\n",
      "fold 654: \n",
      "-- training --\n",
      "182.27563428878784\n",
      "-- train ok ! --\n",
      "fold 655: \n",
      "-- training --\n",
      "176.2799973487854\n",
      "-- train ok ! --\n",
      "0.8630416666666667\n",
      "fold 656: \n",
      "-- training --\n",
      "210.8840811252594\n",
      "-- train ok ! --\n",
      "fold 657: \n",
      "-- training --\n",
      "213.62424659729004\n",
      "-- train ok ! --\n",
      "fold 658: \n",
      "-- training --\n",
      "209.64978885650635\n",
      "-- train ok ! --\n",
      "fold 659: \n",
      "-- training --\n",
      "209.10764169692993\n",
      "-- train ok ! --\n",
      "fold 660: \n",
      "-- training --\n",
      "210.86346197128296\n",
      "-- train ok ! --\n",
      "0.8623958333333335\n",
      "fold 661: \n",
      "-- training --\n",
      "3.487508773803711\n",
      "-- train ok ! --\n",
      "fold 662: \n",
      "-- training --\n",
      "3.458777666091919\n",
      "-- train ok ! --\n",
      "fold 663: \n",
      "-- training --\n",
      "3.4084994792938232\n",
      "-- train ok ! --\n",
      "fold 664: \n",
      "-- training --\n",
      "3.4048233032226562\n",
      "-- train ok ! --\n",
      "fold 665: \n",
      "-- training --\n",
      "3.4106805324554443\n",
      "-- train ok ! --\n",
      "0.7343333333333333\n",
      "fold 666: \n",
      "-- training --\n",
      "7.456399202346802\n",
      "-- train ok ! --\n",
      "fold 667: \n",
      "-- training --\n",
      "7.583506345748901\n",
      "-- train ok ! --\n",
      "fold 668: \n",
      "-- training --\n",
      "7.4835546016693115\n",
      "-- train ok ! --\n",
      "fold 669: \n",
      "-- training --\n",
      "7.658604621887207\n",
      "-- train ok ! --\n",
      "fold 670: \n",
      "-- training --\n",
      "7.405655860900879\n",
      "-- train ok ! --\n",
      "0.8443333333333334\n",
      "fold 671: \n",
      "-- training --\n",
      "13.038352489471436\n",
      "-- train ok ! --\n",
      "fold 672: \n",
      "-- training --\n",
      "13.001814126968384\n",
      "-- train ok ! --\n",
      "fold 673: \n",
      "-- training --\n",
      "13.900117635726929\n",
      "-- train ok ! --\n",
      "fold 674: \n",
      "-- training --\n",
      "14.163459062576294\n",
      "-- train ok ! --\n",
      "fold 675: \n",
      "-- training --\n",
      "13.267940044403076\n",
      "-- train ok ! --\n",
      "0.8506875\n",
      "fold 676: \n",
      "-- training --\n",
      "43.46427059173584\n",
      "-- train ok ! --\n",
      "fold 677: \n",
      "-- training --\n",
      "42.32050824165344\n",
      "-- train ok ! --\n",
      "fold 678: \n",
      "-- training --\n",
      "42.33909368515015\n",
      "-- train ok ! --\n",
      "fold 679: \n",
      "-- training --\n",
      "41.602790117263794\n",
      "-- train ok ! --\n",
      "fold 680: \n",
      "-- training --\n",
      "41.79446601867676\n",
      "-- train ok ! --\n",
      "0.8587083333333334\n",
      "fold 681: \n",
      "-- training --\n",
      "93.52520537376404\n",
      "-- train ok ! --\n",
      "fold 682: \n",
      "-- training --\n",
      "96.22352695465088\n",
      "-- train ok ! --\n",
      "fold 683: \n",
      "-- training --\n",
      "87.1040427684784\n",
      "-- train ok ! --\n",
      "fold 684: \n",
      "-- training --\n",
      "86.05277037620544\n",
      "-- train ok ! --\n",
      "fold 685: \n",
      "-- training --\n",
      "87.81128573417664\n",
      "-- train ok ! --\n",
      "0.860625\n",
      "fold 686: \n",
      "-- training --\n",
      "143.8755919933319\n",
      "-- train ok ! --\n",
      "fold 687: \n",
      "-- training --\n",
      "149.1818778514862\n",
      "-- train ok ! --\n",
      "fold 688: \n",
      "-- training --\n",
      "143.35001420974731\n",
      "-- train ok ! --\n",
      "fold 689: \n",
      "-- training --\n",
      "145.00126099586487\n",
      "-- train ok ! --\n",
      "fold 690: \n",
      "-- training --\n",
      "144.43832421302795\n",
      "-- train ok ! --\n",
      "0.8636874999999999\n",
      "fold 691: \n",
      "-- training --\n",
      "171.79858374595642\n",
      "-- train ok ! --\n",
      "fold 692: \n",
      "-- training --\n",
      "166.98371982574463\n",
      "-- train ok ! --\n",
      "fold 693: \n",
      "-- training --\n",
      "166.75921177864075\n",
      "-- train ok ! --\n",
      "fold 694: \n",
      "-- training --\n",
      "151.07832765579224\n",
      "-- train ok ! --\n",
      "fold 695: \n",
      "-- training --\n",
      "156.90851664543152\n",
      "-- train ok ! --\n",
      "0.8626458333333333\n",
      "fold 696: \n",
      "-- training --\n",
      "168.9833846092224\n",
      "-- train ok ! --\n",
      "fold 697: \n",
      "-- training --\n",
      "178.48047304153442\n",
      "-- train ok ! --\n",
      "fold 698: \n",
      "-- training --\n",
      "182.28062081336975\n",
      "-- train ok ! --\n",
      "fold 699: \n",
      "-- training --\n",
      "183.82093167304993\n",
      "-- train ok ! --\n",
      "fold 700: \n",
      "-- training --\n",
      "182.68740940093994\n",
      "-- train ok ! --\n",
      "0.8628333333333333\n",
      "fold 701: \n",
      "-- training --\n",
      "219.7086091041565\n",
      "-- train ok ! --\n",
      "fold 702: \n",
      "-- training --\n",
      "213.22255325317383\n",
      "-- train ok ! --\n",
      "fold 703: \n",
      "-- training --\n",
      "211.51660132408142\n",
      "-- train ok ! --\n",
      "fold 704: \n",
      "-- training --\n",
      "215.08680367469788\n",
      "-- train ok ! --\n",
      "fold 705: \n",
      "-- training --\n",
      "222.08164405822754\n",
      "-- train ok ! --\n",
      "0.8608541666666667\n",
      "fold 706: \n",
      "-- training --\n",
      "2.8464066982269287\n",
      "-- train ok ! --\n",
      "fold 707: \n",
      "-- training --\n",
      "2.9060802459716797\n",
      "-- train ok ! --\n",
      "fold 708: \n",
      "-- training --\n",
      "2.915302038192749\n",
      "-- train ok ! --\n",
      "fold 709: \n",
      "-- training --\n",
      "2.9124112129211426\n",
      "-- train ok ! --\n",
      "fold 710: \n",
      "-- training --\n",
      "2.9082579612731934\n",
      "-- train ok ! --\n",
      "0.7387291666666667\n",
      "fold 711: \n",
      "-- training --\n",
      "7.3478264808654785\n",
      "-- train ok ! --\n",
      "fold 712: \n",
      "-- training --\n",
      "7.516390323638916\n",
      "-- train ok ! --\n",
      "fold 713: \n",
      "-- training --\n",
      "7.547889709472656\n",
      "-- train ok ! --\n",
      "fold 714: \n",
      "-- training --\n",
      "7.5344672203063965\n",
      "-- train ok ! --\n",
      "fold 715: \n",
      "-- training --\n",
      "7.499345302581787\n",
      "-- train ok ! --\n",
      "0.8463333333333333\n",
      "fold 716: \n",
      "-- training --\n",
      "13.097634077072144\n",
      "-- train ok ! --\n",
      "fold 717: \n",
      "-- training --\n",
      "13.22443437576294\n",
      "-- train ok ! --\n",
      "fold 718: \n",
      "-- training --\n",
      "13.205027341842651\n",
      "-- train ok ! --\n",
      "fold 719: \n",
      "-- training --\n",
      "13.21388292312622\n",
      "-- train ok ! --\n",
      "fold 720: \n",
      "-- training --\n",
      "13.1273775100708\n",
      "-- train ok ! --\n",
      "0.8535208333333333\n",
      "fold 721: \n",
      "-- training --\n",
      "43.848350286483765\n",
      "-- train ok ! --\n",
      "fold 722: \n",
      "-- training --\n",
      "44.07086634635925\n",
      "-- train ok ! --\n",
      "fold 723: \n",
      "-- training --\n",
      "43.92306613922119\n",
      "-- train ok ! --\n",
      "fold 724: \n",
      "-- training --\n",
      "43.80176019668579\n",
      "-- train ok ! --\n",
      "fold 725: \n",
      "-- training --\n",
      "44.01698637008667\n",
      "-- train ok ! --\n",
      "0.8596458333333334\n",
      "fold 726: \n",
      "-- training --\n",
      "92.3600172996521\n",
      "-- train ok ! --\n",
      "fold 727: \n",
      "-- training --\n",
      "89.26713752746582\n",
      "-- train ok ! --\n",
      "fold 728: \n",
      "-- training --\n",
      "88.37076330184937\n",
      "-- train ok ! --\n",
      "fold 729: \n",
      "-- training --\n",
      "90.9053840637207\n",
      "-- train ok ! --\n",
      "fold 730: \n",
      "-- training --\n",
      "87.90548658370972\n",
      "-- train ok ! --\n",
      "0.8608333333333332\n",
      "fold 731: \n",
      "-- training --\n",
      "144.32584691047668\n",
      "-- train ok ! --\n",
      "fold 732: \n",
      "-- training --\n",
      "144.2216398715973\n",
      "-- train ok ! --\n",
      "fold 733: \n",
      "-- training --\n",
      "144.86106133460999\n",
      "-- train ok ! --\n",
      "fold 734: \n",
      "-- training --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153.09867691993713\n",
      "-- train ok ! --\n",
      "fold 735: \n",
      "-- training --\n",
      "146.26311612129211\n",
      "-- train ok ! --\n",
      "0.8641249999999999\n",
      "fold 736: \n",
      "-- training --\n",
      "166.77199912071228\n",
      "-- train ok ! --\n",
      "fold 737: \n",
      "-- training --\n",
      "169.15910840034485\n",
      "-- train ok ! --\n",
      "fold 738: \n",
      "-- training --\n",
      "161.6543152332306\n",
      "-- train ok ! --\n",
      "fold 739: \n",
      "-- training --\n",
      "154.44434356689453\n",
      "-- train ok ! --\n",
      "fold 740: \n",
      "-- training --\n",
      "155.95348286628723\n",
      "-- train ok ! --\n",
      "0.8642083333333334\n",
      "fold 741: \n",
      "-- training --\n",
      "169.48384857177734\n",
      "-- train ok ! --\n",
      "fold 742: \n",
      "-- training --\n",
      "170.49312567710876\n",
      "-- train ok ! --\n",
      "fold 743: \n",
      "-- training --\n",
      "169.33615899085999\n",
      "-- train ok ! --\n",
      "fold 744: \n",
      "-- training --\n",
      "173.52944469451904\n",
      "-- train ok ! --\n",
      "fold 745: \n",
      "-- training --\n",
      "183.80160474777222\n",
      "-- train ok ! --\n",
      "0.8634166666666667\n",
      "fold 746: \n",
      "-- training --\n",
      "211.6776990890503\n",
      "-- train ok ! --\n",
      "fold 747: \n",
      "-- training --\n",
      "210.88516783714294\n",
      "-- train ok ! --\n",
      "fold 748: \n",
      "-- training --\n",
      "209.95014357566833\n",
      "-- train ok ! --\n",
      "fold 749: \n",
      "-- training --\n",
      "210.30523037910461\n",
      "-- train ok ! --\n",
      "fold 750: \n",
      "-- training --\n",
      "213.50059914588928\n",
      "-- train ok ! --\n",
      "0.8620208333333335\n",
      "fold 751: \n",
      "-- training --\n",
      "2.778169631958008\n",
      "-- train ok ! --\n",
      "fold 752: \n",
      "-- training --\n",
      "2.752340793609619\n",
      "-- train ok ! --\n",
      "fold 753: \n",
      "-- training --\n",
      "2.7534501552581787\n",
      "-- train ok ! --\n",
      "fold 754: \n",
      "-- training --\n",
      "2.7489445209503174\n",
      "-- train ok ! --\n",
      "fold 755: \n",
      "-- training --\n",
      "2.7487704753875732\n",
      "-- train ok ! --\n",
      "0.7423333333333334\n",
      "fold 756: \n",
      "-- training --\n",
      "7.523060083389282\n",
      "-- train ok ! --\n",
      "fold 757: \n",
      "-- training --\n",
      "7.524815082550049\n",
      "-- train ok ! --\n",
      "fold 758: \n",
      "-- training --\n",
      "7.513488054275513\n",
      "-- train ok ! --\n",
      "fold 759: \n",
      "-- training --\n",
      "7.622196435928345\n",
      "-- train ok ! --\n",
      "fold 760: \n",
      "-- training --\n",
      "8.073753833770752\n",
      "-- train ok ! --\n",
      "0.8457708333333332\n",
      "fold 761: \n",
      "-- training --\n",
      "14.76129674911499\n",
      "-- train ok ! --\n",
      "fold 762: \n",
      "-- training --\n",
      "15.303294897079468\n",
      "-- train ok ! --\n",
      "fold 763: \n",
      "-- training --\n",
      "14.875229597091675\n",
      "-- train ok ! --\n",
      "fold 764: \n",
      "-- training --\n",
      "14.728479623794556\n",
      "-- train ok ! --\n",
      "fold 765: \n",
      "-- training --\n",
      "14.03958010673523\n",
      "-- train ok ! --\n",
      "0.8521041666666666\n",
      "fold 766: \n",
      "-- training --\n",
      "48.32810854911804\n",
      "-- train ok ! --\n",
      "fold 767: \n",
      "-- training --\n",
      "47.747679471969604\n",
      "-- train ok ! --\n",
      "fold 768: \n",
      "-- training --\n",
      "46.622236490249634\n",
      "-- train ok ! --\n",
      "fold 769: \n",
      "-- training --\n",
      "45.520182847976685\n",
      "-- train ok ! --\n",
      "fold 770: \n",
      "-- training --\n",
      "46.578168630599976\n",
      "-- train ok ! --\n",
      "0.8589583333333332\n",
      "fold 771: \n",
      "-- training --\n",
      "94.0283522605896\n",
      "-- train ok ! --\n",
      "fold 772: \n",
      "-- training --\n",
      "92.50616335868835\n",
      "-- train ok ! --\n",
      "fold 773: \n",
      "-- training --\n",
      "98.59171175956726\n",
      "-- train ok ! --\n",
      "fold 774: \n",
      "-- training --\n",
      "90.52710390090942\n",
      "-- train ok ! --\n",
      "fold 775: \n",
      "-- training --\n",
      "90.69135642051697\n",
      "-- train ok ! --\n",
      "0.8618541666666667\n",
      "fold 776: \n",
      "-- training --\n",
      "146.70781445503235\n",
      "-- train ok ! --\n",
      "fold 777: \n",
      "-- training --\n",
      "159.16532850265503\n",
      "-- train ok ! --\n",
      "fold 778: \n",
      "-- training --\n",
      "148.24705362319946\n",
      "-- train ok ! --\n",
      "fold 779: \n",
      "-- training --\n",
      "162.34734869003296\n",
      "-- train ok ! --\n",
      "fold 780: \n",
      "-- training --\n",
      "147.5208113193512\n",
      "-- train ok ! --\n",
      "0.8616666666666667\n",
      "fold 781: \n",
      "-- training --\n",
      "171.63575911521912\n",
      "-- train ok ! --\n",
      "fold 782: \n",
      "-- training --\n",
      "162.5445601940155\n",
      "-- train ok ! --\n",
      "fold 783: \n",
      "-- training --\n",
      "160.22213077545166\n",
      "-- train ok ! --\n",
      "fold 784: \n",
      "-- training --\n",
      "154.8049759864807\n",
      "-- train ok ! --\n",
      "fold 785: \n",
      "-- training --\n",
      "157.3900864124298\n",
      "-- train ok ! --\n",
      "0.8631666666666666\n",
      "fold 786: \n",
      "-- training --\n",
      "170.22016644477844\n",
      "-- train ok ! --\n",
      "fold 787: \n",
      "-- training --\n",
      "171.0300235748291\n",
      "-- train ok ! --\n",
      "fold 788: \n",
      "-- training --\n",
      "175.9830071926117\n",
      "-- train ok ! --\n",
      "fold 789: \n",
      "-- training --\n",
      "176.9990792274475\n",
      "-- train ok ! --\n",
      "fold 790: \n",
      "-- training --\n",
      "170.30527448654175\n",
      "-- train ok ! --\n",
      "0.8616249999999999\n",
      "fold 791: \n",
      "-- training --\n",
      "215.04669499397278\n",
      "-- train ok ! --\n",
      "fold 792: \n",
      "-- training --\n",
      "228.24001812934875\n",
      "-- train ok ! --\n",
      "fold 793: \n",
      "-- training --\n",
      "226.67916417121887\n",
      "-- train ok ! --\n",
      "fold 794: \n",
      "-- training --\n",
      "227.92721271514893\n",
      "-- train ok ! --\n",
      "fold 795: \n",
      "-- training --\n",
      "214.64148926734924\n",
      "-- train ok ! --\n",
      "0.858625\n",
      "fold 796: \n",
      "-- training --\n",
      "2.760153293609619\n",
      "-- train ok ! --\n",
      "fold 797: \n",
      "-- training --\n",
      "2.7570886611938477\n",
      "-- train ok ! --\n",
      "fold 798: \n",
      "-- training --\n",
      "2.7548105716705322\n",
      "-- train ok ! --\n",
      "fold 799: \n",
      "-- training --\n",
      "2.751530170440674\n",
      "-- train ok ! --\n",
      "fold 800: \n",
      "-- training --\n",
      "2.756817579269409\n",
      "-- train ok ! --\n",
      "0.7442083333333332\n",
      "fold 801: \n",
      "-- training --\n",
      "7.470525026321411\n",
      "-- train ok ! --\n",
      "fold 802: \n",
      "-- training --\n",
      "7.5144944190979\n",
      "-- train ok ! --\n",
      "fold 803: \n",
      "-- training --\n",
      "7.44015097618103\n",
      "-- train ok ! --\n",
      "fold 804: \n",
      "-- training --\n",
      "7.523962020874023\n",
      "-- train ok ! --\n",
      "fold 805: \n",
      "-- training --\n",
      "7.5248048305511475\n",
      "-- train ok ! --\n",
      "0.8455833333333332\n",
      "fold 806: \n",
      "-- training --\n",
      "13.446037530899048\n",
      "-- train ok ! --\n",
      "fold 807: \n",
      "-- training --\n",
      "13.632834672927856\n",
      "-- train ok ! --\n",
      "fold 808: \n",
      "-- training --\n",
      "13.543431282043457\n",
      "-- train ok ! --\n",
      "fold 809: \n",
      "-- training --\n",
      "13.41783332824707\n",
      "-- train ok ! --\n",
      "fold 810: \n",
      "-- training --\n",
      "13.514093399047852\n",
      "-- train ok ! --\n",
      "0.8526875\n",
      "fold 811: \n",
      "-- training --\n",
      "43.76335430145264\n",
      "-- train ok ! --\n",
      "fold 812: \n",
      "-- training --\n",
      "46.689154863357544\n",
      "-- train ok ! --\n",
      "fold 813: \n",
      "-- training --\n",
      "45.792858839035034\n",
      "-- train ok ! --\n",
      "fold 814: \n",
      "-- training --\n",
      "46.560641050338745\n",
      "-- train ok ! --\n",
      "fold 815: \n",
      "-- training --\n",
      "45.7697548866272\n",
      "-- train ok ! --\n",
      "0.8598333333333334\n",
      "fold 816: \n",
      "-- training --\n",
      "92.67377710342407\n",
      "-- train ok ! --\n",
      "fold 817: \n",
      "-- training --\n",
      "93.64284038543701\n",
      "-- train ok ! --\n",
      "fold 818: \n",
      "-- training --\n",
      "90.21472382545471\n",
      "-- train ok ! --\n",
      "fold 819: \n",
      "-- training --\n",
      "90.8694679737091\n",
      "-- train ok ! --\n",
      "fold 820: \n",
      "-- training --\n",
      "90.61348533630371\n",
      "-- train ok ! --\n",
      "0.8625833333333333\n",
      "fold 821: \n",
      "-- training --\n",
      "149.10334467887878\n",
      "-- train ok ! --\n",
      "fold 822: \n",
      "-- training --\n",
      "160.08140087127686\n",
      "-- train ok ! --\n",
      "fold 823: \n",
      "-- training --\n",
      "148.78998947143555\n",
      "-- train ok ! --\n",
      "fold 824: \n",
      "-- training --\n",
      "163.12551927566528\n",
      "-- train ok ! --\n",
      "fold 825: \n",
      "-- training --\n",
      "149.4277148246765\n",
      "-- train ok ! --\n",
      "0.8632916666666667\n",
      "fold 826: \n",
      "-- training --\n",
      "169.44600319862366\n",
      "-- train ok ! --\n",
      "fold 827: \n",
      "-- training --\n",
      "170.12868285179138\n",
      "-- train ok ! --\n",
      "fold 828: \n",
      "-- training --\n",
      "168.92364740371704\n",
      "-- train ok ! --\n",
      "fold 829: \n",
      "-- training --\n",
      "169.76212882995605\n",
      "-- train ok ! --\n",
      "fold 830: \n",
      "-- training --\n",
      "168.54595708847046\n",
      "-- train ok ! --\n",
      "0.8639583333333333\n",
      "fold 831: \n",
      "-- training --\n",
      "170.87362003326416\n",
      "-- train ok ! --\n",
      "fold 832: \n",
      "-- training --\n",
      "172.8053858280182\n",
      "-- train ok ! --\n",
      "fold 833: \n",
      "-- training --\n",
      "170.43420386314392\n",
      "-- train ok ! --\n",
      "fold 834: \n",
      "-- training --\n",
      "172.02263498306274\n",
      "-- train ok ! --\n",
      "fold 835: \n",
      "-- training --\n",
      "181.79349184036255\n",
      "-- train ok ! --\n",
      "0.8634583333333333\n",
      "fold 836: \n",
      "-- training --\n",
      "210.11827898025513\n",
      "-- train ok ! --\n",
      "fold 837: \n",
      "-- training --\n",
      "218.61821389198303\n",
      "-- train ok ! --\n",
      "fold 838: \n",
      "-- training --\n",
      "226.78521823883057\n",
      "-- train ok ! --\n",
      "fold 839: \n",
      "-- training --\n",
      "227.91104817390442\n",
      "-- train ok ! --\n",
      "fold 840: \n",
      "-- training --\n",
      "224.4674952030182\n",
      "-- train ok ! --\n",
      "0.8590208333333333\n",
      "fold 841: \n",
      "-- training --\n",
      "2.8103137016296387\n",
      "-- train ok ! --\n",
      "fold 842: \n",
      "-- training --\n",
      "2.8976001739501953\n",
      "-- train ok ! --\n",
      "fold 843: \n",
      "-- training --\n",
      "2.8996894359588623\n",
      "-- train ok ! --\n",
      "fold 844: \n",
      "-- training --\n",
      "2.900665521621704\n",
      "-- train ok ! --\n",
      "fold 845: \n",
      "-- training --\n",
      "2.8969225883483887\n",
      "-- train ok ! --\n",
      "0.7449791666666666\n",
      "fold 846: \n",
      "-- training --\n",
      "7.577295541763306\n",
      "-- train ok ! --\n",
      "fold 847: \n",
      "-- training --\n",
      "7.7474470138549805\n",
      "-- train ok ! --\n",
      "fold 848: \n",
      "-- training --\n",
      "7.829928398132324\n",
      "-- train ok ! --\n",
      "fold 849: \n",
      "-- training --\n",
      "7.585191965103149\n",
      "-- train ok ! --\n",
      "fold 850: \n",
      "-- training --\n",
      "7.798934459686279\n",
      "-- train ok ! --\n",
      "0.8449375\n",
      "fold 851: \n",
      "-- training --\n",
      "13.628156185150146\n",
      "-- train ok ! --\n",
      "fold 852: \n",
      "-- training --\n",
      "13.828959941864014\n",
      "-- train ok ! --\n",
      "fold 853: \n",
      "-- training --\n",
      "13.905734300613403\n",
      "-- train ok ! --\n",
      "fold 854: \n",
      "-- training --\n",
      "13.855615615844727\n",
      "-- train ok ! --\n",
      "fold 855: \n",
      "-- training --\n",
      "13.93831992149353\n",
      "-- train ok ! --\n",
      "0.8526250000000001\n",
      "fold 856: \n",
      "-- training --\n",
      "43.01839804649353\n",
      "-- train ok ! --\n",
      "fold 857: \n",
      "-- training --\n",
      "43.143909215927124\n",
      "-- train ok ! --\n",
      "fold 858: \n",
      "-- training --\n",
      "43.731128215789795\n",
      "-- train ok ! --\n",
      "fold 859: \n",
      "-- training --\n",
      "44.42372250556946\n",
      "-- train ok ! --\n",
      "fold 860: \n",
      "-- training --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.1294367313385\n",
      "-- train ok ! --\n",
      "0.8581249999999999\n",
      "fold 861: \n",
      "-- training --\n",
      "96.09894919395447\n",
      "-- train ok ! --\n",
      "fold 862: \n",
      "-- training --\n",
      "102.57143640518188\n",
      "-- train ok ! --\n",
      "fold 863: \n",
      "-- training --\n",
      "103.52907490730286\n",
      "-- train ok ! --\n",
      "fold 864: \n",
      "-- training --\n",
      "102.87722563743591\n",
      "-- train ok ! --\n",
      "fold 865: \n",
      "-- training --\n",
      "101.92560648918152\n",
      "-- train ok ! --\n",
      "0.8609166666666667\n",
      "fold 866: \n",
      "-- training --\n",
      "149.08211421966553\n",
      "-- train ok ! --\n",
      "fold 867: \n",
      "-- training --\n",
      "148.06838130950928\n",
      "-- train ok ! --\n",
      "fold 868: \n",
      "-- training --\n",
      "149.9459707736969\n",
      "-- train ok ! --\n",
      "fold 869: \n",
      "-- training --\n",
      "165.01996541023254\n",
      "-- train ok ! --\n",
      "fold 870: \n",
      "-- training --\n",
      "151.5844964981079\n",
      "-- train ok ! --\n",
      "0.8626875\n",
      "fold 871: \n",
      "-- training --\n",
      "161.3347671031952\n",
      "-- train ok ! --\n",
      "fold 872: \n",
      "-- training --\n",
      "170.43531322479248\n",
      "-- train ok ! --\n",
      "fold 873: \n",
      "-- training --\n",
      "169.27884554862976\n",
      "-- train ok ! --\n",
      "fold 874: \n",
      "-- training --\n",
      "170.7889380455017\n",
      "-- train ok ! --\n",
      "fold 875: \n",
      "-- training --\n",
      "170.63199043273926\n",
      "-- train ok ! --\n",
      "0.8615416666666669\n",
      "fold 876: \n",
      "-- training --\n",
      "186.1475899219513\n",
      "-- train ok ! --\n",
      "fold 877: \n",
      "-- training --\n",
      "187.65393471717834\n",
      "-- train ok ! --\n",
      "fold 878: \n",
      "-- training --\n",
      "184.36541604995728\n",
      "-- train ok ! --\n",
      "fold 879: \n",
      "-- training --\n",
      "187.2112317085266\n",
      "-- train ok ! --\n",
      "fold 880: \n",
      "-- training --\n",
      "178.26838064193726\n",
      "-- train ok ! --\n",
      "0.8600208333333332\n",
      "fold 881: \n",
      "-- training --\n",
      "222.55835437774658\n",
      "-- train ok ! --\n",
      "fold 882: \n",
      "-- training --\n",
      "217.1655411720276\n",
      "-- train ok ! --\n",
      "fold 883: \n",
      "-- training --\n",
      "223.4346251487732\n",
      "-- train ok ! --\n",
      "fold 884: \n",
      "-- training --\n",
      "209.79760456085205\n",
      "-- train ok ! --\n",
      "fold 885: \n",
      "-- training --\n",
      "211.14647316932678\n",
      "-- train ok ! --\n",
      "0.8569791666666667\n",
      "fold 886: \n",
      "-- training --\n",
      "2.896664619445801\n",
      "-- train ok ! --\n",
      "fold 887: \n",
      "-- training --\n",
      "2.9027092456817627\n",
      "-- train ok ! --\n",
      "fold 888: \n",
      "-- training --\n",
      "2.983320951461792\n",
      "-- train ok ! --\n",
      "fold 889: \n",
      "-- training --\n",
      "2.9911510944366455\n",
      "-- train ok ! --\n",
      "fold 890: \n",
      "-- training --\n",
      "2.9935548305511475\n",
      "-- train ok ! --\n",
      "0.7475833333333333\n",
      "fold 891: \n",
      "-- training --\n",
      "7.8415093421936035\n",
      "-- train ok ! --\n",
      "fold 892: \n",
      "-- training --\n",
      "7.819029808044434\n",
      "-- train ok ! --\n",
      "fold 893: \n",
      "-- training --\n",
      "7.794471025466919\n",
      "-- train ok ! --\n",
      "fold 894: \n",
      "-- training --\n",
      "7.834341287612915\n",
      "-- train ok ! --\n",
      "fold 895: \n",
      "-- training --\n",
      "7.839715957641602\n",
      "-- train ok ! --\n",
      "0.8451666666666666\n",
      "fold 896: \n",
      "-- training --\n",
      "13.675610065460205\n",
      "-- train ok ! --\n",
      "fold 897: \n",
      "-- training --\n",
      "13.779419898986816\n",
      "-- train ok ! --\n",
      "fold 898: \n",
      "-- training --\n",
      "13.951499462127686\n",
      "-- train ok ! --\n",
      "fold 899: \n",
      "-- training --\n",
      "13.929981231689453\n",
      "-- train ok ! --\n",
      "fold 900: \n",
      "-- training --\n",
      "13.778083086013794\n",
      "-- train ok ! --\n",
      "0.8530208333333335\n",
      "fold 901: \n",
      "-- training --\n",
      "43.78787136077881\n",
      "-- train ok ! --\n",
      "fold 902: \n",
      "-- training --\n",
      "43.5719473361969\n",
      "-- train ok ! --\n",
      "fold 903: \n",
      "-- training --\n",
      "43.434788942337036\n",
      "-- train ok ! --\n",
      "fold 904: \n",
      "-- training --\n",
      "43.04326415061951\n",
      "-- train ok ! --\n",
      "fold 905: \n",
      "-- training --\n",
      "44.14016079902649\n",
      "-- train ok ! --\n",
      "0.8582708333333333\n",
      "fold 906: \n",
      "-- training --\n",
      "96.2637267112732\n",
      "-- train ok ! --\n",
      "fold 907: \n",
      "-- training --\n",
      "93.11213421821594\n",
      "-- train ok ! --\n",
      "fold 908: \n",
      "-- training --\n",
      "93.18476915359497\n",
      "-- train ok ! --\n",
      "fold 909: \n",
      "-- training --\n",
      "97.97080254554749\n",
      "-- train ok ! --\n",
      "fold 910: \n",
      "-- training --\n",
      "104.134690284729\n",
      "-- train ok ! --\n",
      "0.8607291666666667\n",
      "fold 911: \n",
      "-- training --\n",
      "152.58319234848022\n",
      "-- train ok ! --\n",
      "fold 912: \n",
      "-- training --\n",
      "166.52834701538086\n",
      "-- train ok ! --\n",
      "fold 913: \n",
      "-- training --\n",
      "151.3928291797638\n",
      "-- train ok ! --\n",
      "fold 914: \n",
      "-- training --\n",
      "165.75764799118042\n",
      "-- train ok ! --\n",
      "fold 915: \n",
      "-- training --\n",
      "150.7149477005005\n",
      "-- train ok ! --\n",
      "0.8630833333333333\n",
      "fold 916: \n",
      "-- training --\n",
      "164.27036309242249\n",
      "-- train ok ! --\n",
      "fold 917: \n",
      "-- training --\n",
      "156.72529888153076\n",
      "-- train ok ! --\n",
      "fold 918: \n",
      "-- training --\n",
      "158.17655563354492\n",
      "-- train ok ! --\n",
      "fold 919: \n",
      "-- training --\n",
      "169.21135640144348\n",
      "-- train ok ! --\n",
      "fold 920: \n",
      "-- training --\n",
      "165.67452597618103\n",
      "-- train ok ! --\n",
      "0.8621666666666666\n",
      "fold 921: \n",
      "-- training --\n",
      "171.84363889694214\n",
      "-- train ok ! --\n",
      "fold 922: \n",
      "-- training --\n",
      "173.06403064727783\n",
      "-- train ok ! --\n",
      "fold 923: \n",
      "-- training --\n",
      "172.014230966568\n",
      "-- train ok ! --\n",
      "fold 924: \n",
      "-- training --\n",
      "173.08554220199585\n",
      "-- train ok ! --\n",
      "fold 925: \n",
      "-- training --\n",
      "171.99968266487122\n",
      "-- train ok ! --\n",
      "0.8605625\n",
      "fold 926: \n",
      "-- training --\n",
      "227.8622751235962\n",
      "-- train ok ! --\n",
      "fold 927: \n",
      "-- training --\n",
      "214.10155272483826\n",
      "-- train ok ! --\n",
      "fold 928: \n",
      "-- training --\n",
      "211.78032231330872\n",
      "-- train ok ! --\n",
      "fold 929: \n",
      "-- training --\n",
      "222.8620150089264\n",
      "-- train ok ! --\n",
      "fold 930: \n",
      "-- training --\n",
      "227.32179856300354\n",
      "-- train ok ! --\n",
      "0.8581041666666668\n",
      "fold 931: \n",
      "-- training --\n",
      "3.027716875076294\n",
      "-- train ok ! --\n",
      "fold 932: \n",
      "-- training --\n",
      "2.9976582527160645\n",
      "-- train ok ! --\n",
      "fold 933: \n",
      "-- training --\n",
      "2.9467318058013916\n",
      "-- train ok ! --\n",
      "fold 934: \n",
      "-- training --\n",
      "2.94590425491333\n",
      "-- train ok ! --\n",
      "fold 935: \n",
      "-- training --\n",
      "2.944586753845215\n",
      "-- train ok ! --\n",
      "0.7469583333333334\n",
      "fold 936: \n",
      "-- training --\n",
      "7.601606369018555\n",
      "-- train ok ! --\n",
      "fold 937: \n",
      "-- training --\n",
      "7.688765048980713\n",
      "-- train ok ! --\n",
      "fold 938: \n",
      "-- training --\n",
      "7.659539222717285\n",
      "-- train ok ! --\n",
      "fold 939: \n",
      "-- training --\n",
      "7.79925274848938\n",
      "-- train ok ! --\n",
      "fold 940: \n",
      "-- training --\n",
      "7.712723016738892\n",
      "-- train ok ! --\n",
      "0.8455208333333333\n",
      "fold 941: \n",
      "-- training --\n",
      "14.185081958770752\n",
      "-- train ok ! --\n",
      "fold 942: \n",
      "-- training --\n",
      "14.25221562385559\n",
      "-- train ok ! --\n",
      "fold 943: \n",
      "-- training --\n",
      "14.158339500427246\n",
      "-- train ok ! --\n",
      "fold 944: \n",
      "-- training --\n",
      "14.386025428771973\n",
      "-- train ok ! --\n",
      "fold 945: \n",
      "-- training --\n",
      "14.47923755645752\n",
      "-- train ok ! --\n",
      "0.8518333333333334\n",
      "fold 946: \n",
      "-- training --\n",
      "48.13402247428894\n",
      "-- train ok ! --\n",
      "fold 947: \n",
      "-- training --\n",
      "47.68873691558838\n",
      "-- train ok ! --\n",
      "fold 948: \n",
      "-- training --\n",
      "48.406108379364014\n",
      "-- train ok ! --\n",
      "fold 949: \n",
      "-- training --\n",
      "48.86768865585327\n",
      "-- train ok ! --\n",
      "fold 950: \n",
      "-- training --\n",
      "48.833534479141235\n",
      "-- train ok ! --\n",
      "0.8580416666666666\n",
      "fold 951: \n",
      "-- training --\n",
      "103.21826124191284\n",
      "-- train ok ! --\n",
      "fold 952: \n",
      "-- training --\n",
      "106.13707208633423\n",
      "-- train ok ! --\n",
      "fold 953: \n",
      "-- training --\n",
      "93.83381462097168\n",
      "-- train ok ! --\n",
      "fold 954: \n",
      "-- training --\n",
      "94.85028052330017\n",
      "-- train ok ! --\n",
      "fold 955: \n",
      "-- training --\n",
      "101.83843660354614\n",
      "-- train ok ! --\n",
      "0.8598333333333332\n",
      "fold 956: \n",
      "-- training --\n",
      "151.24105429649353\n",
      "-- train ok ! --\n",
      "fold 957: \n",
      "-- training --\n",
      "164.67453980445862\n",
      "-- train ok ! --\n",
      "fold 958: \n",
      "-- training --\n",
      "152.26284337043762\n",
      "-- train ok ! --\n",
      "fold 959: \n",
      "-- training --\n",
      "167.79101729393005\n",
      "-- train ok ! --\n",
      "fold 960: \n",
      "-- training --\n",
      "152.43575072288513\n",
      "-- train ok ! --\n",
      "0.8613125\n",
      "fold 961: \n",
      "-- training --\n",
      "175.54497575759888\n",
      "-- train ok ! --\n",
      "fold 962: \n",
      "-- training --\n",
      "171.3005712032318\n",
      "-- train ok ! --\n",
      "fold 963: \n",
      "-- training --\n",
      "174.73397660255432\n",
      "-- train ok ! --\n",
      "fold 964: \n",
      "-- training --\n",
      "157.50411915779114\n",
      "-- train ok ! --\n",
      "fold 965: \n",
      "-- training --\n",
      "160.84644222259521\n",
      "-- train ok ! --\n",
      "0.8614375000000001\n",
      "fold 966: \n",
      "-- training --\n",
      "186.06875777244568\n",
      "-- train ok ! --\n",
      "fold 967: \n",
      "-- training --\n",
      "187.92827367782593\n",
      "-- train ok ! --\n",
      "fold 968: \n",
      "-- training --\n",
      "186.2891755104065\n",
      "-- train ok ! --\n",
      "fold 969: \n",
      "-- training --\n",
      "186.14049768447876\n",
      "-- train ok ! --\n",
      "fold 970: \n",
      "-- training --\n",
      "177.71972632408142\n",
      "-- train ok ! --\n",
      "0.8588541666666666\n",
      "fold 971: \n",
      "-- training --\n",
      "211.57400035858154\n",
      "-- train ok ! --\n",
      "fold 972: \n",
      "-- training --\n",
      "219.81684279441833\n",
      "-- train ok ! --\n",
      "fold 973: \n",
      "-- training --\n",
      "214.39475893974304\n",
      "-- train ok ! --\n",
      "fold 974: \n",
      "-- training --\n",
      "217.14440059661865\n",
      "-- train ok ! --\n",
      "fold 975: \n",
      "-- training --\n",
      "218.69980025291443\n",
      "-- train ok ! --\n",
      "0.8561666666666665\n",
      "fold 976: \n",
      "-- training --\n",
      "2.8525588512420654\n",
      "-- train ok ! --\n",
      "fold 977: \n",
      "-- training --\n",
      "2.8555946350097656\n",
      "-- train ok ! --\n",
      "fold 978: \n",
      "-- training --\n",
      "2.8513152599334717\n",
      "-- train ok ! --\n",
      "fold 979: \n",
      "-- training --\n",
      "2.8529152870178223\n",
      "-- train ok ! --\n",
      "fold 980: \n",
      "-- training --\n",
      "2.855414628982544\n",
      "-- train ok ! --\n",
      "0.7461041666666667\n",
      "fold 981: \n",
      "-- training --\n",
      "7.744911193847656\n",
      "-- train ok ! --\n",
      "fold 982: \n",
      "-- training --\n",
      "7.748966217041016\n",
      "-- train ok ! --\n",
      "fold 983: \n",
      "-- training --\n",
      "7.791354656219482\n",
      "-- train ok ! --\n",
      "fold 984: \n",
      "-- training --\n",
      "7.823864221572876\n",
      "-- train ok ! --\n",
      "fold 985: \n",
      "-- training --\n",
      "7.8488078117370605\n",
      "-- train ok ! --\n",
      "0.8453333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 986: \n",
      "-- training --\n",
      "14.21054482460022\n",
      "-- train ok ! --\n",
      "fold 987: \n",
      "-- training --\n",
      "14.342503309249878\n",
      "-- train ok ! --\n",
      "fold 988: \n",
      "-- training --\n",
      "14.248502731323242\n",
      "-- train ok ! --\n",
      "fold 989: \n",
      "-- training --\n",
      "14.272944927215576\n",
      "-- train ok ! --\n",
      "fold 990: \n",
      "-- training --\n",
      "14.469484329223633\n",
      "-- train ok ! --\n",
      "0.8526458333333334\n",
      "fold 991: \n",
      "-- training --\n",
      "49.12899303436279\n",
      "-- train ok ! --\n",
      "fold 992: \n",
      "-- training --\n",
      "48.9180121421814\n",
      "-- train ok ! --\n",
      "fold 993: \n",
      "-- training --\n",
      "48.401171922683716\n",
      "-- train ok ! --\n",
      "fold 994: \n",
      "-- training --\n",
      "48.69291543960571\n",
      "-- train ok ! --\n",
      "fold 995: \n",
      "-- training --\n",
      "48.48184394836426\n",
      "-- train ok ! --\n",
      "0.8575416666666668\n",
      "fold 996: \n",
      "-- training --\n",
      "104.48576331138611\n",
      "-- train ok ! --\n",
      "fold 997: \n",
      "-- training --\n",
      "105.95638012886047\n",
      "-- train ok ! --\n",
      "fold 998: \n",
      "-- training --\n",
      "105.63184022903442\n",
      "-- train ok ! --\n",
      "fold 999: \n",
      "-- training --\n",
      "106.20039582252502\n",
      "-- train ok ! --\n",
      "fold 1000: \n",
      "-- training --\n",
      "106.79834747314453\n",
      "-- train ok ! --\n",
      "0.8604166666666668\n",
      "fold 1001: \n",
      "-- training --\n",
      "151.97243785858154\n",
      "-- train ok ! --\n",
      "fold 1002: \n",
      "-- training --\n",
      "151.68403959274292\n",
      "-- train ok ! --\n",
      "fold 1003: \n",
      "-- training --\n",
      "151.49838495254517\n",
      "-- train ok ! --\n",
      "fold 1004: \n",
      "-- training --\n",
      "162.16768407821655\n",
      "-- train ok ! --\n",
      "fold 1005: \n",
      "-- training --\n",
      "152.12621569633484\n",
      "-- train ok ! --\n",
      "0.861125\n",
      "fold 1006: \n",
      "-- training --\n",
      "160.93182635307312\n",
      "-- train ok ! --\n",
      "fold 1007: \n",
      "-- training --\n",
      "157.72617411613464\n",
      "-- train ok ! --\n",
      "fold 1008: \n",
      "-- training --\n",
      "160.47948169708252\n",
      "-- train ok ! --\n",
      "fold 1009: \n",
      "-- training --\n",
      "157.93383479118347\n",
      "-- train ok ! --\n",
      "fold 1010: \n",
      "-- training --\n",
      "160.8181483745575\n",
      "-- train ok ! --\n",
      "0.8614166666666666\n",
      "fold 1011: \n",
      "-- training --\n",
      "184.56975317001343\n",
      "-- train ok ! --\n",
      "fold 1012: \n",
      "-- training --\n",
      "188.49225759506226\n",
      "-- train ok ! --\n",
      "fold 1013: \n",
      "-- training --\n",
      "178.91240549087524\n",
      "-- train ok ! --\n",
      "fold 1014: \n",
      "-- training --\n",
      "173.96507024765015\n",
      "-- train ok ! --\n",
      "fold 1015: \n",
      "-- training --\n",
      "175.42935490608215\n",
      "-- train ok ! --\n",
      "0.8606875\n",
      "fold 1016: \n",
      "-- training --\n",
      "217.23550271987915\n",
      "-- train ok ! --\n",
      "fold 1017: \n",
      "-- training --\n",
      "214.52096009254456\n",
      "-- train ok ! --\n",
      "fold 1018: \n",
      "-- training --\n",
      "223.94522309303284\n",
      "-- train ok ! --\n",
      "fold 1019: \n",
      "-- training --\n",
      "229.23198461532593\n",
      "-- train ok ! --\n",
      "fold 1020: \n",
      "-- training --\n",
      "227.85848879814148\n",
      "-- train ok ! --\n",
      "0.8555208333333333\n",
      "fold 1021: \n",
      "-- training --\n",
      "2.897108554840088\n",
      "-- train ok ! --\n",
      "fold 1022: \n",
      "-- training --\n",
      "2.8971610069274902\n",
      "-- train ok ! --\n",
      "fold 1023: \n",
      "-- training --\n",
      "2.888681173324585\n",
      "-- train ok ! --\n",
      "fold 1024: \n",
      "-- training --\n",
      "2.890918254852295\n",
      "-- train ok ! --\n",
      "fold 1025: \n",
      "-- training --\n",
      "2.891209602355957\n",
      "-- train ok ! --\n",
      "0.7473541666666667\n",
      "fold 1026: \n",
      "-- training --\n",
      "7.982633113861084\n",
      "-- train ok ! --\n",
      "fold 1027: \n",
      "-- training --\n",
      "8.116603136062622\n",
      "-- train ok ! --\n",
      "fold 1028: \n",
      "-- training --\n",
      "8.039429187774658\n",
      "-- train ok ! --\n",
      "fold 1029: \n",
      "-- training --\n",
      "8.06711459159851\n",
      "-- train ok ! --\n",
      "fold 1030: \n",
      "-- training --\n",
      "8.04482913017273\n",
      "-- train ok ! --\n",
      "0.8434375000000001\n",
      "fold 1031: \n",
      "-- training --\n",
      "14.661731243133545\n",
      "-- train ok ! --\n",
      "fold 1032: \n",
      "-- training --\n",
      "14.695507049560547\n",
      "-- train ok ! --\n",
      "fold 1033: \n",
      "-- training --\n",
      "14.575294733047485\n",
      "-- train ok ! --\n",
      "fold 1034: \n",
      "-- training --\n",
      "14.503018617630005\n",
      "-- train ok ! --\n",
      "fold 1035: \n",
      "-- training --\n",
      "14.789846420288086\n",
      "-- train ok ! --\n",
      "0.8510625\n",
      "fold 1036: \n",
      "-- training --\n",
      "49.9352822303772\n",
      "-- train ok ! --\n",
      "fold 1037: \n",
      "-- training --\n",
      "50.56733465194702\n",
      "-- train ok ! --\n",
      "fold 1038: \n",
      "-- training --\n",
      "49.31643581390381\n",
      "-- train ok ! --\n",
      "fold 1039: \n",
      "-- training --\n",
      "49.84844374656677\n",
      "-- train ok ! --\n",
      "fold 1040: \n",
      "-- training --\n",
      "49.279897928237915\n",
      "-- train ok ! --\n",
      "0.8573958333333334\n",
      "fold 1041: \n",
      "-- training --\n",
      "104.75737595558167\n",
      "-- train ok ! --\n",
      "fold 1042: \n",
      "-- training --\n",
      "96.29071879386902\n",
      "-- train ok ! --\n",
      "fold 1043: \n",
      "-- training --\n",
      "95.8184609413147\n",
      "-- train ok ! --\n",
      "fold 1044: \n",
      "-- training --\n",
      "97.96811294555664\n",
      "-- train ok ! --\n",
      "fold 1045: \n",
      "-- training --\n",
      "106.92764616012573\n",
      "-- train ok ! --\n",
      "0.8586874999999999\n",
      "fold 1046: \n",
      "-- training --\n",
      "153.3572165966034\n",
      "-- train ok ! --\n",
      "fold 1047: \n",
      "-- training --\n",
      "168.34610414505005\n",
      "-- train ok ! --\n",
      "fold 1048: \n",
      "-- training --\n",
      "153.4270899295807\n",
      "-- train ok ! --\n",
      "fold 1049: \n",
      "-- training --\n",
      "168.8699812889099\n",
      "-- train ok ! --\n",
      "fold 1050: \n",
      "-- training --\n",
      "153.1197144985199\n",
      "-- train ok ! --\n",
      "0.8600833333333332\n",
      "fold 1051: \n",
      "-- training --\n",
      "162.00990200042725\n",
      "-- train ok ! --\n",
      "fold 1052: \n",
      "-- training --\n",
      "160.17582058906555\n",
      "-- train ok ! --\n",
      "fold 1053: \n",
      "-- training --\n",
      "175.96932435035706\n",
      "-- train ok ! --\n",
      "fold 1054: \n",
      "-- training --\n",
      "172.2839798927307\n",
      "-- train ok ! --\n",
      "fold 1055: \n",
      "-- training --\n",
      "166.70733404159546\n",
      "-- train ok ! --\n",
      "0.8592291666666666\n",
      "fold 1056: \n",
      "-- training --\n",
      "187.08441495895386\n",
      "-- train ok ! --\n",
      "fold 1057: \n",
      "-- training --\n",
      "188.8823323249817\n",
      "-- train ok ! --\n",
      "fold 1058: \n",
      "-- training --\n",
      "189.86491417884827\n",
      "-- train ok ! --\n",
      "fold 1059: \n",
      "-- training --\n",
      "189.57390904426575\n",
      "-- train ok ! --\n",
      "fold 1060: \n",
      "-- training --\n",
      "186.5853464603424\n",
      "-- train ok ! --\n",
      "0.8584375\n",
      "fold 1061: \n",
      "-- training --\n",
      "227.2068178653717\n",
      "-- train ok ! --\n",
      "fold 1062: \n",
      "-- training --\n",
      "229.1015419960022\n",
      "-- train ok ! --\n",
      "fold 1063: \n",
      "-- training --\n",
      "227.59202313423157\n",
      "-- train ok ! --\n",
      "fold 1064: \n",
      "-- training --\n",
      "229.5468020439148\n",
      "-- train ok ! --\n",
      "fold 1065: \n",
      "-- training --\n",
      "228.01004719734192\n",
      "-- train ok ! --\n",
      "0.8542708333333333\n",
      "fold 1066: \n",
      "-- training --\n",
      "2.9009897708892822\n",
      "-- train ok ! --\n",
      "fold 1067: \n",
      "-- training --\n",
      "2.997997760772705\n",
      "-- train ok ! --\n",
      "fold 1068: \n",
      "-- training --\n",
      "2.9921982288360596\n",
      "-- train ok ! --\n",
      "fold 1069: \n",
      "-- training --\n",
      "2.8982505798339844\n",
      "-- train ok ! --\n",
      "fold 1070: \n",
      "-- training --\n",
      "2.8939011096954346\n",
      "-- train ok ! --\n",
      "0.7481875\n",
      "fold 1071: \n",
      "-- training --\n",
      "7.984894275665283\n",
      "-- train ok ! --\n",
      "fold 1072: \n",
      "-- training --\n",
      "7.8972673416137695\n",
      "-- train ok ! --\n",
      "fold 1073: \n",
      "-- training --\n",
      "7.94490909576416\n",
      "-- train ok ! --\n",
      "fold 1074: \n",
      "-- training --\n",
      "7.980803728103638\n",
      "-- train ok ! --\n",
      "fold 1075: \n",
      "-- training --\n",
      "8.007457971572876\n",
      "-- train ok ! --\n",
      "0.8447291666666666\n",
      "fold 1076: \n",
      "-- training --\n",
      "14.49732494354248\n",
      "-- train ok ! --\n",
      "fold 1077: \n",
      "-- training --\n",
      "14.663728952407837\n",
      "-- train ok ! --\n",
      "fold 1078: \n",
      "-- training --\n",
      "14.657509326934814\n",
      "-- train ok ! --\n",
      "fold 1079: \n",
      "-- training --\n",
      "14.911273717880249\n",
      "-- train ok ! --\n",
      "fold 1080: \n",
      "-- training --\n",
      "14.505675315856934\n",
      "-- train ok ! --\n",
      "0.8507708333333334\n",
      "fold 1081: \n",
      "-- training --\n",
      "50.10567307472229\n",
      "-- train ok ! --\n",
      "fold 1082: \n",
      "-- training --\n",
      "51.10215353965759\n",
      "-- train ok ! --\n",
      "fold 1083: \n",
      "-- training --\n",
      "50.39427304267883\n",
      "-- train ok ! --\n",
      "fold 1084: \n",
      "-- training --\n",
      "49.881622076034546\n",
      "-- train ok ! --\n",
      "fold 1085: \n",
      "-- training --\n",
      "50.233986139297485\n",
      "-- train ok ! --\n",
      "0.8569791666666667\n",
      "fold 1086: \n",
      "-- training --\n",
      "106.56418347358704\n",
      "-- train ok ! --\n",
      "fold 1087: \n",
      "-- training --\n",
      "107.56461954116821\n",
      "-- train ok ! --\n",
      "fold 1088: \n",
      "-- training --\n",
      "108.43326902389526\n",
      "-- train ok ! --\n",
      "fold 1089: \n",
      "-- training --\n",
      "108.84579920768738\n",
      "-- train ok ! --\n",
      "fold 1090: \n",
      "-- training --\n",
      "99.2982428073883\n",
      "-- train ok ! --\n",
      "0.8594583333333332\n",
      "fold 1091: \n",
      "-- training --\n",
      "153.7833104133606\n",
      "-- train ok ! --\n",
      "fold 1092: \n",
      "-- training --\n",
      "152.92343401908875\n",
      "-- train ok ! --\n",
      "fold 1093: \n",
      "-- training --\n",
      "154.19174790382385\n",
      "-- train ok ! --\n",
      "fold 1094: \n",
      "-- training --\n",
      "154.48877573013306\n",
      "-- train ok ! --\n",
      "fold 1095: \n",
      "-- training --\n",
      "153.11942648887634\n",
      "-- train ok ! --\n",
      "0.8599583333333334\n",
      "fold 1096: \n",
      "-- training --\n",
      "165.02176642417908\n",
      "-- train ok ! --\n",
      "fold 1097: \n",
      "-- training --\n",
      "174.44950985908508\n",
      "-- train ok ! --\n",
      "fold 1098: \n",
      "-- training --\n",
      "172.68626165390015\n",
      "-- train ok ! --\n",
      "fold 1099: \n",
      "-- training --\n",
      "172.79640245437622\n",
      "-- train ok ! --\n",
      "fold 1100: \n",
      "-- training --\n",
      "173.66672778129578\n",
      "-- train ok ! --\n",
      "0.8590833333333334\n",
      "fold 1101: \n",
      "-- training --\n",
      "187.19914984703064\n",
      "-- train ok ! --\n",
      "fold 1102: \n",
      "-- training --\n",
      "180.63319444656372\n",
      "-- train ok ! --\n",
      "fold 1103: \n",
      "-- training --\n",
      "173.6815903186798\n",
      "-- train ok ! --\n",
      "fold 1104: \n",
      "-- training --\n",
      "174.81108355522156\n",
      "-- train ok ! --\n",
      "fold 1105: \n",
      "-- training --\n",
      "173.83528089523315\n",
      "-- train ok ! --\n",
      "0.8592916666666666\n",
      "fold 1106: \n",
      "-- training --\n",
      "221.46713709831238\n",
      "-- train ok ! --\n",
      "fold 1107: \n",
      "-- training --\n",
      "217.5707597732544\n",
      "-- train ok ! --\n",
      "fold 1108: \n",
      "-- training --\n",
      "220.98179745674133\n",
      "-- train ok ! --\n",
      "fold 1109: \n",
      "-- training --\n",
      "229.33114194869995\n",
      "-- train ok ! --\n",
      "fold 1110: \n",
      "-- training --\n",
      "218.76905751228333\n",
      "-- train ok ! --\n",
      "0.8542083333333335\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 15):\n",
    "    for n in [5, 20, 30, 50, 70, 100, 130, 150, 200]:\n",
    "        start_transform = time.time()\n",
    "        pca = PCA(n_components=n)\n",
    "        pca.fit(X_train)\n",
    "        X_train_pca = pca.transform(X_train)\n",
    "        pca.transform(X_train)\n",
    "        end_transform = time.time()\n",
    "        X_train_pca = pd.DataFrame(X_train_pca)\n",
    "        performance_accuracy = []\n",
    "        pipe = Pipeline([('knn',KNeighborsClassifier(n_neighbors=k, p=1))])\n",
    "        for (train_index, test_index) in kfold.split(X_train_pca):\n",
    "            print('fold '+str(fold_count)+': ')\n",
    "            print('-- training --')\n",
    "            X_kfold_train_pca, y_kfold_train = X_train_pca.iloc[train_index], y_train.loc[train_index]\n",
    "            X_kfold_test_pca, y_kfold_test = X_train_pca.iloc[test_index], y_train.loc[test_index]\n",
    "            start = time.time()\n",
    "            pipe.fit(X_kfold_train_pca, y_kfold_train)\n",
    "            y_kfold_pred = pipe.predict(X_kfold_test_pca)\n",
    "            end = time.time()\n",
    "            train_time = end-start + end_transform - start_transform\n",
    "            print(train_time)\n",
    "            print('-- train ok ! --')\n",
    "            perf = accuracy_score(y_kfold_test, y_kfold_pred)\n",
    "            performance_accuracy.append(perf)\n",
    "\n",
    "            fold_count = fold_count + 1\n",
    "        validation_times_manhattan_pca.append((k,n,np.mean(train_time)))\n",
    "        accs_knn_manhattan_pca.append((k,n,np.mean(performance_accuracy)))\n",
    "        print(np.mean(performance_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1111: \n",
      "-- training --\n",
      "2.5098018646240234\n",
      "-- train ok ! --\n",
      "fold 1112: \n",
      "-- training --\n",
      "2.555839776992798\n",
      "-- train ok ! --\n",
      "fold 1113: \n",
      "-- training --\n",
      "2.5824224948883057\n",
      "-- train ok ! --\n",
      "fold 1114: \n",
      "-- training --\n",
      "2.537692070007324\n",
      "-- train ok ! --\n",
      "fold 1115: \n",
      "-- training --\n",
      "2.5881481170654297\n",
      "-- train ok ! --\n",
      "0.6955208333333334\n",
      "fold 1116: \n",
      "-- training --\n",
      "4.302845001220703\n",
      "-- train ok ! --\n",
      "fold 1117: \n",
      "-- training --\n",
      "4.458846807479858\n",
      "-- train ok ! --\n",
      "fold 1118: \n",
      "-- training --\n",
      "4.4652509689331055\n",
      "-- train ok ! --\n",
      "fold 1119: \n",
      "-- training --\n",
      "4.465671539306641\n",
      "-- train ok ! --\n",
      "fold 1120: \n",
      "-- training --\n",
      "4.4617133140563965\n",
      "-- train ok ! --\n",
      "0.8214583333333334\n",
      "fold 1121: \n",
      "-- training --\n",
      "5.241373062133789\n",
      "-- train ok ! --\n",
      "fold 1122: \n",
      "-- training --\n",
      "5.457489728927612\n",
      "-- train ok ! --\n",
      "fold 1123: \n",
      "-- training --\n",
      "5.445727586746216\n",
      "-- train ok ! --\n",
      "fold 1124: \n",
      "-- training --\n",
      "5.431725740432739\n",
      "-- train ok ! --\n",
      "fold 1125: \n",
      "-- training --\n",
      "5.4181599617004395\n",
      "-- train ok ! --\n",
      "0.8329583333333334\n",
      "fold 1126: \n",
      "-- training --\n",
      "9.115204572677612\n",
      "-- train ok ! --\n",
      "fold 1127: \n",
      "-- training --\n",
      "9.095938920974731\n",
      "-- train ok ! --\n",
      "fold 1128: \n",
      "-- training --\n",
      "9.078266382217407\n",
      "-- train ok ! --\n",
      "fold 1129: \n",
      "-- training --\n",
      "9.048086166381836\n",
      "-- train ok ! --\n",
      "fold 1130: \n",
      "-- training --\n",
      "9.200315237045288\n",
      "-- train ok ! --\n",
      "0.8397083333333333\n",
      "fold 1131: \n",
      "-- training --\n",
      "15.003289937973022\n",
      "-- train ok ! --\n",
      "fold 1132: \n",
      "-- training --\n",
      "14.854379653930664\n",
      "-- train ok ! --\n",
      "fold 1133: \n",
      "-- training --\n",
      "14.85336971282959\n",
      "-- train ok ! --\n",
      "fold 1134: \n",
      "-- training --\n",
      "14.911299228668213\n",
      "-- train ok ! --\n",
      "fold 1135: \n",
      "-- training --\n",
      "14.767870903015137\n",
      "-- train ok ! --\n",
      "0.8425208333333334\n",
      "fold 1136: \n",
      "-- training --\n",
      "20.826502561569214\n",
      "-- train ok ! --\n",
      "fold 1137: \n",
      "-- training --\n",
      "21.162325382232666\n",
      "-- train ok ! --\n",
      "fold 1138: \n",
      "-- training --\n",
      "20.824095964431763\n",
      "-- train ok ! --\n",
      "fold 1139: \n",
      "-- training --\n",
      "20.845859050750732\n",
      "-- train ok ! --\n",
      "fold 1140: \n",
      "-- training --\n",
      "21.220409631729126\n",
      "-- train ok ! --\n",
      "0.8459791666666667\n",
      "fold 1141: \n",
      "-- training --\n",
      "25.099634885787964\n",
      "-- train ok ! --\n",
      "fold 1142: \n",
      "-- training --\n",
      "25.2385356426239\n",
      "-- train ok ! --\n",
      "fold 1143: \n",
      "-- training --\n",
      "26.93628430366516\n",
      "-- train ok ! --\n",
      "fold 1144: \n",
      "-- training --\n",
      "26.547850608825684\n",
      "-- train ok ! --\n",
      "fold 1145: \n",
      "-- training --\n",
      "26.0362446308136\n",
      "-- train ok ! --\n",
      "0.8499166666666665\n",
      "fold 1146: \n",
      "-- training --\n",
      "28.217622995376587\n",
      "-- train ok ! --\n",
      "fold 1147: \n",
      "-- training --\n",
      "28.38387632369995\n",
      "-- train ok ! --\n",
      "fold 1148: \n",
      "-- training --\n",
      "28.88268280029297\n",
      "-- train ok ! --\n",
      "fold 1149: \n",
      "-- training --\n",
      "30.19604992866516\n",
      "-- train ok ! --\n",
      "fold 1150: \n",
      "-- training --\n",
      "30.00386357307434\n",
      "-- train ok ! --\n",
      "0.8496666666666668\n",
      "fold 1151: \n",
      "-- training --\n",
      "37.399792194366455\n",
      "-- train ok ! --\n",
      "fold 1152: \n",
      "-- training --\n",
      "37.40115523338318\n",
      "-- train ok ! --\n",
      "fold 1153: \n",
      "-- training --\n",
      "37.43881344795227\n",
      "-- train ok ! --\n",
      "fold 1154: \n",
      "-- training --\n",
      "37.1581130027771\n",
      "-- train ok ! --\n",
      "fold 1155: \n",
      "-- training --\n",
      "37.01081705093384\n",
      "-- train ok ! --\n",
      "0.8493541666666667\n",
      "fold 1156: \n",
      "-- training --\n",
      "2.7075355052948\n",
      "-- train ok ! --\n",
      "fold 1157: \n",
      "-- training --\n",
      "2.7253031730651855\n",
      "-- train ok ! --\n",
      "fold 1158: \n",
      "-- training --\n",
      "2.7260994911193848\n",
      "-- train ok ! --\n",
      "fold 1159: \n",
      "-- training --\n",
      "2.6711905002593994\n",
      "-- train ok ! --\n",
      "fold 1160: \n",
      "-- training --\n",
      "2.6617801189422607\n",
      "-- train ok ! --\n",
      "0.6963541666666667\n",
      "fold 1161: \n",
      "-- training --\n",
      "4.481407642364502\n",
      "-- train ok ! --\n",
      "fold 1162: \n",
      "-- training --\n",
      "4.475826025009155\n",
      "-- train ok ! --\n",
      "fold 1163: \n",
      "-- training --\n",
      "4.328312635421753\n",
      "-- train ok ! --\n",
      "fold 1164: \n",
      "-- training --\n",
      "4.493159532546997\n",
      "-- train ok ! --\n",
      "fold 1165: \n",
      "-- training --\n",
      "4.491040945053101\n",
      "-- train ok ! --\n",
      "0.8213958333333334\n",
      "fold 1166: \n",
      "-- training --\n",
      "5.998101711273193\n",
      "-- train ok ! --\n",
      "fold 1167: \n",
      "-- training --\n",
      "5.887131214141846\n",
      "-- train ok ! --\n",
      "fold 1168: \n",
      "-- training --\n",
      "5.856505870819092\n",
      "-- train ok ! --\n",
      "fold 1169: \n",
      "-- training --\n",
      "5.816532611846924\n",
      "-- train ok ! --\n",
      "fold 1170: \n",
      "-- training --\n",
      "5.809706211090088\n",
      "-- train ok ! --\n",
      "0.8325416666666667\n",
      "fold 1171: \n",
      "-- training --\n",
      "10.344358682632446\n",
      "-- train ok ! --\n",
      "fold 1172: \n",
      "-- training --\n",
      "10.25156283378601\n",
      "-- train ok ! --\n",
      "fold 1173: \n",
      "-- training --\n",
      "10.299631357192993\n",
      "-- train ok ! --\n",
      "fold 1174: \n",
      "-- training --\n",
      "10.233098983764648\n",
      "-- train ok ! --\n",
      "fold 1175: \n",
      "-- training --\n",
      "10.397032022476196\n",
      "-- train ok ! --\n",
      "0.8387083333333333\n",
      "fold 1176: \n",
      "-- training --\n",
      "17.502371549606323\n",
      "-- train ok ! --\n",
      "fold 1177: \n",
      "-- training --\n",
      "17.805643558502197\n",
      "-- train ok ! --\n",
      "fold 1178: \n",
      "-- training --\n",
      "17.735732316970825\n",
      "-- train ok ! --\n",
      "fold 1179: \n",
      "-- training --\n",
      "17.566553831100464\n",
      "-- train ok ! --\n",
      "fold 1180: \n",
      "-- training --\n",
      "17.283552885055542\n",
      "-- train ok ! --\n",
      "0.8413333333333334\n",
      "fold 1181: \n",
      "-- training --\n",
      "23.640527725219727\n",
      "-- train ok ! --\n",
      "fold 1182: \n",
      "-- training --\n",
      "24.693487405776978\n",
      "-- train ok ! --\n",
      "fold 1183: \n",
      "-- training --\n",
      "23.349090337753296\n",
      "-- train ok ! --\n",
      "fold 1184: \n",
      "-- training --\n",
      "24.8122615814209\n",
      "-- train ok ! --\n",
      "fold 1185: \n",
      "-- training --\n",
      "23.58750867843628\n",
      "-- train ok ! --\n",
      "0.8446458333333332\n",
      "fold 1186: \n",
      "-- training --\n",
      "29.37385845184326\n",
      "-- train ok ! --\n",
      "fold 1187: \n",
      "-- training --\n",
      "28.91094970703125\n",
      "-- train ok ! --\n",
      "fold 1188: \n",
      "-- training --\n",
      "29.55742883682251\n",
      "-- train ok ! --\n",
      "fold 1189: \n",
      "-- training --\n",
      "28.65928626060486\n",
      "-- train ok ! --\n",
      "fold 1190: \n",
      "-- training --\n",
      "28.191681623458862\n",
      "-- train ok ! --\n",
      "0.8462083333333335\n",
      "fold 1191: \n",
      "-- training --\n",
      "33.19797658920288\n",
      "-- train ok ! --\n",
      "fold 1192: \n",
      "-- training --\n",
      "33.372478723526\n",
      "-- train ok ! --\n",
      "fold 1193: \n",
      "-- training --\n",
      "32.840487241744995\n",
      "-- train ok ! --\n",
      "fold 1194: \n",
      "-- training --\n",
      "33.05904674530029\n",
      "-- train ok ! --\n",
      "fold 1195: \n",
      "-- training --\n",
      "33.06506085395813\n",
      "-- train ok ! --\n",
      "0.8464791666666667\n",
      "fold 1196: \n",
      "-- training --\n",
      "43.59676504135132\n",
      "-- train ok ! --\n",
      "fold 1197: \n",
      "-- training --\n",
      "43.87619066238403\n",
      "-- train ok ! --\n",
      "fold 1198: \n",
      "-- training --\n",
      "42.06222701072693\n",
      "-- train ok ! --\n",
      "fold 1199: \n",
      "-- training --\n",
      "40.359920501708984\n",
      "-- train ok ! --\n",
      "fold 1200: \n",
      "-- training --\n",
      "41.070772886276245\n",
      "-- train ok ! --\n",
      "0.8478333333333333\n",
      "fold 1201: \n",
      "-- training --\n",
      "2.6598598957061768\n",
      "-- train ok ! --\n",
      "fold 1202: \n",
      "-- training --\n",
      "2.6540822982788086\n",
      "-- train ok ! --\n",
      "fold 1203: \n",
      "-- training --\n",
      "2.6564810276031494\n",
      "-- train ok ! --\n",
      "fold 1204: \n",
      "-- training --\n",
      "2.651573657989502\n",
      "-- train ok ! --\n",
      "fold 1205: \n",
      "-- training --\n",
      "2.723588466644287\n",
      "-- train ok ! --\n",
      "0.7209791666666667\n",
      "fold 1206: \n",
      "-- training --\n",
      "4.6573326587677\n",
      "-- train ok ! --\n",
      "fold 1207: \n",
      "-- training --\n",
      "4.836914300918579\n",
      "-- train ok ! --\n",
      "fold 1208: \n",
      "-- training --\n",
      "4.869309186935425\n",
      "-- train ok ! --\n",
      "fold 1209: \n",
      "-- training --\n",
      "4.845962285995483\n",
      "-- train ok ! --\n",
      "fold 1210: \n",
      "-- training --\n",
      "4.626660346984863\n",
      "-- train ok ! --\n",
      "0.8372708333333332\n",
      "fold 1211: \n",
      "-- training --\n",
      "5.685851812362671\n",
      "-- train ok ! --\n",
      "fold 1212: \n",
      "-- training --\n",
      "5.883119821548462\n",
      "-- train ok ! --\n",
      "fold 1213: \n",
      "-- training --\n",
      "5.867191553115845\n",
      "-- train ok ! --\n",
      "fold 1214: \n",
      "-- training --\n",
      "5.8312225341796875\n",
      "-- train ok ! --\n",
      "fold 1215: \n",
      "-- training --\n",
      "5.8559205532073975\n",
      "-- train ok ! --\n",
      "0.846125\n",
      "fold 1216: \n",
      "-- training --\n",
      "10.717342615127563\n",
      "-- train ok ! --\n",
      "fold 1217: \n",
      "-- training --\n",
      "11.05228877067566\n",
      "-- train ok ! --\n",
      "fold 1218: \n",
      "-- training --\n",
      "10.802245140075684\n",
      "-- train ok ! --\n",
      "fold 1219: \n",
      "-- training --\n",
      "10.790753364562988\n",
      "-- train ok ! --\n",
      "fold 1220: \n",
      "-- training --\n",
      "10.841318845748901\n",
      "-- train ok ! --\n",
      "0.8528333333333332\n",
      "fold 1221: \n",
      "-- training --\n",
      "18.011775016784668\n",
      "-- train ok ! --\n",
      "fold 1222: \n",
      "-- training --\n",
      "18.237571716308594\n",
      "-- train ok ! --\n",
      "fold 1223: \n",
      "-- training --\n",
      "17.19029474258423\n",
      "-- train ok ! --\n",
      "fold 1224: \n",
      "-- training --\n",
      "17.34149432182312\n",
      "-- train ok ! --\n",
      "fold 1225: \n",
      "-- training --\n",
      "16.901844263076782\n",
      "-- train ok ! --\n",
      "0.8547708333333335\n",
      "fold 1226: \n",
      "-- training --\n",
      "24.785528659820557\n",
      "-- train ok ! --\n",
      "fold 1227: \n",
      "-- training --\n",
      "24.424187183380127\n",
      "-- train ok ! --\n",
      "fold 1228: \n",
      "-- training --\n",
      "24.400047540664673\n",
      "-- train ok ! --\n",
      "fold 1229: \n",
      "-- training --\n",
      "24.28949999809265\n",
      "-- train ok ! --\n",
      "fold 1230: \n",
      "-- training --\n",
      "24.873024940490723\n",
      "-- train ok ! --\n",
      "0.8550416666666665\n",
      "fold 1231: \n",
      "-- training --\n",
      "30.626758098602295\n",
      "-- train ok ! --\n",
      "fold 1232: \n",
      "-- training --\n",
      "30.738528728485107\n",
      "-- train ok ! --\n",
      "fold 1233: \n",
      "-- training --\n",
      "30.886247396469116\n",
      "-- train ok ! --\n",
      "fold 1234: \n",
      "-- training --\n",
      "30.67060875892639\n",
      "-- train ok ! --\n",
      "fold 1235: \n",
      "-- training --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.7839252948761\n",
      "-- train ok ! --\n",
      "0.8567916666666665\n",
      "fold 1236: \n",
      "-- training --\n",
      "33.17346382141113\n",
      "-- train ok ! --\n",
      "fold 1237: \n",
      "-- training --\n",
      "33.03625845909119\n",
      "-- train ok ! --\n",
      "fold 1238: \n",
      "-- training --\n",
      "32.81373405456543\n",
      "-- train ok ! --\n",
      "fold 1239: \n",
      "-- training --\n",
      "32.67842221260071\n",
      "-- train ok ! --\n",
      "fold 1240: \n",
      "-- training --\n",
      "32.80339574813843\n",
      "-- train ok ! --\n",
      "0.8557708333333334\n",
      "fold 1241: \n",
      "-- training --\n",
      "45.88925313949585\n",
      "-- train ok ! --\n",
      "fold 1242: \n",
      "-- training --\n",
      "45.928478956222534\n",
      "-- train ok ! --\n",
      "fold 1243: \n",
      "-- training --\n",
      "45.483561992645264\n",
      "-- train ok ! --\n",
      "fold 1244: \n",
      "-- training --\n",
      "44.63035321235657\n",
      "-- train ok ! --\n",
      "fold 1245: \n",
      "-- training --\n",
      "43.02883744239807\n",
      "-- train ok ! --\n",
      "0.8564791666666667\n",
      "fold 1246: \n",
      "-- training --\n",
      "2.8989458084106445\n",
      "-- train ok ! --\n",
      "fold 1247: \n",
      "-- training --\n",
      "2.8661065101623535\n",
      "-- train ok ! --\n",
      "fold 1248: \n",
      "-- training --\n",
      "2.863302230834961\n",
      "-- train ok ! --\n",
      "fold 1249: \n",
      "-- training --\n",
      "2.862736463546753\n",
      "-- train ok ! --\n",
      "fold 1250: \n",
      "-- training --\n",
      "2.864802122116089\n",
      "-- train ok ! --\n",
      "0.7280208333333333\n",
      "fold 1251: \n",
      "-- training --\n",
      "4.629863023757935\n",
      "-- train ok ! --\n",
      "fold 1252: \n",
      "-- training --\n",
      "4.796168327331543\n",
      "-- train ok ! --\n",
      "fold 1253: \n",
      "-- training --\n",
      "4.806978225708008\n",
      "-- train ok ! --\n",
      "fold 1254: \n",
      "-- training --\n",
      "4.797540903091431\n",
      "-- train ok ! --\n",
      "fold 1255: \n",
      "-- training --\n",
      "4.799947261810303\n",
      "-- train ok ! --\n",
      "0.8419791666666667\n",
      "fold 1256: \n",
      "-- training --\n",
      "6.086022138595581\n",
      "-- train ok ! --\n",
      "fold 1257: \n",
      "-- training --\n",
      "6.249167203903198\n",
      "-- train ok ! --\n",
      "fold 1258: \n",
      "-- training --\n",
      "6.2447662353515625\n",
      "-- train ok ! --\n",
      "fold 1259: \n",
      "-- training --\n",
      "6.246706247329712\n",
      "-- train ok ! --\n",
      "fold 1260: \n",
      "-- training --\n",
      "5.999518394470215\n",
      "-- train ok ! --\n",
      "0.8482708333333333\n",
      "fold 1261: \n",
      "-- training --\n",
      "11.484775304794312\n",
      "-- train ok ! --\n",
      "fold 1262: \n",
      "-- training --\n",
      "11.106374502182007\n",
      "-- train ok ! --\n",
      "fold 1263: \n",
      "-- training --\n",
      "10.8396315574646\n",
      "-- train ok ! --\n",
      "fold 1264: \n",
      "-- training --\n",
      "10.750349283218384\n",
      "-- train ok ! --\n",
      "fold 1265: \n",
      "-- training --\n",
      "10.654126405715942\n",
      "-- train ok ! --\n",
      "0.8556666666666667\n",
      "fold 1266: \n",
      "-- training --\n",
      "19.103463411331177\n",
      "-- train ok ! --\n",
      "fold 1267: \n",
      "-- training --\n",
      "18.936737775802612\n",
      "-- train ok ! --\n",
      "fold 1268: \n",
      "-- training --\n",
      "18.98248839378357\n",
      "-- train ok ! --\n",
      "fold 1269: \n",
      "-- training --\n",
      "18.945932865142822\n",
      "-- train ok ! --\n",
      "fold 1270: \n",
      "-- training --\n",
      "18.8986713886261\n",
      "-- train ok ! --\n",
      "0.8569791666666667\n",
      "fold 1271: \n",
      "-- training --\n",
      "25.987504243850708\n",
      "-- train ok ! --\n",
      "fold 1272: \n",
      "-- training --\n",
      "27.251638412475586\n",
      "-- train ok ! --\n",
      "fold 1273: \n",
      "-- training --\n",
      "25.903361320495605\n",
      "-- train ok ! --\n",
      "fold 1274: \n",
      "-- training --\n",
      "27.212677717208862\n",
      "-- train ok ! --\n",
      "fold 1275: \n",
      "-- training --\n",
      "26.442168951034546\n",
      "-- train ok ! --\n",
      "0.8585833333333334\n",
      "fold 1276: \n",
      "-- training --\n",
      "31.671952962875366\n",
      "-- train ok ! --\n",
      "fold 1277: \n",
      "-- training --\n",
      "31.761202812194824\n",
      "-- train ok ! --\n",
      "fold 1278: \n",
      "-- training --\n",
      "31.70058798789978\n",
      "-- train ok ! --\n",
      "fold 1279: \n",
      "-- training --\n",
      "31.744900941848755\n",
      "-- train ok ! --\n",
      "fold 1280: \n",
      "-- training --\n",
      "31.936114072799683\n",
      "-- train ok ! --\n",
      "0.8607083333333334\n",
      "fold 1281: \n",
      "-- training --\n",
      "35.78711175918579\n",
      "-- train ok ! --\n",
      "fold 1282: \n",
      "-- training --\n",
      "36.16273903846741\n",
      "-- train ok ! --\n",
      "fold 1283: \n",
      "-- training --\n",
      "36.10057187080383\n",
      "-- train ok ! --\n",
      "fold 1284: \n",
      "-- training --\n",
      "36.042125940322876\n",
      "-- train ok ! --\n",
      "fold 1285: \n",
      "-- training --\n",
      "36.11210894584656\n",
      "-- train ok ! --\n",
      "0.8596875\n",
      "fold 1286: \n",
      "-- training --\n",
      "44.431578159332275\n",
      "-- train ok ! --\n",
      "fold 1287: \n",
      "-- training --\n",
      "44.349565744400024\n",
      "-- train ok ! --\n",
      "fold 1288: \n",
      "-- training --\n",
      "44.67652368545532\n",
      "-- train ok ! --\n",
      "fold 1289: \n",
      "-- training --\n",
      "43.9731810092926\n",
      "-- train ok ! --\n",
      "fold 1290: \n",
      "-- training --\n",
      "44.43806505203247\n",
      "-- train ok ! --\n",
      "0.8585625\n",
      "fold 1291: \n",
      "-- training --\n",
      "2.6383790969848633\n",
      "-- train ok ! --\n",
      "fold 1292: \n",
      "-- training --\n",
      "2.662397623062134\n",
      "-- train ok ! --\n",
      "fold 1293: \n",
      "-- training --\n",
      "2.663090229034424\n",
      "-- train ok ! --\n",
      "fold 1294: \n",
      "-- training --\n",
      "2.6300530433654785\n",
      "-- train ok ! --\n",
      "fold 1295: \n",
      "-- training --\n",
      "2.59607195854187\n",
      "-- train ok ! --\n",
      "0.7364999999999999\n",
      "fold 1296: \n",
      "-- training --\n",
      "4.499101161956787\n",
      "-- train ok ! --\n",
      "fold 1297: \n",
      "-- training --\n",
      "4.507277250289917\n",
      "-- train ok ! --\n",
      "fold 1298: \n",
      "-- training --\n",
      "4.509385824203491\n",
      "-- train ok ! --\n",
      "fold 1299: \n",
      "-- training --\n",
      "4.505456447601318\n",
      "-- train ok ! --\n",
      "fold 1300: \n",
      "-- training --\n",
      "4.494076251983643\n",
      "-- train ok ! --\n",
      "0.8442708333333334\n",
      "fold 1301: \n",
      "-- training --\n",
      "6.189582586288452\n",
      "-- train ok ! --\n",
      "fold 1302: \n",
      "-- training --\n",
      "6.136551856994629\n",
      "-- train ok ! --\n",
      "fold 1303: \n",
      "-- training --\n",
      "6.088466167449951\n",
      "-- train ok ! --\n",
      "fold 1304: \n",
      "-- training --\n",
      "6.121210336685181\n",
      "-- train ok ! --\n",
      "fold 1305: \n",
      "-- training --\n",
      "6.134362697601318\n",
      "-- train ok ! --\n",
      "0.8509166666666668\n",
      "fold 1306: \n",
      "-- training --\n",
      "10.859311819076538\n",
      "-- train ok ! --\n",
      "fold 1307: \n",
      "-- training --\n",
      "10.717362403869629\n",
      "-- train ok ! --\n",
      "fold 1308: \n",
      "-- training --\n",
      "10.73932147026062\n",
      "-- train ok ! --\n",
      "fold 1309: \n",
      "-- training --\n",
      "10.590437889099121\n",
      "-- train ok ! --\n",
      "fold 1310: \n",
      "-- training --\n",
      "10.746557474136353\n",
      "-- train ok ! --\n",
      "0.8556250000000001\n",
      "fold 1311: \n",
      "-- training --\n",
      "18.094274044036865\n",
      "-- train ok ! --\n",
      "fold 1312: \n",
      "-- training --\n",
      "18.243138790130615\n",
      "-- train ok ! --\n",
      "fold 1313: \n",
      "-- training --\n",
      "17.861878633499146\n",
      "-- train ok ! --\n",
      "fold 1314: \n",
      "-- training --\n",
      "17.91840887069702\n",
      "-- train ok ! --\n",
      "fold 1315: \n",
      "-- training --\n",
      "17.9273943901062\n",
      "-- train ok ! --\n",
      "0.8573958333333334\n",
      "fold 1316: \n",
      "-- training --\n",
      "26.208926677703857\n",
      "-- train ok ! --\n",
      "fold 1317: \n",
      "-- training --\n",
      "26.02204179763794\n",
      "-- train ok ! --\n",
      "fold 1318: \n",
      "-- training --\n",
      "26.244999885559082\n",
      "-- train ok ! --\n",
      "fold 1319: \n",
      "-- training --\n",
      "26.042484998703003\n",
      "-- train ok ! --\n",
      "fold 1320: \n",
      "-- training --\n",
      "26.096694707870483\n",
      "-- train ok ! --\n",
      "0.8577708333333334\n",
      "fold 1321: \n",
      "-- training --\n",
      "33.04766535758972\n",
      "-- train ok ! --\n",
      "fold 1322: \n",
      "-- training --\n",
      "32.55413031578064\n",
      "-- train ok ! --\n",
      "fold 1323: \n",
      "-- training --\n",
      "32.96715235710144\n",
      "-- train ok ! --\n",
      "fold 1324: \n",
      "-- training --\n",
      "32.37939262390137\n",
      "-- train ok ! --\n",
      "fold 1325: \n",
      "-- training --\n",
      "32.86332154273987\n",
      "-- train ok ! --\n",
      "0.8588333333333333\n",
      "fold 1326: \n",
      "-- training --\n",
      "36.556703090667725\n",
      "-- train ok ! --\n",
      "fold 1327: \n",
      "-- training --\n",
      "36.86612391471863\n",
      "-- train ok ! --\n",
      "fold 1328: \n",
      "-- training --\n",
      "36.79286766052246\n",
      "-- train ok ! --\n",
      "fold 1329: \n",
      "-- training --\n",
      "37.187044858932495\n",
      "-- train ok ! --\n",
      "fold 1330: \n",
      "-- training --\n",
      "36.465261697769165\n",
      "-- train ok ! --\n",
      "0.8594166666666666\n",
      "fold 1331: \n",
      "-- training --\n",
      "48.3514404296875\n",
      "-- train ok ! --\n",
      "fold 1332: \n",
      "-- training --\n",
      "48.397435903549194\n",
      "-- train ok ! --\n",
      "fold 1333: \n",
      "-- training --\n",
      "47.87104821205139\n",
      "-- train ok ! --\n",
      "fold 1334: \n",
      "-- training --\n",
      "48.04132294654846\n",
      "-- train ok ! --\n",
      "fold 1335: \n",
      "-- training --\n",
      "47.54223084449768\n",
      "-- train ok ! --\n",
      "0.8581041666666668\n",
      "fold 1336: \n",
      "-- training --\n",
      "2.720482349395752\n",
      "-- train ok ! --\n",
      "fold 1337: \n",
      "-- training --\n",
      "2.756472110748291\n",
      "-- train ok ! --\n",
      "fold 1338: \n",
      "-- training --\n",
      "2.7570037841796875\n",
      "-- train ok ! --\n",
      "fold 1339: \n",
      "-- training --\n",
      "2.727262020111084\n",
      "-- train ok ! --\n",
      "fold 1340: \n",
      "-- training --\n",
      "2.683722734451294\n",
      "-- train ok ! --\n",
      "0.7389375\n",
      "fold 1341: \n",
      "-- training --\n",
      "4.638899326324463\n",
      "-- train ok ! --\n",
      "fold 1342: \n",
      "-- training --\n",
      "4.632166147232056\n",
      "-- train ok ! --\n",
      "fold 1343: \n",
      "-- training --\n",
      "4.638622283935547\n",
      "-- train ok ! --\n",
      "fold 1344: \n",
      "-- training --\n",
      "4.620855093002319\n",
      "-- train ok ! --\n",
      "fold 1345: \n",
      "-- training --\n",
      "4.636396169662476\n",
      "-- train ok ! --\n",
      "0.8444583333333334\n",
      "fold 1346: \n",
      "-- training --\n",
      "6.0339508056640625\n",
      "-- train ok ! --\n",
      "fold 1347: \n",
      "-- training --\n",
      "5.9663519859313965\n",
      "-- train ok ! --\n",
      "fold 1348: \n",
      "-- training --\n",
      "6.016501188278198\n",
      "-- train ok ! --\n",
      "fold 1349: \n",
      "-- training --\n",
      "6.024695634841919\n",
      "-- train ok ! --\n",
      "fold 1350: \n",
      "-- training --\n",
      "6.023866415023804\n",
      "-- train ok ! --\n",
      "0.8519375\n",
      "fold 1351: \n",
      "-- training --\n",
      "11.597945213317871\n",
      "-- train ok ! --\n",
      "fold 1352: \n",
      "-- training --\n",
      "11.510451793670654\n",
      "-- train ok ! --\n",
      "fold 1353: \n",
      "-- training --\n",
      "11.594663143157959\n",
      "-- train ok ! --\n",
      "fold 1354: \n",
      "-- training --\n",
      "11.611952066421509\n",
      "-- train ok ! --\n",
      "fold 1355: \n",
      "-- training --\n",
      "11.525323629379272\n",
      "-- train ok ! --\n",
      "0.8559374999999999\n",
      "fold 1356: \n",
      "-- training --\n",
      "19.519296407699585\n",
      "-- train ok ! --\n",
      "fold 1357: \n",
      "-- training --\n",
      "19.631981372833252\n",
      "-- train ok ! --\n",
      "fold 1358: \n",
      "-- training --\n",
      "19.75144052505493\n",
      "-- train ok ! --\n",
      "fold 1359: \n",
      "-- training --\n",
      "19.882054328918457\n",
      "-- train ok ! --\n",
      "fold 1360: \n",
      "-- training --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.87670922279358\n",
      "-- train ok ! --\n",
      "0.8581666666666667\n",
      "fold 1361: \n",
      "-- training --\n",
      "27.275864839553833\n",
      "-- train ok ! --\n",
      "fold 1362: \n",
      "-- training --\n",
      "29.25928807258606\n",
      "-- train ok ! --\n",
      "fold 1363: \n",
      "-- training --\n",
      "27.17627716064453\n",
      "-- train ok ! --\n",
      "fold 1364: \n",
      "-- training --\n",
      "28.7270827293396\n",
      "-- train ok ! --\n",
      "fold 1365: \n",
      "-- training --\n",
      "26.984129905700684\n",
      "-- train ok ! --\n",
      "0.8604583333333334\n",
      "fold 1366: \n",
      "-- training --\n",
      "33.67358446121216\n",
      "-- train ok ! --\n",
      "fold 1367: \n",
      "-- training --\n",
      "33.15258049964905\n",
      "-- train ok ! --\n",
      "fold 1368: \n",
      "-- training --\n",
      "33.43112802505493\n",
      "-- train ok ! --\n",
      "fold 1369: \n",
      "-- training --\n",
      "33.40131950378418\n",
      "-- train ok ! --\n",
      "fold 1370: \n",
      "-- training --\n",
      "33.87045454978943\n",
      "-- train ok ! --\n",
      "0.8598958333333334\n",
      "fold 1371: \n",
      "-- training --\n",
      "35.63088130950928\n",
      "-- train ok ! --\n",
      "fold 1372: \n",
      "-- training --\n",
      "35.78765320777893\n",
      "-- train ok ! --\n",
      "fold 1373: \n",
      "-- training --\n",
      "35.28521943092346\n",
      "-- train ok ! --\n",
      "fold 1374: \n",
      "-- training --\n",
      "35.82780981063843\n",
      "-- train ok ! --\n",
      "fold 1375: \n",
      "-- training --\n",
      "35.21661901473999\n",
      "-- train ok ! --\n",
      "0.8616041666666666\n",
      "fold 1376: \n",
      "-- training --\n",
      "46.38139581680298\n",
      "-- train ok ! --\n",
      "fold 1377: \n",
      "-- training --\n",
      "49.005021810531616\n",
      "-- train ok ! --\n",
      "fold 1378: \n",
      "-- training --\n",
      "49.188563108444214\n",
      "-- train ok ! --\n",
      "fold 1379: \n",
      "-- training --\n",
      "48.5599148273468\n",
      "-- train ok ! --\n",
      "fold 1380: \n",
      "-- training --\n",
      "48.953070402145386\n",
      "-- train ok ! --\n",
      "0.8596041666666666\n",
      "fold 1381: \n",
      "-- training --\n",
      "2.70794415473938\n",
      "-- train ok ! --\n",
      "fold 1382: \n",
      "-- training --\n",
      "2.706888437271118\n",
      "-- train ok ! --\n",
      "fold 1383: \n",
      "-- training --\n",
      "2.706312894821167\n",
      "-- train ok ! --\n",
      "fold 1384: \n",
      "-- training --\n",
      "2.7141811847686768\n",
      "-- train ok ! --\n",
      "fold 1385: \n",
      "-- training --\n",
      "2.709561586380005\n",
      "-- train ok ! --\n",
      "0.7427291666666667\n",
      "fold 1386: \n",
      "-- training --\n",
      "4.726288080215454\n",
      "-- train ok ! --\n",
      "fold 1387: \n",
      "-- training --\n",
      "4.870070457458496\n",
      "-- train ok ! --\n",
      "fold 1388: \n",
      "-- training --\n",
      "4.879759788513184\n",
      "-- train ok ! --\n",
      "fold 1389: \n",
      "-- training --\n",
      "4.884066343307495\n",
      "-- train ok ! --\n",
      "fold 1390: \n",
      "-- training --\n",
      "4.842137575149536\n",
      "-- train ok ! --\n",
      "0.8452083333333335\n",
      "fold 1391: \n",
      "-- training --\n",
      "6.309844255447388\n",
      "-- train ok ! --\n",
      "fold 1392: \n",
      "-- training --\n",
      "6.4666149616241455\n",
      "-- train ok ! --\n",
      "fold 1393: \n",
      "-- training --\n",
      "6.504379987716675\n",
      "-- train ok ! --\n",
      "fold 1394: \n",
      "-- training --\n",
      "6.40024733543396\n",
      "-- train ok ! --\n",
      "fold 1395: \n",
      "-- training --\n",
      "6.392331838607788\n",
      "-- train ok ! --\n",
      "0.8518125\n",
      "fold 1396: \n",
      "-- training --\n",
      "11.489519119262695\n",
      "-- train ok ! --\n",
      "fold 1397: \n",
      "-- training --\n",
      "11.31499457359314\n",
      "-- train ok ! --\n",
      "fold 1398: \n",
      "-- training --\n",
      "11.466959953308105\n",
      "-- train ok ! --\n",
      "fold 1399: \n",
      "-- training --\n",
      "11.295919418334961\n",
      "-- train ok ! --\n",
      "fold 1400: \n",
      "-- training --\n",
      "11.311893701553345\n",
      "-- train ok ! --\n",
      "0.8559166666666667\n",
      "fold 1401: \n",
      "-- training --\n",
      "20.370774745941162\n",
      "-- train ok ! --\n",
      "fold 1402: \n",
      "-- training --\n",
      "20.580754280090332\n",
      "-- train ok ! --\n",
      "fold 1403: \n",
      "-- training --\n",
      "20.61526322364807\n",
      "-- train ok ! --\n",
      "fold 1404: \n",
      "-- training --\n",
      "20.2762610912323\n",
      "-- train ok ! --\n",
      "fold 1405: \n",
      "-- training --\n",
      "20.30798888206482\n",
      "-- train ok ! --\n",
      "0.8581666666666667\n",
      "fold 1406: \n",
      "-- training --\n",
      "27.786670684814453\n",
      "-- train ok ! --\n",
      "fold 1407: \n",
      "-- training --\n",
      "29.634081602096558\n",
      "-- train ok ! --\n",
      "fold 1408: \n",
      "-- training --\n",
      "27.954907178878784\n",
      "-- train ok ! --\n",
      "fold 1409: \n",
      "-- training --\n",
      "29.371076345443726\n",
      "-- train ok ! --\n",
      "fold 1410: \n",
      "-- training --\n",
      "28.105412006378174\n",
      "-- train ok ! --\n",
      "0.859\n",
      "fold 1411: \n",
      "-- training --\n",
      "32.458518505096436\n",
      "-- train ok ! --\n",
      "fold 1412: \n",
      "-- training --\n",
      "31.538055658340454\n",
      "-- train ok ! --\n",
      "fold 1413: \n",
      "-- training --\n",
      "32.30341458320618\n",
      "-- train ok ! --\n",
      "fold 1414: \n",
      "-- training --\n",
      "31.45241117477417\n",
      "-- train ok ! --\n",
      "fold 1415: \n",
      "-- training --\n",
      "32.09816527366638\n",
      "-- train ok ! --\n",
      "0.8596458333333332\n",
      "fold 1416: \n",
      "-- training --\n",
      "36.0877320766449\n",
      "-- train ok ! --\n",
      "fold 1417: \n",
      "-- training --\n",
      "36.22186851501465\n",
      "-- train ok ! --\n",
      "fold 1418: \n",
      "-- training --\n",
      "35.843066453933716\n",
      "-- train ok ! --\n",
      "fold 1419: \n",
      "-- training --\n",
      "36.254485845565796\n",
      "-- train ok ! --\n",
      "fold 1420: \n",
      "-- training --\n",
      "35.91826915740967\n",
      "-- train ok ! --\n",
      "0.8579166666666665\n",
      "fold 1421: \n",
      "-- training --\n",
      "49.883963108062744\n",
      "-- train ok ! --\n",
      "fold 1422: \n",
      "-- training --\n",
      "50.19776749610901\n",
      "-- train ok ! --\n",
      "fold 1423: \n",
      "-- training --\n",
      "50.18316721916199\n",
      "-- train ok ! --\n",
      "fold 1424: \n",
      "-- training --\n",
      "50.02458906173706\n",
      "-- train ok ! --\n",
      "fold 1425: \n",
      "-- training --\n",
      "47.647807121276855\n",
      "-- train ok ! --\n",
      "0.8581041666666666\n",
      "fold 1426: \n",
      "-- training --\n",
      "2.6192641258239746\n",
      "-- train ok ! --\n",
      "fold 1427: \n",
      "-- training --\n",
      "2.615955352783203\n",
      "-- train ok ! --\n",
      "fold 1428: \n",
      "-- training --\n",
      "2.617918014526367\n",
      "-- train ok ! --\n",
      "fold 1429: \n",
      "-- training --\n",
      "2.6179323196411133\n",
      "-- train ok ! --\n",
      "fold 1430: \n",
      "-- training --\n",
      "2.6173927783966064\n",
      "-- train ok ! --\n",
      "0.7446458333333332\n",
      "fold 1431: \n",
      "-- training --\n",
      "4.624434947967529\n",
      "-- train ok ! --\n",
      "fold 1432: \n",
      "-- training --\n",
      "4.816214561462402\n",
      "-- train ok ! --\n",
      "fold 1433: \n",
      "-- training --\n",
      "4.633988618850708\n",
      "-- train ok ! --\n",
      "fold 1434: \n",
      "-- training --\n",
      "4.631223917007446\n",
      "-- train ok ! --\n",
      "fold 1435: \n",
      "-- training --\n",
      "4.61763596534729\n",
      "-- train ok ! --\n",
      "0.8452708333333334\n",
      "fold 1436: \n",
      "-- training --\n",
      "6.1242125034332275\n",
      "-- train ok ! --\n",
      "fold 1437: \n",
      "-- training --\n",
      "6.122741937637329\n",
      "-- train ok ! --\n",
      "fold 1438: \n",
      "-- training --\n",
      "6.080877304077148\n",
      "-- train ok ! --\n",
      "fold 1439: \n",
      "-- training --\n",
      "6.427456855773926\n",
      "-- train ok ! --\n",
      "fold 1440: \n",
      "-- training --\n",
      "6.3850836753845215\n",
      "-- train ok ! --\n",
      "0.8518541666666668\n",
      "fold 1441: \n",
      "-- training --\n",
      "11.940122604370117\n",
      "-- train ok ! --\n",
      "fold 1442: \n",
      "-- training --\n",
      "11.711914539337158\n",
      "-- train ok ! --\n",
      "fold 1443: \n",
      "-- training --\n",
      "11.845492839813232\n",
      "-- train ok ! --\n",
      "fold 1444: \n",
      "-- training --\n",
      "11.763734817504883\n",
      "-- train ok ! --\n",
      "fold 1445: \n",
      "-- training --\n",
      "11.719000339508057\n",
      "-- train ok ! --\n",
      "0.8570416666666667\n",
      "fold 1446: \n",
      "-- training --\n",
      "19.64279556274414\n",
      "-- train ok ! --\n",
      "fold 1447: \n",
      "-- training --\n",
      "19.33576989173889\n",
      "-- train ok ! --\n",
      "fold 1448: \n",
      "-- training --\n",
      "19.3058762550354\n",
      "-- train ok ! --\n",
      "fold 1449: \n",
      "-- training --\n",
      "19.390445947647095\n",
      "-- train ok ! --\n",
      "fold 1450: \n",
      "-- training --\n",
      "19.22632336616516\n",
      "-- train ok ! --\n",
      "0.8596041666666666\n",
      "fold 1451: \n",
      "-- training --\n",
      "28.5114483833313\n",
      "-- train ok ! --\n",
      "fold 1452: \n",
      "-- training --\n",
      "29.956507682800293\n",
      "-- train ok ! --\n",
      "fold 1453: \n",
      "-- training --\n",
      "28.324500560760498\n",
      "-- train ok ! --\n",
      "fold 1454: \n",
      "-- training --\n",
      "30.501714944839478\n",
      "-- train ok ! --\n",
      "fold 1455: \n",
      "-- training --\n",
      "28.479389429092407\n",
      "-- train ok ! --\n",
      "0.8590833333333332\n",
      "fold 1456: \n",
      "-- training --\n",
      "34.87576508522034\n",
      "-- train ok ! --\n",
      "fold 1457: \n",
      "-- training --\n",
      "35.14266896247864\n",
      "-- train ok ! --\n",
      "fold 1458: \n",
      "-- training --\n",
      "34.96974158287048\n",
      "-- train ok ! --\n",
      "fold 1459: \n",
      "-- training --\n",
      "34.639607667922974\n",
      "-- train ok ! --\n",
      "fold 1460: \n",
      "-- training --\n",
      "34.892902135849\n",
      "-- train ok ! --\n",
      "0.8602916666666667\n",
      "fold 1461: \n",
      "-- training --\n",
      "38.89589548110962\n",
      "-- train ok ! --\n",
      "fold 1462: \n",
      "-- training --\n",
      "39.29604172706604\n",
      "-- train ok ! --\n",
      "fold 1463: \n",
      "-- training --\n",
      "38.85537815093994\n",
      "-- train ok ! --\n",
      "fold 1464: \n",
      "-- training --\n",
      "37.05344891548157\n",
      "-- train ok ! --\n",
      "fold 1465: \n",
      "-- training --\n",
      "36.84235239028931\n",
      "-- train ok ! --\n",
      "0.8603958333333332\n",
      "fold 1466: \n",
      "-- training --\n",
      "48.35259413719177\n",
      "-- train ok ! --\n",
      "fold 1467: \n",
      "-- training --\n",
      "47.565911293029785\n",
      "-- train ok ! --\n",
      "fold 1468: \n",
      "-- training --\n",
      "47.86474537849426\n",
      "-- train ok ! --\n",
      "fold 1469: \n",
      "-- training --\n",
      "47.6524543762207\n",
      "-- train ok ! --\n",
      "fold 1470: \n",
      "-- training --\n",
      "47.89320921897888\n",
      "-- train ok ! --\n",
      "0.8605208333333334\n",
      "fold 1471: \n",
      "-- training --\n",
      "2.620412826538086\n",
      "-- train ok ! --\n",
      "fold 1472: \n",
      "-- training --\n",
      "2.6212923526763916\n",
      "-- train ok ! --\n",
      "fold 1473: \n",
      "-- training --\n",
      "2.6142966747283936\n",
      "-- train ok ! --\n",
      "fold 1474: \n",
      "-- training --\n",
      "2.6151742935180664\n",
      "-- train ok ! --\n",
      "fold 1475: \n",
      "-- training --\n",
      "2.610560894012451\n",
      "-- train ok ! --\n",
      "0.7465625\n",
      "fold 1476: \n",
      "-- training --\n",
      "4.704667329788208\n",
      "-- train ok ! --\n",
      "fold 1477: \n",
      "-- training --\n",
      "4.708521366119385\n",
      "-- train ok ! --\n",
      "fold 1478: \n",
      "-- training --\n",
      "4.719807386398315\n",
      "-- train ok ! --\n",
      "fold 1479: \n",
      "-- training --\n",
      "4.688899517059326\n",
      "-- train ok ! --\n",
      "fold 1480: \n",
      "-- training --\n",
      "4.656806230545044\n",
      "-- train ok ! --\n",
      "0.8441666666666666\n",
      "fold 1481: \n",
      "-- training --\n",
      "6.425533294677734\n",
      "-- train ok ! --\n",
      "fold 1482: \n",
      "-- training --\n",
      "6.428140878677368\n",
      "-- train ok ! --\n",
      "fold 1483: \n",
      "-- training --\n",
      "6.448788404464722\n",
      "-- train ok ! --\n",
      "fold 1484: \n",
      "-- training --\n",
      "6.4397807121276855\n",
      "-- train ok ! --\n",
      "fold 1485: \n",
      "-- training --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.3884899616241455\n",
      "-- train ok ! --\n",
      "0.8513958333333334\n",
      "fold 1486: \n",
      "-- training --\n",
      "11.502240180969238\n",
      "-- train ok ! --\n",
      "fold 1487: \n",
      "-- training --\n",
      "11.462939977645874\n",
      "-- train ok ! --\n",
      "fold 1488: \n",
      "-- training --\n",
      "11.61156678199768\n",
      "-- train ok ! --\n",
      "fold 1489: \n",
      "-- training --\n",
      "11.397571563720703\n",
      "-- train ok ! --\n",
      "fold 1490: \n",
      "-- training --\n",
      "11.442030668258667\n",
      "-- train ok ! --\n",
      "0.8559166666666667\n",
      "fold 1491: \n",
      "-- training --\n",
      "19.813299417495728\n",
      "-- train ok ! --\n",
      "fold 1492: \n",
      "-- training --\n",
      "20.62045192718506\n",
      "-- train ok ! --\n",
      "fold 1493: \n",
      "-- training --\n",
      "20.923282384872437\n",
      "-- train ok ! --\n",
      "fold 1494: \n",
      "-- training --\n",
      "19.739583730697632\n",
      "-- train ok ! --\n",
      "fold 1495: \n",
      "-- training --\n",
      "19.57942509651184\n",
      "-- train ok ! --\n",
      "0.8570625000000002\n",
      "fold 1496: \n",
      "-- training --\n",
      "28.738182544708252\n",
      "-- train ok ! --\n",
      "fold 1497: \n",
      "-- training --\n",
      "28.1728036403656\n",
      "-- train ok ! --\n",
      "fold 1498: \n",
      "-- training --\n",
      "28.720978498458862\n",
      "-- train ok ! --\n",
      "fold 1499: \n",
      "-- training --\n",
      "28.149068355560303\n",
      "-- train ok ! --\n",
      "fold 1500: \n",
      "-- training --\n",
      "28.619142055511475\n",
      "-- train ok ! --\n",
      "0.8588958333333334\n",
      "fold 1501: \n",
      "-- training --\n",
      "33.161386251449585\n",
      "-- train ok ! --\n",
      "fold 1502: \n",
      "-- training --\n",
      "32.66860628128052\n",
      "-- train ok ! --\n",
      "fold 1503: \n",
      "-- training --\n",
      "32.96549105644226\n",
      "-- train ok ! --\n",
      "fold 1504: \n",
      "-- training --\n",
      "32.449228286743164\n",
      "-- train ok ! --\n",
      "fold 1505: \n",
      "-- training --\n",
      "32.63064956665039\n",
      "-- train ok ! --\n",
      "0.8581041666666668\n",
      "fold 1506: \n",
      "-- training --\n",
      "37.26470136642456\n",
      "-- train ok ! --\n",
      "fold 1507: \n",
      "-- training --\n",
      "37.51976275444031\n",
      "-- train ok ! --\n",
      "fold 1508: \n",
      "-- training --\n",
      "36.99301481246948\n",
      "-- train ok ! --\n",
      "fold 1509: \n",
      "-- training --\n",
      "37.69923520088196\n",
      "-- train ok ! --\n",
      "fold 1510: \n",
      "-- training --\n",
      "37.37176537513733\n",
      "-- train ok ! --\n",
      "0.8583124999999999\n",
      "fold 1511: \n",
      "-- training --\n",
      "51.678789377212524\n",
      "-- train ok ! --\n",
      "fold 1512: \n",
      "-- training --\n",
      "52.25809454917908\n",
      "-- train ok ! --\n",
      "fold 1513: \n",
      "-- training --\n",
      "51.55902647972107\n",
      "-- train ok ! --\n",
      "fold 1514: \n",
      "-- training --\n",
      "51.70985388755798\n",
      "-- train ok ! --\n",
      "fold 1515: \n",
      "-- training --\n",
      "51.47130560874939\n",
      "-- train ok ! --\n",
      "0.8580416666666666\n",
      "fold 1516: \n",
      "-- training --\n",
      "2.706995964050293\n",
      "-- train ok ! --\n",
      "fold 1517: \n",
      "-- training --\n",
      "2.7783050537109375\n",
      "-- train ok ! --\n",
      "fold 1518: \n",
      "-- training --\n",
      "2.7739109992980957\n",
      "-- train ok ! --\n",
      "fold 1519: \n",
      "-- training --\n",
      "2.774498224258423\n",
      "-- train ok ! --\n",
      "fold 1520: \n",
      "-- training --\n",
      "2.776171922683716\n",
      "-- train ok ! --\n",
      "0.7469791666666665\n",
      "fold 1521: \n",
      "-- training --\n",
      "5.096279621124268\n",
      "-- train ok ! --\n",
      "fold 1522: \n",
      "-- training --\n",
      "5.075982332229614\n",
      "-- train ok ! --\n",
      "fold 1523: \n",
      "-- training --\n",
      "5.071424961090088\n",
      "-- train ok ! --\n",
      "fold 1524: \n",
      "-- training --\n",
      "5.093687057495117\n",
      "-- train ok ! --\n",
      "fold 1525: \n",
      "-- training --\n",
      "5.099295139312744\n",
      "-- train ok ! --\n",
      "0.843375\n",
      "fold 1526: \n",
      "-- training --\n",
      "6.6766886711120605\n",
      "-- train ok ! --\n",
      "fold 1527: \n",
      "-- training --\n",
      "6.732531785964966\n",
      "-- train ok ! --\n",
      "fold 1528: \n",
      "-- training --\n",
      "6.538938999176025\n",
      "-- train ok ! --\n",
      "fold 1529: \n",
      "-- training --\n",
      "6.718862771987915\n",
      "-- train ok ! --\n",
      "fold 1530: \n",
      "-- training --\n",
      "6.709352493286133\n",
      "-- train ok ! --\n",
      "0.8514791666666668\n",
      "fold 1531: \n",
      "-- training --\n",
      "12.521889209747314\n",
      "-- train ok ! --\n",
      "fold 1532: \n",
      "-- training --\n",
      "12.547345638275146\n",
      "-- train ok ! --\n",
      "fold 1533: \n",
      "-- training --\n",
      "12.525565385818481\n",
      "-- train ok ! --\n",
      "fold 1534: \n",
      "-- training --\n",
      "12.575380325317383\n",
      "-- train ok ! --\n",
      "fold 1535: \n",
      "-- training --\n",
      "12.497058629989624\n",
      "-- train ok ! --\n",
      "0.8553750000000001\n",
      "fold 1536: \n",
      "-- training --\n",
      "21.075337886810303\n",
      "-- train ok ! --\n",
      "fold 1537: \n",
      "-- training --\n",
      "21.245588302612305\n",
      "-- train ok ! --\n",
      "fold 1538: \n",
      "-- training --\n",
      "21.33890962600708\n",
      "-- train ok ! --\n",
      "fold 1539: \n",
      "-- training --\n",
      "21.476817846298218\n",
      "-- train ok ! --\n",
      "fold 1540: \n",
      "-- training --\n",
      "21.36334228515625\n",
      "-- train ok ! --\n",
      "0.8580208333333333\n",
      "fold 1541: \n",
      "-- training --\n",
      "29.39274764060974\n",
      "-- train ok ! --\n",
      "fold 1542: \n",
      "-- training --\n",
      "30.8298397064209\n",
      "-- train ok ! --\n",
      "fold 1543: \n",
      "-- training --\n",
      "29.178090572357178\n",
      "-- train ok ! --\n",
      "fold 1544: \n",
      "-- training --\n",
      "31.051730394363403\n",
      "-- train ok ! --\n",
      "fold 1545: \n",
      "-- training --\n",
      "29.362777948379517\n",
      "-- train ok ! --\n",
      "0.8588541666666666\n",
      "fold 1546: \n",
      "-- training --\n",
      "35.83659267425537\n",
      "-- train ok ! --\n",
      "fold 1547: \n",
      "-- training --\n",
      "33.47826433181763\n",
      "-- train ok ! --\n",
      "fold 1548: \n",
      "-- training --\n",
      "33.75912594795227\n",
      "-- train ok ! --\n",
      "fold 1549: \n",
      "-- training --\n",
      "33.39876651763916\n",
      "-- train ok ! --\n",
      "fold 1550: \n",
      "-- training --\n",
      "36.26781964302063\n",
      "-- train ok ! --\n",
      "0.8592083333333334\n",
      "fold 1551: \n",
      "-- training --\n",
      "40.39954853057861\n",
      "-- train ok ! --\n",
      "fold 1552: \n",
      "-- training --\n",
      "40.51788806915283\n",
      "-- train ok ! --\n",
      "fold 1553: \n",
      "-- training --\n",
      "39.82823181152344\n",
      "-- train ok ! --\n",
      "fold 1554: \n",
      "-- training --\n",
      "40.84878420829773\n",
      "-- train ok ! --\n",
      "fold 1555: \n",
      "-- training --\n",
      "40.59698963165283\n",
      "-- train ok ! --\n",
      "0.8588749999999999\n",
      "fold 1556: \n",
      "-- training --\n",
      "52.40792274475098\n",
      "-- train ok ! --\n",
      "fold 1557: \n",
      "-- training --\n",
      "51.60227704048157\n",
      "-- train ok ! --\n",
      "fold 1558: \n",
      "-- training --\n",
      "52.20388102531433\n",
      "-- train ok ! --\n",
      "fold 1559: \n",
      "-- training --\n",
      "52.16149687767029\n",
      "-- train ok ! --\n",
      "fold 1560: \n",
      "-- training --\n",
      "49.40998363494873\n",
      "-- train ok ! --\n",
      "0.858625\n",
      "fold 1561: \n",
      "-- training --\n",
      "2.9384021759033203\n",
      "-- train ok ! --\n",
      "fold 1562: \n",
      "-- training --\n",
      "2.9355969429016113\n",
      "-- train ok ! --\n",
      "fold 1563: \n",
      "-- training --\n",
      "2.9320802688598633\n",
      "-- train ok ! --\n",
      "fold 1564: \n",
      "-- training --\n",
      "2.9301693439483643\n",
      "-- train ok ! --\n",
      "fold 1565: \n",
      "-- training --\n",
      "2.931210994720459\n",
      "-- train ok ! --\n",
      "0.7468541666666666\n",
      "fold 1566: \n",
      "-- training --\n",
      "4.774734973907471\n",
      "-- train ok ! --\n",
      "fold 1567: \n",
      "-- training --\n",
      "4.749241828918457\n",
      "-- train ok ! --\n",
      "fold 1568: \n",
      "-- training --\n",
      "4.741604566574097\n",
      "-- train ok ! --\n",
      "fold 1569: \n",
      "-- training --\n",
      "4.747754335403442\n",
      "-- train ok ! --\n",
      "fold 1570: \n",
      "-- training --\n",
      "4.730671167373657\n",
      "-- train ok ! --\n",
      "0.8427291666666668\n",
      "fold 1571: \n",
      "-- training --\n",
      "6.507836580276489\n",
      "-- train ok ! --\n",
      "fold 1572: \n",
      "-- training --\n",
      "6.4990925788879395\n",
      "-- train ok ! --\n",
      "fold 1573: \n",
      "-- training --\n",
      "6.516972541809082\n",
      "-- train ok ! --\n",
      "fold 1574: \n",
      "-- training --\n",
      "6.504960060119629\n",
      "-- train ok ! --\n",
      "fold 1575: \n",
      "-- training --\n",
      "6.532315015792847\n",
      "-- train ok ! --\n",
      "0.8498125\n",
      "fold 1576: \n",
      "-- training --\n",
      "12.400640726089478\n",
      "-- train ok ! --\n",
      "fold 1577: \n",
      "-- training --\n",
      "12.472697496414185\n",
      "-- train ok ! --\n",
      "fold 1578: \n",
      "-- training --\n",
      "12.246344804763794\n",
      "-- train ok ! --\n",
      "fold 1579: \n",
      "-- training --\n",
      "12.363684177398682\n",
      "-- train ok ! --\n",
      "fold 1580: \n",
      "-- training --\n",
      "12.257132053375244\n",
      "-- train ok ! --\n",
      "0.8553541666666667\n",
      "fold 1581: \n",
      "-- training --\n",
      "21.44014835357666\n",
      "-- train ok ! --\n",
      "fold 1582: \n",
      "-- training --\n",
      "21.646060943603516\n",
      "-- train ok ! --\n",
      "fold 1583: \n",
      "-- training --\n",
      "21.277474641799927\n",
      "-- train ok ! --\n",
      "fold 1584: \n",
      "-- training --\n",
      "21.40943741798401\n",
      "-- train ok ! --\n",
      "fold 1585: \n",
      "-- training --\n",
      "21.593138217926025\n",
      "-- train ok ! --\n",
      "0.8566666666666667\n",
      "fold 1586: \n",
      "-- training --\n",
      "29.265037298202515\n",
      "-- train ok ! --\n",
      "fold 1587: \n",
      "-- training --\n",
      "30.848921298980713\n",
      "-- train ok ! --\n",
      "fold 1588: \n",
      "-- training --\n",
      "29.741268157958984\n",
      "-- train ok ! --\n",
      "fold 1589: \n",
      "-- training --\n",
      "29.14303231239319\n",
      "-- train ok ! --\n",
      "fold 1590: \n",
      "-- training --\n",
      "29.61436414718628\n",
      "-- train ok ! --\n",
      "0.8574999999999999\n",
      "fold 1591: \n",
      "-- training --\n",
      "36.741183280944824\n",
      "-- train ok ! --\n",
      "fold 1592: \n",
      "-- training --\n",
      "36.3730194568634\n",
      "-- train ok ! --\n",
      "fold 1593: \n",
      "-- training --\n",
      "36.11947560310364\n",
      "-- train ok ! --\n",
      "fold 1594: \n",
      "-- training --\n",
      "35.90276646614075\n",
      "-- train ok ! --\n",
      "fold 1595: \n",
      "-- training --\n",
      "36.13702130317688\n",
      "-- train ok ! --\n",
      "0.8572291666666667\n",
      "fold 1596: \n",
      "-- training --\n",
      "38.15191674232483\n",
      "-- train ok ! --\n",
      "fold 1597: \n",
      "-- training --\n",
      "38.45378661155701\n",
      "-- train ok ! --\n",
      "fold 1598: \n",
      "-- training --\n",
      "38.251770973205566\n",
      "-- train ok ! --\n",
      "fold 1599: \n",
      "-- training --\n",
      "38.48041129112244\n",
      "-- train ok ! --\n",
      "fold 1600: \n",
      "-- training --\n",
      "38.50065994262695\n",
      "-- train ok ! --\n",
      "0.8567083333333334\n",
      "fold 1601: \n",
      "-- training --\n",
      "49.541560649871826\n",
      "-- train ok ! --\n",
      "fold 1602: \n",
      "-- training --\n",
      "49.57255291938782\n",
      "-- train ok ! --\n",
      "fold 1603: \n",
      "-- training --\n",
      "49.820728063583374\n",
      "-- train ok ! --\n",
      "fold 1604: \n",
      "-- training --\n",
      "48.75527620315552\n",
      "-- train ok ! --\n",
      "fold 1605: \n",
      "-- training --\n",
      "49.72431135177612\n",
      "-- train ok ! --\n",
      "0.8564166666666667\n",
      "fold 1606: \n",
      "-- training --\n",
      "2.6334896087646484\n",
      "-- train ok ! --\n",
      "fold 1607: \n",
      "-- training --\n",
      "2.6421926021575928\n",
      "-- train ok ! --\n",
      "fold 1608: \n",
      "-- training --\n",
      "2.6368296146392822\n",
      "-- train ok ! --\n",
      "fold 1609: \n",
      "-- training --\n",
      "2.637053966522217\n",
      "-- train ok ! --\n",
      "fold 1610: \n",
      "-- training --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6358211040496826\n",
      "-- train ok ! --\n",
      "0.7490625\n",
      "fold 1611: \n",
      "-- training --\n",
      "4.87616229057312\n",
      "-- train ok ! --\n",
      "fold 1612: \n",
      "-- training --\n",
      "4.992298603057861\n",
      "-- train ok ! --\n",
      "fold 1613: \n",
      "-- training --\n",
      "4.861636161804199\n",
      "-- train ok ! --\n",
      "fold 1614: \n",
      "-- training --\n",
      "4.815596103668213\n",
      "-- train ok ! --\n",
      "fold 1615: \n",
      "-- training --\n",
      "4.801753997802734\n",
      "-- train ok ! --\n",
      "0.8443333333333334\n",
      "fold 1616: \n",
      "-- training --\n",
      "6.372837543487549\n",
      "-- train ok ! --\n",
      "fold 1617: \n",
      "-- training --\n",
      "6.478344202041626\n",
      "-- train ok ! --\n",
      "fold 1618: \n",
      "-- training --\n",
      "6.334886074066162\n",
      "-- train ok ! --\n",
      "fold 1619: \n",
      "-- training --\n",
      "6.28258204460144\n",
      "-- train ok ! --\n",
      "fold 1620: \n",
      "-- training --\n",
      "6.3346734046936035\n",
      "-- train ok ! --\n",
      "0.8505416666666665\n",
      "fold 1621: \n",
      "-- training --\n",
      "12.255043745040894\n",
      "-- train ok ! --\n",
      "fold 1622: \n",
      "-- training --\n",
      "11.914133071899414\n",
      "-- train ok ! --\n",
      "fold 1623: \n",
      "-- training --\n",
      "11.979498147964478\n",
      "-- train ok ! --\n",
      "fold 1624: \n",
      "-- training --\n",
      "11.699313640594482\n",
      "-- train ok ! --\n",
      "fold 1625: \n",
      "-- training --\n",
      "11.772783994674683\n",
      "-- train ok ! --\n",
      "0.8554791666666667\n",
      "fold 1626: \n",
      "-- training --\n",
      "22.017419815063477\n",
      "-- train ok ! --\n",
      "fold 1627: \n",
      "-- training --\n",
      "22.18973135948181\n",
      "-- train ok ! --\n",
      "fold 1628: \n",
      "-- training --\n",
      "22.334150552749634\n",
      "-- train ok ! --\n",
      "fold 1629: \n",
      "-- training --\n",
      "22.484363317489624\n",
      "-- train ok ! --\n",
      "fold 1630: \n",
      "-- training --\n",
      "22.18415641784668\n",
      "-- train ok ! --\n",
      "0.8560208333333333\n",
      "fold 1631: \n",
      "-- training --\n",
      "29.6059353351593\n",
      "-- train ok ! --\n",
      "fold 1632: \n",
      "-- training --\n",
      "31.345802545547485\n",
      "-- train ok ! --\n",
      "fold 1633: \n",
      "-- training --\n",
      "29.545214414596558\n",
      "-- train ok ! --\n",
      "fold 1634: \n",
      "-- training --\n",
      "31.87411594390869\n",
      "-- train ok ! --\n",
      "fold 1635: \n",
      "-- training --\n",
      "29.88768243789673\n",
      "-- train ok ! --\n",
      "0.8580833333333333\n",
      "fold 1636: \n",
      "-- training --\n",
      "37.241249561309814\n",
      "-- train ok ! --\n",
      "fold 1637: \n",
      "-- training --\n",
      "35.471778869628906\n",
      "-- train ok ! --\n",
      "fold 1638: \n",
      "-- training --\n",
      "37.31689763069153\n",
      "-- train ok ! --\n",
      "fold 1639: \n",
      "-- training --\n",
      "35.97442317008972\n",
      "-- train ok ! --\n",
      "fold 1640: \n",
      "-- training --\n",
      "37.51382303237915\n",
      "-- train ok ! --\n",
      "0.8577083333333333\n",
      "fold 1641: \n",
      "-- training --\n",
      "40.70892810821533\n",
      "-- train ok ! --\n",
      "fold 1642: \n",
      "-- training --\n",
      "39.17686343193054\n",
      "-- train ok ! --\n",
      "fold 1643: \n",
      "-- training --\n",
      "38.72921657562256\n",
      "-- train ok ! --\n",
      "fold 1644: \n",
      "-- training --\n",
      "38.66942620277405\n",
      "-- train ok ! --\n",
      "fold 1645: \n",
      "-- training --\n",
      "38.7431538105011\n",
      "-- train ok ! --\n",
      "0.8568958333333333\n",
      "fold 1646: \n",
      "-- training --\n",
      "50.27207541465759\n",
      "-- train ok ! --\n",
      "fold 1647: \n",
      "-- training --\n",
      "50.50118827819824\n",
      "-- train ok ! --\n",
      "fold 1648: \n",
      "-- training --\n",
      "54.07224893569946\n",
      "-- train ok ! --\n",
      "fold 1649: \n",
      "-- training --\n",
      "53.23060464859009\n",
      "-- train ok ! --\n",
      "fold 1650: \n",
      "-- training --\n",
      "52.705307483673096\n",
      "-- train ok ! --\n",
      "0.8566874999999999\n",
      "fold 1651: \n",
      "-- training --\n",
      "2.7944226264953613\n",
      "-- train ok ! --\n",
      "fold 1652: \n",
      "-- training --\n",
      "2.845975399017334\n",
      "-- train ok ! --\n",
      "fold 1653: \n",
      "-- training --\n",
      "2.8473875522613525\n",
      "-- train ok ! --\n",
      "fold 1654: \n",
      "-- training --\n",
      "2.8471829891204834\n",
      "-- train ok ! --\n",
      "fold 1655: \n",
      "-- training --\n",
      "2.837857961654663\n",
      "-- train ok ! --\n",
      "0.7494583333333333\n",
      "fold 1656: \n",
      "-- training --\n",
      "4.974546670913696\n",
      "-- train ok ! --\n",
      "fold 1657: \n",
      "-- training --\n",
      "5.16109824180603\n",
      "-- train ok ! --\n",
      "fold 1658: \n",
      "-- training --\n",
      "5.174103736877441\n",
      "-- train ok ! --\n",
      "fold 1659: \n",
      "-- training --\n",
      "4.990488290786743\n",
      "-- train ok ! --\n",
      "fold 1660: \n",
      "-- training --\n",
      "5.161900281906128\n",
      "-- train ok ! --\n",
      "0.8424583333333333\n",
      "fold 1661: \n",
      "-- training --\n",
      "6.489562511444092\n",
      "-- train ok ! --\n",
      "fold 1662: \n",
      "-- training --\n",
      "6.674475193023682\n",
      "-- train ok ! --\n",
      "fold 1663: \n",
      "-- training --\n",
      "6.647886753082275\n",
      "-- train ok ! --\n",
      "fold 1664: \n",
      "-- training --\n",
      "6.696669816970825\n",
      "-- train ok ! --\n",
      "fold 1665: \n",
      "-- training --\n",
      "6.623948812484741\n",
      "-- train ok ! --\n",
      "0.850125\n",
      "fold 1666: \n",
      "-- training --\n",
      "13.020190715789795\n",
      "-- train ok ! --\n",
      "fold 1667: \n",
      "-- training --\n",
      "13.029045343399048\n",
      "-- train ok ! --\n",
      "fold 1668: \n",
      "-- training --\n",
      "13.014700412750244\n",
      "-- train ok ! --\n",
      "fold 1669: \n",
      "-- training --\n",
      "12.832656145095825\n",
      "-- train ok ! --\n",
      "fold 1670: \n",
      "-- training --\n",
      "12.759770631790161\n",
      "-- train ok ! --\n",
      "0.8546458333333333\n",
      "fold 1671: \n",
      "-- training --\n",
      "22.20716691017151\n",
      "-- train ok ! --\n",
      "fold 1672: \n",
      "-- training --\n",
      "22.36567234992981\n",
      "-- train ok ! --\n",
      "fold 1673: \n",
      "-- training --\n",
      "22.329986572265625\n",
      "-- train ok ! --\n",
      "fold 1674: \n",
      "-- training --\n",
      "22.161927700042725\n",
      "-- train ok ! --\n",
      "fold 1675: \n",
      "-- training --\n",
      "22.479915618896484\n",
      "-- train ok ! --\n",
      "0.8536875\n",
      "fold 1676: \n",
      "-- training --\n",
      "30.432767629623413\n",
      "-- train ok ! --\n",
      "fold 1677: \n",
      "-- training --\n",
      "32.75539970397949\n",
      "-- train ok ! --\n",
      "fold 1678: \n",
      "-- training --\n",
      "30.278813362121582\n",
      "-- train ok ! --\n",
      "fold 1679: \n",
      "-- training --\n",
      "32.15183424949646\n",
      "-- train ok ! --\n",
      "fold 1680: \n",
      "-- training --\n",
      "30.116139888763428\n",
      "-- train ok ! --\n",
      "0.8571250000000001\n",
      "fold 1681: \n",
      "-- training --\n",
      "36.692808628082275\n",
      "-- train ok ! --\n",
      "fold 1682: \n",
      "-- training --\n",
      "34.96313524246216\n",
      "-- train ok ! --\n",
      "fold 1683: \n",
      "-- training --\n",
      "34.52712059020996\n",
      "-- train ok ! --\n",
      "fold 1684: \n",
      "-- training --\n",
      "34.65323519706726\n",
      "-- train ok ! --\n",
      "fold 1685: \n",
      "-- training --\n",
      "34.25848984718323\n",
      "-- train ok ! --\n",
      "0.8567291666666665\n",
      "fold 1686: \n",
      "-- training --\n",
      "39.79433608055115\n",
      "-- train ok ! --\n",
      "fold 1687: \n",
      "-- training --\n",
      "38.933432817459106\n",
      "-- train ok ! --\n",
      "fold 1688: \n",
      "-- training --\n",
      "38.79790902137756\n",
      "-- train ok ! --\n",
      "fold 1689: \n",
      "-- training --\n",
      "39.03100943565369\n",
      "-- train ok ! --\n",
      "fold 1690: \n",
      "-- training --\n",
      "39.32256531715393\n",
      "-- train ok ! --\n",
      "0.8571041666666666\n",
      "fold 1691: \n",
      "-- training --\n",
      "54.32545447349548\n",
      "-- train ok ! --\n",
      "fold 1692: \n",
      "-- training --\n",
      "53.50305771827698\n",
      "-- train ok ! --\n",
      "fold 1693: \n",
      "-- training --\n",
      "54.01503539085388\n",
      "-- train ok ! --\n",
      "fold 1694: \n",
      "-- training --\n",
      "53.60538601875305\n",
      "-- train ok ! --\n",
      "fold 1695: \n",
      "-- training --\n",
      "53.05196666717529\n",
      "-- train ok ! --\n",
      "0.8549374999999999\n",
      "fold 1696: \n",
      "-- training --\n",
      "2.950716257095337\n",
      "-- train ok ! --\n",
      "fold 1697: \n",
      "-- training --\n",
      "3.004006862640381\n",
      "-- train ok ! --\n",
      "fold 1698: \n",
      "-- training --\n",
      "3.002448558807373\n",
      "-- train ok ! --\n",
      "fold 1699: \n",
      "-- training --\n",
      "2.9534075260162354\n",
      "-- train ok ! --\n",
      "fold 1700: \n",
      "-- training --\n",
      "3.0010159015655518\n",
      "-- train ok ! --\n",
      "0.7497916666666666\n",
      "fold 1701: \n",
      "-- training --\n",
      "5.04835844039917\n",
      "-- train ok ! --\n",
      "fold 1702: \n",
      "-- training --\n",
      "5.225833892822266\n",
      "-- train ok ! --\n",
      "fold 1703: \n",
      "-- training --\n",
      "5.303476572036743\n",
      "-- train ok ! --\n",
      "fold 1704: \n",
      "-- training --\n",
      "5.2432475090026855\n",
      "-- train ok ! --\n",
      "fold 1705: \n",
      "-- training --\n",
      "5.308827638626099\n",
      "-- train ok ! --\n",
      "0.8423124999999999\n",
      "fold 1706: \n",
      "-- training --\n",
      "6.920877933502197\n",
      "-- train ok ! --\n",
      "fold 1707: \n",
      "-- training --\n",
      "7.0552520751953125\n",
      "-- train ok ! --\n",
      "fold 1708: \n",
      "-- training --\n",
      "6.880432844161987\n",
      "-- train ok ! --\n",
      "fold 1709: \n",
      "-- training --\n",
      "6.840190172195435\n",
      "-- train ok ! --\n",
      "fold 1710: \n",
      "-- training --\n",
      "6.854773283004761\n",
      "-- train ok ! --\n",
      "0.8500833333333333\n",
      "fold 1711: \n",
      "-- training --\n",
      "13.147180080413818\n",
      "-- train ok ! --\n",
      "fold 1712: \n",
      "-- training --\n",
      "13.13678765296936\n",
      "-- train ok ! --\n",
      "fold 1713: \n",
      "-- training --\n",
      "12.948211193084717\n",
      "-- train ok ! --\n",
      "fold 1714: \n",
      "-- training --\n",
      "12.769401788711548\n",
      "-- train ok ! --\n",
      "fold 1715: \n",
      "-- training --\n",
      "12.833268404006958\n",
      "-- train ok ! --\n",
      "0.8539166666666667\n",
      "fold 1716: \n",
      "-- training --\n",
      "21.885380029678345\n",
      "-- train ok ! --\n",
      "fold 1717: \n",
      "-- training --\n",
      "22.316612720489502\n",
      "-- train ok ! --\n",
      "fold 1718: \n",
      "-- training --\n",
      "22.1287841796875\n",
      "-- train ok ! --\n",
      "fold 1719: \n",
      "-- training --\n",
      "22.314728260040283\n",
      "-- train ok ! --\n",
      "fold 1720: \n",
      "-- training --\n",
      "21.970397472381592\n",
      "-- train ok ! --\n",
      "0.8553958333333334\n",
      "fold 1721: \n",
      "-- training --\n",
      "30.277568578720093\n",
      "-- train ok ! --\n",
      "fold 1722: \n",
      "-- training --\n",
      "30.424611806869507\n",
      "-- train ok ! --\n",
      "fold 1723: \n",
      "-- training --\n",
      "30.745033979415894\n",
      "-- train ok ! --\n",
      "fold 1724: \n",
      "-- training --\n",
      "32.7630033493042\n",
      "-- train ok ! --\n",
      "fold 1725: \n",
      "-- training --\n",
      "30.586907386779785\n",
      "-- train ok ! --\n",
      "0.8567499999999999\n",
      "fold 1726: \n",
      "-- training --\n",
      "37.21834111213684\n",
      "-- train ok ! --\n",
      "fold 1727: \n",
      "-- training --\n",
      "36.93157410621643\n",
      "-- train ok ! --\n",
      "fold 1728: \n",
      "-- training --\n",
      "36.17746663093567\n",
      "-- train ok ! --\n",
      "fold 1729: \n",
      "-- training --\n",
      "34.75799489021301\n",
      "-- train ok ! --\n",
      "fold 1730: \n",
      "-- training --\n",
      "36.0068416595459\n",
      "-- train ok ! --\n",
      "0.8562291666666667\n",
      "fold 1731: \n",
      "-- training --\n",
      "39.31744623184204\n",
      "-- train ok ! --\n",
      "fold 1732: \n",
      "-- training --\n",
      "40.12148308753967\n",
      "-- train ok ! --\n",
      "fold 1733: \n",
      "-- training --\n",
      "39.032991886138916\n",
      "-- train ok ! --\n",
      "fold 1734: \n",
      "-- training --\n",
      "42.277769327163696\n",
      "-- train ok ! --\n",
      "fold 1735: \n",
      "-- training --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.695725440979004\n",
      "-- train ok ! --\n",
      "0.8549791666666667\n",
      "fold 1736: \n",
      "-- training --\n",
      "53.99653506278992\n",
      "-- train ok ! --\n",
      "fold 1737: \n",
      "-- training --\n",
      "53.78936004638672\n",
      "-- train ok ! --\n",
      "fold 1738: \n",
      "-- training --\n",
      "52.15015435218811\n",
      "-- train ok ! --\n",
      "fold 1739: \n",
      "-- training --\n",
      "52.418556451797485\n",
      "-- train ok ! --\n",
      "fold 1740: \n",
      "-- training --\n",
      "54.94852948188782\n",
      "-- train ok ! --\n",
      "0.8551041666666667\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 15):\n",
    "    for n in [5, 20, 30, 50, 70, 100, 130, 150, 200]:\n",
    "        start_transform = time.time()\n",
    "        pca = PCA(n_components=n)\n",
    "        pca.fit(X_train)\n",
    "        X_train_pca = pca.transform(X_train)\n",
    "        pca.transform(X_train)\n",
    "        end_transform = time.time()\n",
    "        X_train_pca = pd.DataFrame(X_train_pca)\n",
    "        performance_accuracy = []\n",
    "        pipe = Pipeline([('knn',KNeighborsClassifier(n_neighbors=k, p=2))])\n",
    "        for (train_index, test_index) in kfold.split(X_train_pca):\n",
    "            print('fold '+str(fold_count)+': ')\n",
    "            print('-- training --')\n",
    "            X_kfold_train_pca, y_kfold_train = X_train_pca.iloc[train_index], y_train.loc[train_index]\n",
    "            X_kfold_test_pca, y_kfold_test = X_train_pca.iloc[test_index], y_train.loc[test_index]\n",
    "            start = time.time()\n",
    "            pipe.fit(X_kfold_train_pca, y_kfold_train)\n",
    "            y_kfold_pred = pipe.predict(X_kfold_test_pca)\n",
    "            end = time.time()\n",
    "            train_time = end-start + end_transform - start_transform\n",
    "            print(train_time)\n",
    "            print('-- train ok ! --')\n",
    "            perf = accuracy_score(y_kfold_test, y_kfold_pred)\n",
    "            performance_accuracy.append(perf)\n",
    "\n",
    "            fold_count = fold_count + 1\n",
    "        validation_times_euclidean_pca.append((k,n,np.mean(train_time)))\n",
    "        accs_knn_euclidean_pca.append((k,n,np.mean(performance_accuracy)))\n",
    "        print(np.mean(performance_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>componentes principales</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2.803779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>4.942146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>6.249109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>11.781786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>70</td>\n",
       "      <td>18.352786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>14</td>\n",
       "      <td>70</td>\n",
       "      <td>21.970397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>14</td>\n",
       "      <td>100</td>\n",
       "      <td>30.586907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>14</td>\n",
       "      <td>130</td>\n",
       "      <td>36.006842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>14</td>\n",
       "      <td>150</td>\n",
       "      <td>41.695725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>14</td>\n",
       "      <td>200</td>\n",
       "      <td>54.948529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      k  componentes principales       time\n",
       "0     5                        5   2.803779\n",
       "1     5                       20   4.942146\n",
       "2     5                       30   6.249109\n",
       "3     5                       50  11.781786\n",
       "4     5                       70  18.352786\n",
       "..   ..                      ...        ...\n",
       "169  14                       70  21.970397\n",
       "170  14                      100  30.586907\n",
       "171  14                      130  36.006842\n",
       "172  14                      150  41.695725\n",
       "173  14                      200  54.948529\n",
       "\n",
       "[174 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times_knn_manhattan_pca = pd.DataFrame(validation_times_manhattan_pca, columns=['k','componentes principales','time'])\n",
    "times_knn_manhattan_pca.to_csv('results/times_knn_manhattan_pca.csv', index=False)\n",
    "times_knn_euclidean_pca = pd.DataFrame(validation_times_euclidean_pca, columns=['k','componentes principales','time'])\n",
    "times_knn_euclidean_pca.to_csv('results/times_knn_euclidean_pca.csv', index=False)\n",
    "times_knn_euclidean_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformaciones no lineales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = data.drop(['label'], axis = 1), data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = SEED)\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "\n",
    "y_train.index = X_train.index\n",
    "performance_accuracy = []\n",
    "\n",
    "fold_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 100)\n",
      "4.977585554122925\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "pca.fit(X_train)\n",
    "X_pca = pca.transform(X_train)\n",
    "pca.transform(X_train)\n",
    "end = time.time()\n",
    "print(X_pca.shape)\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = X_pca.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-dd52707ed50c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_pca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "X_pca.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca_tsne = TSNE(n_components=3).fit_transform(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 3)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pca_tsne.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca_tsne = pd.DataFrame(X_pca_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 56: \n",
      "-- training --\n",
      "0.6754734516143799\n",
      "-- train ok ! --\n",
      "fold 57: \n",
      "-- training --\n",
      "0.6925036907196045\n",
      "-- train ok ! --\n",
      "fold 58: \n",
      "-- training --\n",
      "0.6824872493743896\n",
      "-- train ok ! --\n",
      "fold 59: \n",
      "-- training --\n",
      "0.6723136901855469\n",
      "-- train ok ! --\n",
      "fold 60: \n",
      "-- training --\n",
      "0.6882717609405518\n",
      "-- train ok ! --\n",
      "0.84275\n"
     ]
    }
   ],
   "source": [
    "performance_accuracy = []\n",
    "pipe = Pipeline([('knn',KNeighborsClassifier(n_neighbors=10, p=1))])\n",
    "for (train_index, test_index) in kfold.split(X_pca_tsne):\n",
    "    print('fold '+str(fold_count)+': ')\n",
    "    print('-- training --')\n",
    "    X_kfold_train, y_kfold_train = X_pca_tsne.iloc[train_index], y_train.loc[train_index]\n",
    "    X_kfold_test, y_kfold_test = X_pca_tsne.iloc[test_index], y_train.loc[test_index]\n",
    "    start = time.time()\n",
    "    pipe.fit(X_kfold_train, y_kfold_train)\n",
    "    y_kfold_pred = pipe.predict(X_kfold_test)\n",
    "    end = time.time()\n",
    "    train_time = end-start\n",
    "    print(train_time)\n",
    "    print('-- train ok ! --')\n",
    "    perf = accuracy_score(y_kfold_test, y_kfold_pred)\n",
    "    performance_accuracy.append(perf)\n",
    "\n",
    "    fold_count = fold_count + 1\n",
    "print(np.mean(performance_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_pca = KernelPCA(n_components=7, kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kernel_pca  = kernel_pca.fit_transform(X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
